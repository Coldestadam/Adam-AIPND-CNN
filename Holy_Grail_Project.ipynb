{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Holy Grail Project",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPXAT3S/wI6LFAHEET0jRnq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Coldestadam/Adam-AIPND-CNN/blob/master/Holy_Grail_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9X9DOzai8TK"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ispCw0yHj_yK"
      },
      "source": [
        "# Uploading Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "id": "xfcgiYD_kBhF",
        "outputId": "d18ea5ef-1721-4ec1-f384-465c5d3407ff"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9f3131e4-eaa9-4104-8bfa-06fa3b28b215\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9f3131e4-eaa9-4104-8bfa-06fa3b28b215\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving concrete_data.csv to concrete_data.csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'concrete_data.csv': b'Cement,Blast Furnace Slag,Fly Ash,Water,Superplasticizer,Coarse Aggregate,Fine Aggregate,Age,Strength\\r\\n540.0 ,0.0 ,0.0 ,162.0 ,2.5 ,1040.0 ,676.0 ,28 ,79.99 \\r\\n540.0 ,0.0 ,0.0 ,162.0 ,2.5 ,1055.0 ,676.0 ,28 ,61.89 \\r\\n332.5 ,142.5 ,0.0 ,228.0 ,0.0 ,932.0 ,594.0 ,270 ,40.27 \\r\\n332.5 ,142.5 ,0.0 ,228.0 ,0.0 ,932.0 ,594.0 ,365 ,41.05 \\r\\n198.6 ,132.4 ,0.0 ,192.0 ,0.0 ,978.4 ,825.5 ,360 ,44.30 \\r\\n266.0 ,114.0 ,0.0 ,228.0 ,0.0 ,932.0 ,670.0 ,90 ,47.03 \\r\\n380.0 ,95.0 ,0.0 ,228.0 ,0.0 ,932.0 ,594.0 ,365 ,43.70 \\r\\n380.0 ,95.0 ,0.0 ,228.0 ,0.0 ,932.0 ,594.0 ,28 ,36.45 \\r\\n266.0 ,114.0 ,0.0 ,228.0 ,0.0 ,932.0 ,670.0 ,28 ,45.85 \\r\\n475.0 ,0.0 ,0.0 ,228.0 ,0.0 ,932.0 ,594.0 ,28 ,39.29 \\r\\n198.6 ,132.4 ,0.0 ,192.0 ,0.0 ,978.4 ,825.5 ,90 ,38.07 \\r\\n198.6 ,132.4 ,0.0 ,192.0 ,0.0 ,978.4 ,825.5 ,28 ,28.02 \\r\\n427.5 ,47.5 ,0.0 ,228.0 ,0.0 ,932.0 ,594.0 ,270 ,43.01 \\r\\n190.0 ,190.0 ,0.0 ,228.0 ,0.0 ,932.0 ,670.0 ,90 ,42.33 \\r\\n304.0 ,76.0 ,0.0 ,228.0 ,0.0 ,932.0 ,670.0 ,28 ,47.81 \\r\\n380.0 ,0.0 ,0.0 ,228.0 ,0.0 ,932.0 ,670.0 ,90 ,52.91 \\r\\n139.6 ,209.4 ,0.0 ,192.0 ,0.0 ,1047.0 ,806.9 ,90 ,39.36 \\r\\n342.0 ,38.0 ,0.0 ,228.0 ,0.0 ,932.0 ,670.0 ,365 ,56.14 \\r\\n380.0 ,95.0 ,0.0 ,228.0 ,0.0 ,932.0 ,594.0 ,90 ,40.56 \\r\\n475.0 ,0.0 ,0.0 ,228.0 ,0.0 ,932.0 ,594.0 ,180 ,42.62 \\r\\n427.5 ,47.5 ,0.0 ,228.0 ,0.0 ,932.0 ,594.0 ,180 ,41.84 \\r\\n139.6 ,209.4 ,0.0 ,192.0 ,0.0 ,1047.0 ,806.9 ,28 ,28.24 \\r\\n139.6 ,209.4 ,0.0 ,192.0 ,0.0 ,1047.0 ,806.9 ,3 ,8.06 \\r\\n139.6 ,209.4 ,0.0 ,192.0 ,0.0 ,1047.0 ,806.9 ,180 ,44.21 \\r\\n380.0 ,0.0 ,0.0 ,228.0 ,0.0 ,932.0 ,670.0 ,365 ,52.52 \\r\\n380.0 ,0.0 ,0.0 ,228.0 ,0.0 ,932.0 ,670.0 ,270 ,53.30 \\r\\n380.0 ,95.0 ,0.0 ,228.0 ,0.0 ,932.0 ,594.0 ,270 ,41.15 \\r\\n342.0 ,38.0 ,0.0 ,228.0 ,0.0 ,932.0 ,670.0 ,180 ,52.12 \\r\\n427.5 ,47.5 ,0.0 ,228.0 ,0.0 ,932.0 ,594.0 ,28 ,37.43 \\r\\n475.0 ,0.0 ,0.0 ,228.0 ,0.0 ,932.0 ,594.0 ,7 ,38.60 \\r\\n304.0 ,76.0 ,0.0 ,228.0 ,0.0 ,932.0 ,670.0 ,365 ,55.26 \\r\\n266.0 ,114.0 ,0.0 ,228.0 ,0.0 ,932.0 ,670.0 ,365 ,52.91 \\r\\n198.6 ,132.4 ,0.0 ,192.0 ,0.0 ,978.4 ,825.5 ,180 ,41.72 \\r\\n475.0 ,0.0 ,0.0 ,228.0 ,0.0 ,932.0 ,594.0 ,270 ,42.13 \\r\\n190.0 ,190.0 ,0.0 ,228.0 ,0.0 ,932.0 ,670.0 ,365 ,53.69 \\r\\n237.5 ,237.5 ,0.0 ,228.0 ,0.0 ,932.0 ,594.0 ,270 ,38.41 \\r\\n237.5 ,237.5 ,0.0 ,228.0 ,0.0 ,932.0 ,594.0 ,28 ,30.08 \\r\\n332.5 ,142.5 ,0.0 ,228.0 ,0.0 ,932.0 ,594.0 ,90 ,37.72 \\r\\n475.0 ,0.0 ,0.0 ,228.0 ,0.0 ,932.0 ,594.0 ,90 ,42.23 \\r\\n237.5 ,237.5 ,0.0 ,228.0 ,0.0 ,932.0 ,594.0 ,180 ,36.25 \\r\\n342.0 ,38.0 ,0.0 ,228.0 ,0.0 ,932.0 ,670.0 ,90 ,50.46 \\r\\n427.5 ,47.5 ,0.0 ,228.0 ,0.0 ,932.0 ,594.0 ,365 ,43.70 \\r\\n237.5 ,237.5 ,0.0 ,228.0 ,0.0 ,932.0 ,594.0 ,365 ,39.00 \\r\\n380.0 ,0.0 ,0.0 ,228.0 ,0.0 ,932.0 ,670.0 ,180 ,53.10 \\r\\n427.5 ,47.5 ,0.0 ,228.0 ,0.0 ,932.0 ,594.0 ,90 ,41.54 \\r\\n427.5 ,47.5 ,0.0 ,228.0 ,0.0 ,932.0 ,594.0 ,7 ,35.08 \\r\\n349.0 ,0.0 ,0.0 ,192.0 ,0.0 ,1047.0 ,806.9 ,3 ,15.05 \\r\\n380.0 ,95.0 ,0.0 ,228.0 ,0.0 ,932.0 ,594.0 ,180 ,40.76 \\r\\n237.5 ,237.5 ,0.0 ,228.0 ,0.0 ,932.0 ,594.0 ,7 ,26.26 \\r\\n380.0 ,95.0 ,0.0 ,228.0 ,0.0 ,932.0 ,594.0 ,7 ,32.82 \\r\\n332.5 ,142.5 ,0.0 ,228.0 ,0.0 ,932.0 ,594.0 ,180 ,39.78 \\r\\n190.0 ,190.0 ,0.0 ,228.0 ,0.0 ,932.0 ,670.0 ,180 ,46.93 \\r\\n237.5 ,237.5 ,0.0 ,228.0 ,0.0 ,932.0 ,594.0 ,90 ,33.12 \\r\\n304.0 ,76.0 ,0.0 ,228.0 ,0.0 ,932.0 ,670.0 ,90 ,49.19 \\r\\n139.6 ,209.4 ,0.0 ,192.0 ,0.0 ,1047.0 ,806.9 ,7 ,14.59 \\r\\n198.6 ,132.4 ,0.0 ,192.0 ,0.0 ,978.4 ,825.5 ,7 ,14.64 \\r\\n475.0 ,0.0 ,0.0 ,228.0 ,0.0 ,932.0 ,594.0 ,365 ,41.93 \\r\\n198.6 ,132.4 ,0.0 ,192.0 ,0.0 ,978.4 ,825.5 ,3 ,9.13 \\r\\n304.0 ,76.0 ,0.0 ,228.0 ,0.0 ,932.0 ,670.0 ,180 ,50.95 \\r\\n332.5 ,142.5 ,0.0 ,228.0 ,0.0 ,932.0 ,594.0 ,28 ,33.02 \\r\\n304.0 ,76.0 ,0.0 ,228.0 ,0.0 ,932.0 ,670.0 ,270 ,54.38 \\r\\n266.0 ,114.0 ,0.0 ,228.0 ,0.0 ,932.0 ,670.0 ,270 ,51.73 \\r\\n310.0 ,0.0 ,0.0 ,192.0 ,0.0 ,971.0 ,850.6 ,3 ,9.87 \\r\\n190.0 ,190.0 ,0.0 ,228.0 ,0.0 ,932.0 ,670.0 ,270 ,50.66 \\r\\n266.0 ,114.0 ,0.0 ,228.0 ,0.0 ,932.0 ,670.0 ,180 ,48.70 \\r\\n342.0 ,38.0 ,0.0 ,228.0 ,0.0 ,932.0 ,670.0 ,270 ,55.06 \\r\\n139.6 ,209.4 ,0.0 ,192.0 ,0.0 ,1047.0 ,806.9 ,360 ,44.70 \\r\\n332.5 ,142.5 ,0.0 ,228.0 ,0.0 ,932.0 ,594.0 ,7 ,30.28 \\r\\n190.0 ,190.0 ,0.0 ,228.0 ,0.0 ,932.0 ,670.0 ,28 ,40.86 \\r\\n485.0 ,0.0 ,0.0 ,146.0 ,0.0 ,1120.0 ,800.0 ,28 ,71.99 \\r\\n374.0 ,189.2 ,0.0 ,170.1 ,10.1 ,926.1 ,756.7 ,3 ,34.40 \\r\\n313.3 ,262.2 ,0.0 ,175.5 ,8.6 ,1046.9 ,611.8 ,3 ,28.80 \\r\\n425.0 ,106.3 ,0.0 ,153.5 ,16.5 ,852.1 ,887.1 ,3 ,33.40 \\r\\n425.0 ,106.3 ,0.0 ,151.4 ,18.6 ,936.0 ,803.7 ,3 ,36.30 \\r\\n375.0 ,93.8 ,0.0 ,126.6 ,23.4 ,852.1 ,992.6 ,3 ,29.00 \\r\\n475.0 ,118.8 ,0.0 ,181.1 ,8.9 ,852.1 ,781.5 ,3 ,37.80 \\r\\n469.0 ,117.2 ,0.0 ,137.8 ,32.2 ,852.1 ,840.5 ,3 ,40.20 \\r\\n425.0 ,106.3 ,0.0 ,153.5 ,16.5 ,852.1 ,887.1 ,3 ,33.40 \\r\\n388.6 ,97.1 ,0.0 ,157.9 ,12.1 ,852.1 ,925.7 ,3 ,28.10 \\r\\n531.3 ,0.0 ,0.0 ,141.8 ,28.2 ,852.1 ,893.7 ,3 ,41.30 \\r\\n425.0 ,106.3 ,0.0 ,153.5 ,16.5 ,852.1 ,887.1 ,3 ,33.40 \\r\\n318.8 ,212.5 ,0.0 ,155.7 ,14.3 ,852.1 ,880.4 ,3 ,25.20 \\r\\n401.8 ,94.7 ,0.0 ,147.4 ,11.4 ,946.8 ,852.1 ,3 ,41.10 \\r\\n362.6 ,189.0 ,0.0 ,164.9 ,11.6 ,944.7 ,755.8 ,3 ,35.30 \\r\\n323.7 ,282.8 ,0.0 ,183.8 ,10.3 ,942.7 ,659.9 ,3 ,28.30 \\r\\n379.5 ,151.2 ,0.0 ,153.9 ,15.9 ,1134.3 ,605.0 ,3 ,28.60 \\r\\n362.6 ,189.0 ,0.0 ,164.9 ,11.6 ,944.7 ,755.8 ,3 ,35.30 \\r\\n286.3 ,200.9 ,0.0 ,144.7 ,11.2 ,1004.6 ,803.7 ,3 ,24.40 \\r\\n362.6 ,189.0 ,0.0 ,164.9 ,11.6 ,944.7 ,755.8 ,3 ,35.30 \\r\\n439.0 ,177.0 ,0.0 ,186.0 ,11.1 ,884.9 ,707.9 ,3 ,39.30 \\r\\n389.9 ,189.0 ,0.0 ,145.9 ,22.0 ,944.7 ,755.8 ,3 ,40.60 \\r\\n362.6 ,189.0 ,0.0 ,164.9 ,11.6 ,944.7 ,755.8 ,3 ,35.30 \\r\\n337.9 ,189.0 ,0.0 ,174.9 ,9.5 ,944.7 ,755.8 ,3 ,24.10 \\r\\n374.0 ,189.2 ,0.0 ,170.1 ,10.1 ,926.1 ,756.7 ,7 ,46.20 \\r\\n313.3 ,262.2 ,0.0 ,175.5 ,8.6 ,1046.9 ,611.8 ,7 ,42.80 \\r\\n425.0 ,106.3 ,0.0 ,153.5 ,16.5 ,852.1 ,887.1 ,7 ,49.20 \\r\\n425.0 ,106.3 ,0.0 ,151.4 ,18.6 ,936.0 ,803.7 ,7 ,46.80 \\r\\n375.0 ,93.8 ,0.0 ,126.6 ,23.4 ,852.1 ,992.6 ,7 ,45.70 \\r\\n475.0 ,118.8 ,0.0 ,181.1 ,8.9 ,852.1 ,781.5 ,7 ,55.60 \\r\\n469.0 ,117.2 ,0.0 ,137.8 ,32.2 ,852.1 ,840.5 ,7 ,54.90 \\r\\n425.0 ,106.3 ,0.0 ,153.5 ,16.5 ,852.1 ,887.1 ,7 ,49.20 \\r\\n388.6 ,97.1 ,0.0 ,157.9 ,12.1 ,852.1 ,925.7 ,7 ,34.90 \\r\\n531.3 ,0.0 ,0.0 ,141.8 ,28.2 ,852.1 ,893.7 ,7 ,46.90 \\r\\n425.0 ,106.3 ,0.0 ,153.5 ,16.5 ,852.1 ,887.1 ,7 ,49.20 \\r\\n318.8 ,212.5 ,0.0 ,155.7 ,14.3 ,852.1 ,880.4 ,7 ,33.40 \\r\\n401.8 ,94.7 ,0.0 ,147.4 ,11.4 ,946.8 ,852.1 ,7 ,54.10 \\r\\n362.6 ,189.0 ,0.0 ,164.9 ,11.6 ,944.7 ,755.8 ,7 ,55.90 \\r\\n323.7 ,282.8 ,0.0 ,183.8 ,10.3 ,942.7 ,659.9 ,7 ,49.80 \\r\\n379.5 ,151.2 ,0.0 ,153.9 ,15.9 ,1134.3 ,605.0 ,7 ,47.10 \\r\\n362.6 ,189.0 ,0.0 ,164.9 ,11.6 ,944.7 ,755.8 ,7 ,55.90 \\r\\n286.3 ,200.9 ,0.0 ,144.7 ,11.2 ,1004.6 ,803.7 ,7 ,38.00 \\r\\n362.6 ,189.0 ,0.0 ,164.9 ,11.6 ,944.7 ,755.8 ,7 ,55.90 \\r\\n439.0 ,177.0 ,0.0 ,186.0 ,11.1 ,884.9 ,707.9 ,7 ,56.10 \\r\\n389.9 ,189.0 ,0.0 ,145.9 ,22.0 ,944.7 ,755.8 ,7 ,59.09 \\r\\n362.6 ,189.0 ,0.0 ,164.9 ,11.6 ,944.7 ,755.8 ,7 ,22.90 \\r\\n337.9 ,189.0 ,0.0 ,174.9 ,9.5 ,944.7 ,755.8 ,7 ,35.10 \\r\\n374.0 ,189.2 ,0.0 ,170.1 ,10.1 ,926.1 ,756.7 ,28 ,61.09 \\r\\n313.3 ,262.2 ,0.0 ,175.5 ,8.6 ,1046.9 ,611.8 ,28 ,59.80 \\r\\n425.0 ,106.3 ,0.0 ,153.5 ,16.5 ,852.1 ,887.1 ,28 ,60.29 \\r\\n425.0 ,106.3 ,0.0 ,151.4 ,18.6 ,936.0 ,803.7 ,28 ,61.80 \\r\\n375.0 ,93.8 ,0.0 ,126.6 ,23.4 ,852.1 ,992.6 ,28 ,56.70 \\r\\n475.0 ,118.8 ,0.0 ,181.1 ,8.9 ,852.1 ,781.5 ,28 ,68.30 \\r\\n469.0 ,117.2 ,0.0 ,137.8 ,32.2 ,852.1 ,840.5 ,28 ,66.90 \\r\\n425.0 ,106.3 ,0.0 ,153.5 ,16.5 ,852.1 ,887.1 ,28 ,60.29 \\r\\n388.6 ,97.1 ,0.0 ,157.9 ,12.1 ,852.1 ,925.7 ,28 ,50.70 \\r\\n531.3 ,0.0 ,0.0 ,141.8 ,28.2 ,852.1 ,893.7 ,28 ,56.40 \\r\\n425.0 ,106.3 ,0.0 ,153.5 ,16.5 ,852.1 ,887.1 ,28 ,60.29 \\r\\n318.8 ,212.5 ,0.0 ,155.7 ,14.3 ,852.1 ,880.4 ,28 ,55.50 \\r\\n401.8 ,94.7 ,0.0 ,147.4 ,11.4 ,946.8 ,852.1 ,28 ,68.50 \\r\\n362.6 ,189.0 ,0.0 ,164.9 ,11.6 ,944.7 ,755.8 ,28 ,71.30 \\r\\n323.7 ,282.8 ,0.0 ,183.8 ,10.3 ,942.7 ,659.9 ,28 ,74.70 \\r\\n379.5 ,151.2 ,0.0 ,153.9 ,15.9 ,1134.3 ,605.0 ,28 ,52.20 \\r\\n362.6 ,189.0 ,0.0 ,164.9 ,11.6 ,944.7 ,755.8 ,28 ,71.30 \\r\\n286.3 ,200.9 ,0.0 ,144.7 ,11.2 ,1004.6 ,803.7 ,28 ,67.70 \\r\\n362.6 ,189.0 ,0.0 ,164.9 ,11.6 ,944.7 ,755.8 ,28 ,71.30 \\r\\n439.0 ,177.0 ,0.0 ,186.0 ,11.1 ,884.9 ,707.9 ,28 ,66.00 \\r\\n389.9 ,189.0 ,0.0 ,145.9 ,22.0 ,944.7 ,755.8 ,28 ,74.50 \\r\\n362.6 ,189.0 ,0.0 ,164.9 ,11.6 ,944.7 ,755.8 ,28 ,71.30 \\r\\n337.9 ,189.0 ,0.0 ,174.9 ,9.5 ,944.7 ,755.8 ,28 ,49.90 \\r\\n374.0 ,189.2 ,0.0 ,170.1 ,10.1 ,926.1 ,756.7 ,56 ,63.40 \\r\\n313.3 ,262.2 ,0.0 ,175.5 ,8.6 ,1046.9 ,611.8 ,56 ,64.90 \\r\\n425.0 ,106.3 ,0.0 ,153.5 ,16.5 ,852.1 ,887.1 ,56 ,64.30 \\r\\n425.0 ,106.3 ,0.0 ,151.4 ,18.6 ,936.0 ,803.7 ,56 ,64.90 \\r\\n375.0 ,93.8 ,0.0 ,126.6 ,23.4 ,852.1 ,992.6 ,56 ,60.20 \\r\\n475.0 ,118.8 ,0.0 ,181.1 ,8.9 ,852.1 ,781.5 ,56 ,72.30 \\r\\n469.0 ,117.2 ,0.0 ,137.8 ,32.2 ,852.1 ,840.5 ,56 ,69.30 \\r\\n425.0 ,106.3 ,0.0 ,153.5 ,16.5 ,852.1 ,887.1 ,56 ,64.30 \\r\\n388.6 ,97.1 ,0.0 ,157.9 ,12.1 ,852.1 ,925.7 ,56 ,55.20 \\r\\n531.3 ,0.0 ,0.0 ,141.8 ,28.2 ,852.1 ,893.7 ,56 ,58.80 \\r\\n425.0 ,106.3 ,0.0 ,153.5 ,16.5 ,852.1 ,887.1 ,56 ,64.30 \\r\\n318.8 ,212.5 ,0.0 ,155.7 ,14.3 ,852.1 ,880.4 ,56 ,66.10 \\r\\n401.8 ,94.7 ,0.0 ,147.4 ,11.4 ,946.8 ,852.1 ,56 ,73.70 \\r\\n362.6 ,189.0 ,0.0 ,164.9 ,11.6 ,944.7 ,755.8 ,56 ,77.30 \\r\\n323.7 ,282.8 ,0.0 ,183.8 ,10.3 ,942.7 ,659.9 ,56 ,80.20 \\r\\n379.5 ,151.2 ,0.0 ,153.9 ,15.9 ,1134.3 ,605.0 ,56 ,54.90 \\r\\n362.6 ,189.0 ,0.0 ,164.9 ,11.6 ,944.7 ,755.8 ,56 ,77.30 \\r\\n286.3 ,200.9 ,0.0 ,144.7 ,11.2 ,1004.6 ,803.7 ,56 ,72.99 \\r\\n362.6 ,189.0 ,0.0 ,164.9 ,11.6 ,944.7 ,755.8 ,56 ,77.30 \\r\\n439.0 ,177.0 ,0.0 ,186.0 ,11.1 ,884.9 ,707.9 ,56 ,71.70 \\r\\n389.9 ,189.0 ,0.0 ,145.9 ,22.0 ,944.7 ,755.8 ,56 ,79.40 \\r\\n362.6 ,189.0 ,0.0 ,164.9 ,11.6 ,944.7 ,755.8 ,56 ,77.30 \\r\\n337.9 ,189.0 ,0.0 ,174.9 ,9.5 ,944.7 ,755.8 ,56 ,59.89 \\r\\n374.0 ,189.2 ,0.0 ,170.1 ,10.1 ,926.1 ,756.7 ,91 ,64.90 \\r\\n313.3 ,262.2 ,0.0 ,175.5 ,8.6 ,1046.9 ,611.8 ,91 ,66.60 \\r\\n425.0 ,106.3 ,0.0 ,153.5 ,16.5 ,852.1 ,887.1 ,91 ,65.20 \\r\\n425.0 ,106.3 ,0.0 ,151.4 ,18.6 ,936.0 ,803.7 ,91 ,66.70 \\r\\n375.0 ,93.8 ,0.0 ,126.6 ,23.4 ,852.1 ,992.6 ,91 ,62.50 \\r\\n475.0 ,118.8 ,0.0 ,181.1 ,8.9 ,852.1 ,781.5 ,91 ,74.19 \\r\\n469.0 ,117.2 ,0.0 ,137.8 ,32.2 ,852.1 ,840.5 ,91 ,70.70 \\r\\n425.0 ,106.3 ,0.0 ,153.5 ,16.5 ,852.1 ,887.1 ,91 ,65.20 \\r\\n388.6 ,97.1 ,0.0 ,157.9 ,12.1 ,852.1 ,925.7 ,91 ,57.60 \\r\\n531.3 ,0.0 ,0.0 ,141.8 ,28.2 ,852.1 ,893.7 ,91 ,59.20 \\r\\n425.0 ,106.3 ,0.0 ,153.5 ,16.5 ,852.1 ,887.1 ,91 ,65.20 \\r\\n318.8 ,212.5 ,0.0 ,155.7 ,14.3 ,852.1 ,880.4 ,91 ,68.10 \\r\\n401.8 ,94.7 ,0.0 ,147.4 ,11.4 ,946.8 ,852.1 ,91 ,75.50 \\r\\n362.6 ,189.0 ,0.0 ,164.9 ,11.6 ,944.7 ,755.8 ,91 ,79.30 \\r\\n379.5 ,151.2 ,0.0 ,153.9 ,15.9 ,1134.3 ,605.0 ,91 ,56.50 \\r\\n362.6 ,189.0 ,0.0 ,164.9 ,11.6 ,944.7 ,755.8 ,91 ,79.30 \\r\\n286.3 ,200.9 ,0.0 ,144.7 ,11.2 ,1004.6 ,803.7 ,91 ,76.80 \\r\\n362.6 ,189.0 ,0.0 ,164.9 ,11.6 ,944.7 ,755.8 ,91 ,79.30 \\r\\n439.0 ,177.0 ,0.0 ,186.0 ,11.1 ,884.9 ,707.9 ,91 ,73.30 \\r\\n389.9 ,189.0 ,0.0 ,145.9 ,22.0 ,944.7 ,755.8 ,91 ,82.60 \\r\\n362.6 ,189.0 ,0.0 ,164.9 ,11.6 ,944.7 ,755.8 ,91 ,79.30 \\r\\n337.9 ,189.0 ,0.0 ,174.9 ,9.5 ,944.7 ,755.8 ,91 ,67.80 \\r\\n222.4 ,0.0 ,96.7 ,189.3 ,4.5 ,967.1 ,870.3 ,3 ,11.58 \\r\\n222.4 ,0.0 ,96.7 ,189.3 ,4.5 ,967.1 ,870.3 ,14 ,24.45 \\r\\n222.4 ,0.0 ,96.7 ,189.3 ,4.5 ,967.1 ,870.3 ,28 ,24.89 \\r\\n222.4 ,0.0 ,96.7 ,189.3 ,4.5 ,967.1 ,870.3 ,56 ,29.45 \\r\\n222.4 ,0.0 ,96.7 ,189.3 ,4.5 ,967.1 ,870.3 ,100 ,40.71 \\r\\n233.8 ,0.0 ,94.6 ,197.9 ,4.6 ,947.0 ,852.2 ,3 ,10.38 \\r\\n233.8 ,0.0 ,94.6 ,197.9 ,4.6 ,947.0 ,852.2 ,14 ,22.14 \\r\\n233.8 ,0.0 ,94.6 ,197.9 ,4.6 ,947.0 ,852.2 ,28 ,22.84 \\r\\n233.8 ,0.0 ,94.6 ,197.9 ,4.6 ,947.0 ,852.2 ,56 ,27.66 \\r\\n233.8 ,0.0 ,94.6 ,197.9 ,4.6 ,947.0 ,852.2 ,100 ,34.56 \\r\\n194.7 ,0.0 ,100.5 ,165.6 ,7.5 ,1006.4 ,905.9 ,3 ,12.45 \\r\\n194.7 ,0.0 ,100.5 ,165.6 ,7.5 ,1006.4 ,905.9 ,14 ,24.99 \\r\\n194.7 ,0.0 ,100.5 ,165.6 ,7.5 ,1006.4 ,905.9 ,28 ,25.72 \\r\\n194.7 ,0.0 ,100.5 ,165.6 ,7.5 ,1006.4 ,905.9 ,56 ,33.96 \\r\\n194.7 ,0.0 ,100.5 ,165.6 ,7.5 ,1006.4 ,905.9 ,100 ,37.34 \\r\\n190.7 ,0.0 ,125.4 ,162.1 ,7.8 ,1090.0 ,804.0 ,3 ,15.04 \\r\\n190.7 ,0.0 ,125.4 ,162.1 ,7.8 ,1090.0 ,804.0 ,14 ,21.06 \\r\\n190.7 ,0.0 ,125.4 ,162.1 ,7.8 ,1090.0 ,804.0 ,28 ,26.40 \\r\\n190.7 ,0.0 ,125.4 ,162.1 ,7.8 ,1090.0 ,804.0 ,56 ,35.34 \\r\\n190.7 ,0.0 ,125.4 ,162.1 ,7.8 ,1090.0 ,804.0 ,100 ,40.57 \\r\\n212.1 ,0.0 ,121.6 ,180.3 ,5.7 ,1057.6 ,779.3 ,3 ,12.47 \\r\\n212.1 ,0.0 ,121.6 ,180.3 ,5.7 ,1057.6 ,779.3 ,14 ,20.92 \\r\\n212.1 ,0.0 ,121.6 ,180.3 ,5.7 ,1057.6 ,779.3 ,28 ,24.90 \\r\\n212.1 ,0.0 ,121.6 ,180.3 ,5.7 ,1057.6 ,779.3 ,56 ,34.20 \\r\\n212.1 ,0.0 ,121.6 ,180.3 ,5.7 ,1057.6 ,779.3 ,100 ,39.61 \\r\\n230.0 ,0.0 ,118.3 ,195.5 ,4.6 ,1029.4 ,758.6 ,3 ,10.03 \\r\\n230.0 ,0.0 ,118.3 ,195.5 ,4.6 ,1029.4 ,758.6 ,14 ,20.08 \\r\\n230.0 ,0.0 ,118.3 ,195.5 ,4.6 ,1029.4 ,758.6 ,28 ,24.48 \\r\\n230.0 ,0.0 ,118.3 ,195.5 ,4.6 ,1029.4 ,758.6 ,56 ,31.54 \\r\\n230.0 ,0.0 ,118.3 ,195.5 ,4.6 ,1029.4 ,758.6 ,100 ,35.34 \\r\\n190.3 ,0.0 ,125.2 ,161.9 ,9.9 ,1088.1 ,802.6 ,3 ,9.45 \\r\\n190.3 ,0.0 ,125.2 ,161.9 ,9.9 ,1088.1 ,802.6 ,14 ,22.72 \\r\\n190.3 ,0.0 ,125.2 ,161.9 ,9.9 ,1088.1 ,802.6 ,28 ,28.47 \\r\\n190.3 ,0.0 ,125.2 ,161.9 ,9.9 ,1088.1 ,802.6 ,56 ,38.56 \\r\\n190.3 ,0.0 ,125.2 ,161.9 ,9.9 ,1088.1 ,802.6 ,100 ,40.39 \\r\\n166.1 ,0.0 ,163.3 ,176.5 ,4.5 ,1058.6 ,780.1 ,3 ,10.76 \\r\\n166.1 ,0.0 ,163.3 ,176.5 ,4.5 ,1058.6 ,780.1 ,14 ,25.48 \\r\\n166.1 ,0.0 ,163.3 ,176.5 ,4.5 ,1058.6 ,780.1 ,28 ,21.54 \\r\\n166.1 ,0.0 ,163.3 ,176.5 ,4.5 ,1058.6 ,780.1 ,56 ,28.63 \\r\\n166.1 ,0.0 ,163.3 ,176.5 ,4.5 ,1058.6 ,780.1 ,100 ,33.54 \\r\\n168.0 ,42.1 ,163.8 ,121.8 ,5.7 ,1058.7 ,780.1 ,3 ,7.75 \\r\\n168.0 ,42.1 ,163.8 ,121.8 ,5.7 ,1058.7 ,780.1 ,14 ,17.82 \\r\\n168.0 ,42.1 ,163.8 ,121.8 ,5.7 ,1058.7 ,780.1 ,28 ,24.24 \\r\\n168.0 ,42.1 ,163.8 ,121.8 ,5.7 ,1058.7 ,780.1 ,56 ,32.85 \\r\\n168.0 ,42.1 ,163.8 ,121.8 ,5.7 ,1058.7 ,780.1 ,100 ,39.23 \\r\\n213.7 ,98.1 ,24.5 ,181.7 ,6.9 ,1065.8 ,785.4 ,3 ,18.00 \\r\\n213.7 ,98.1 ,24.5 ,181.7 ,6.9 ,1065.8 ,785.4 ,14 ,30.39 \\r\\n213.7 ,98.1 ,24.5 ,181.7 ,6.9 ,1065.8 ,785.4 ,28 ,45.71 \\r\\n213.7 ,98.1 ,24.5 ,181.7 ,6.9 ,1065.8 ,785.4 ,56 ,50.77 \\r\\n213.7 ,98.1 ,24.5 ,181.7 ,6.9 ,1065.8 ,785.4 ,100 ,53.90 \\r\\n213.8 ,98.1 ,24.5 ,181.7 ,6.7 ,1066.0 ,785.5 ,3 ,13.18 \\r\\n213.8 ,98.1 ,24.5 ,181.7 ,6.7 ,1066.0 ,785.5 ,14 ,17.84 \\r\\n213.8 ,98.1 ,24.5 ,181.7 ,6.7 ,1066.0 ,785.5 ,28 ,40.23 \\r\\n213.8 ,98.1 ,24.5 ,181.7 ,6.7 ,1066.0 ,785.5 ,56 ,47.13 \\r\\n213.8 ,98.1 ,24.5 ,181.7 ,6.7 ,1066.0 ,785.5 ,100 ,49.97 \\r\\n229.7 ,0.0 ,118.2 ,195.2 ,6.1 ,1028.1 ,757.6 ,3 ,13.36 \\r\\n229.7 ,0.0 ,118.2 ,195.2 ,6.1 ,1028.1 ,757.6 ,14 ,22.32 \\r\\n229.7 ,0.0 ,118.2 ,195.2 ,6.1 ,1028.1 ,757.6 ,28 ,24.54 \\r\\n229.7 ,0.0 ,118.2 ,195.2 ,6.1 ,1028.1 ,757.6 ,56 ,31.35 \\r\\n229.7 ,0.0 ,118.2 ,195.2 ,6.1 ,1028.1 ,757.6 ,100 ,40.86 \\r\\n238.1 ,0.0 ,94.1 ,186.7 ,7.0 ,949.9 ,847.0 ,3 ,19.93 \\r\\n238.1 ,0.0 ,94.1 ,186.7 ,7.0 ,949.9 ,847.0 ,14 ,25.69 \\r\\n238.1 ,0.0 ,94.1 ,186.7 ,7.0 ,949.9 ,847.0 ,28 ,30.23 \\r\\n238.1 ,0.0 ,94.1 ,186.7 ,7.0 ,949.9 ,847.0 ,56 ,39.59 \\r\\n238.1 ,0.0 ,94.1 ,186.7 ,7.0 ,949.9 ,847.0 ,100 ,44.30 \\r\\n250.0 ,0.0 ,95.7 ,187.4 ,5.5 ,956.9 ,861.2 ,3 ,13.82 \\r\\n250.0 ,0.0 ,95.7 ,187.4 ,5.5 ,956.9 ,861.2 ,14 ,24.92 \\r\\n250.0 ,0.0 ,95.7 ,187.4 ,5.5 ,956.9 ,861.2 ,28 ,29.22 \\r\\n250.0 ,0.0 ,95.7 ,187.4 ,5.5 ,956.9 ,861.2 ,56 ,38.33 \\r\\n250.0 ,0.0 ,95.7 ,187.4 ,5.5 ,956.9 ,861.2 ,100 ,42.35 \\r\\n212.5 ,0.0 ,100.4 ,159.3 ,8.7 ,1007.8 ,903.6 ,3 ,13.54 \\r\\n212.5 ,0.0 ,100.4 ,159.3 ,8.7 ,1007.8 ,903.6 ,14 ,26.31 \\r\\n212.5 ,0.0 ,100.4 ,159.3 ,8.7 ,1007.8 ,903.6 ,28 ,31.64 \\r\\n212.5 ,0.0 ,100.4 ,159.3 ,8.7 ,1007.8 ,903.6 ,56 ,42.55 \\r\\n212.5 ,0.0 ,100.4 ,159.3 ,8.7 ,1007.8 ,903.6 ,100 ,42.92 \\r\\n212.6 ,0.0 ,100.4 ,159.4 ,10.4 ,1003.8 ,903.8 ,3 ,13.33 \\r\\n212.6 ,0.0 ,100.4 ,159.4 ,10.4 ,1003.8 ,903.8 ,14 ,25.37 \\r\\n212.6 ,0.0 ,100.4 ,159.4 ,10.4 ,1003.8 ,903.8 ,28 ,37.40 \\r\\n212.6 ,0.0 ,100.4 ,159.4 ,10.4 ,1003.8 ,903.8 ,56 ,44.40 \\r\\n212.6 ,0.0 ,100.4 ,159.4 ,10.4 ,1003.8 ,903.8 ,100 ,47.74 \\r\\n212.0 ,0.0 ,124.8 ,159.0 ,7.8 ,1085.4 ,799.5 ,3 ,19.52 \\r\\n212.0 ,0.0 ,124.8 ,159.0 ,7.8 ,1085.4 ,799.5 ,14 ,31.35 \\r\\n212.0 ,0.0 ,124.8 ,159.0 ,7.8 ,1085.4 ,799.5 ,28 ,38.50 \\r\\n212.0 ,0.0 ,124.8 ,159.0 ,7.8 ,1085.4 ,799.5 ,56 ,45.08 \\r\\n212.0 ,0.0 ,124.8 ,159.0 ,7.8 ,1085.4 ,799.5 ,100 ,47.82 \\r\\n231.8 ,0.0 ,121.6 ,174.0 ,6.7 ,1056.4 ,778.5 ,3 ,15.44 \\r\\n231.8 ,0.0 ,121.6 ,174.0 ,6.7 ,1056.4 ,778.5 ,14 ,26.77 \\r\\n231.8 ,0.0 ,121.6 ,174.0 ,6.7 ,1056.4 ,778.5 ,28 ,33.73 \\r\\n231.8 ,0.0 ,121.6 ,174.0 ,6.7 ,1056.4 ,778.5 ,56 ,42.70 \\r\\n231.8 ,0.0 ,121.6 ,174.0 ,6.7 ,1056.4 ,778.5 ,100 ,45.84 \\r\\n251.4 ,0.0 ,118.3 ,188.5 ,5.8 ,1028.4 ,757.7 ,3 ,17.22 \\r\\n251.4 ,0.0 ,118.3 ,188.5 ,5.8 ,1028.4 ,757.7 ,14 ,29.93 \\r\\n251.4 ,0.0 ,118.3 ,188.5 ,5.8 ,1028.4 ,757.7 ,28 ,29.65 \\r\\n251.4 ,0.0 ,118.3 ,188.5 ,5.8 ,1028.4 ,757.7 ,56 ,36.97 \\r\\n251.4 ,0.0 ,118.3 ,188.5 ,5.8 ,1028.4 ,757.7 ,100 ,43.58 \\r\\n251.4 ,0.0 ,118.3 ,188.5 ,6.4 ,1028.4 ,757.7 ,3 ,13.12 \\r\\n251.4 ,0.0 ,118.3 ,188.5 ,6.4 ,1028.4 ,757.7 ,14 ,24.43 \\r\\n251.4 ,0.0 ,118.3 ,188.5 ,6.4 ,1028.4 ,757.7 ,28 ,32.66 \\r\\n251.4 ,0.0 ,118.3 ,188.5 ,6.4 ,1028.4 ,757.7 ,56 ,36.64 \\r\\n251.4 ,0.0 ,118.3 ,188.5 ,6.4 ,1028.4 ,757.7 ,100 ,44.21 \\r\\n181.4 ,0.0 ,167.0 ,169.6 ,7.6 ,1055.6 ,777.8 ,3 ,13.62 \\r\\n181.4 ,0.0 ,167.0 ,169.6 ,7.6 ,1055.6 ,777.8 ,14 ,21.60 \\r\\n181.4 ,0.0 ,167.0 ,169.6 ,7.6 ,1055.6 ,777.8 ,28 ,27.77 \\r\\n181.4 ,0.0 ,167.0 ,169.6 ,7.6 ,1055.6 ,777.8 ,56 ,35.57 \\r\\n181.4 ,0.0 ,167.0 ,169.6 ,7.6 ,1055.6 ,777.8 ,100 ,45.37 \\r\\n182.0 ,45.2 ,122.0 ,170.2 ,8.2 ,1059.4 ,780.7 ,3 ,7.32 \\r\\n182.0 ,45.2 ,122.0 ,170.2 ,8.2 ,1059.4 ,780.7 ,14 ,21.50 \\r\\n182.0 ,45.2 ,122.0 ,170.2 ,8.2 ,1059.4 ,780.7 ,28 ,31.27 \\r\\n182.0 ,45.2 ,122.0 ,170.2 ,8.2 ,1059.4 ,780.7 ,56 ,43.50 \\r\\n182.0 ,45.2 ,122.0 ,170.2 ,8.2 ,1059.4 ,780.7 ,100 ,48.67 \\r\\n168.9 ,42.2 ,124.3 ,158.3 ,10.8 ,1080.8 ,796.2 ,3 ,7.40 \\r\\n168.9 ,42.2 ,124.3 ,158.3 ,10.8 ,1080.8 ,796.2 ,14 ,23.51 \\r\\n168.9 ,42.2 ,124.3 ,158.3 ,10.8 ,1080.8 ,796.2 ,28 ,31.12 \\r\\n168.9 ,42.2 ,124.3 ,158.3 ,10.8 ,1080.8 ,796.2 ,56 ,39.15 \\r\\n168.9 ,42.2 ,124.3 ,158.3 ,10.8 ,1080.8 ,796.2 ,100 ,48.15 \\r\\n290.4 ,0.0 ,96.2 ,168.1 ,9.4 ,961.2 ,865.0 ,3 ,22.50 \\r\\n290.4 ,0.0 ,96.2 ,168.1 ,9.4 ,961.2 ,865.0 ,14 ,34.67 \\r\\n290.4 ,0.0 ,96.2 ,168.1 ,9.4 ,961.2 ,865.0 ,28 ,34.74 \\r\\n290.4 ,0.0 ,96.2 ,168.1 ,9.4 ,961.2 ,865.0 ,56 ,45.08 \\r\\n290.4 ,0.0 ,96.2 ,168.1 ,9.4 ,961.2 ,865.0 ,100 ,48.97 \\r\\n277.1 ,0.0 ,97.4 ,160.6 ,11.8 ,973.9 ,875.6 ,3 ,23.14 \\r\\n277.1 ,0.0 ,97.4 ,160.6 ,11.8 ,973.9 ,875.6 ,14 ,41.89 \\r\\n277.1 ,0.0 ,97.4 ,160.6 ,11.8 ,973.9 ,875.6 ,28 ,48.28 \\r\\n277.1 ,0.0 ,97.4 ,160.6 ,11.8 ,973.9 ,875.6 ,56 ,51.04 \\r\\n277.1 ,0.0 ,97.4 ,160.6 ,11.8 ,973.9 ,875.6 ,100 ,55.64 \\r\\n295.7 ,0.0 ,95.6 ,171.5 ,8.9 ,955.1 ,859.2 ,3 ,22.95 \\r\\n295.7 ,0.0 ,95.6 ,171.5 ,8.9 ,955.1 ,859.2 ,14 ,35.23 \\r\\n295.7 ,0.0 ,95.6 ,171.5 ,8.9 ,955.1 ,859.2 ,28 ,39.94 \\r\\n295.7 ,0.0 ,95.6 ,171.5 ,8.9 ,955.1 ,859.2 ,56 ,48.72 \\r\\n295.7 ,0.0 ,95.6 ,171.5 ,8.9 ,955.1 ,859.2 ,100 ,52.04 \\r\\n251.8 ,0.0 ,99.9 ,146.1 ,12.4 ,1006.0 ,899.8 ,3 ,21.02 \\r\\n251.8 ,0.0 ,99.9 ,146.1 ,12.4 ,1006.0 ,899.8 ,14 ,33.36 \\r\\n251.8 ,0.0 ,99.9 ,146.1 ,12.4 ,1006.0 ,899.8 ,28 ,33.94 \\r\\n251.8 ,0.0 ,99.9 ,146.1 ,12.4 ,1006.0 ,899.8 ,56 ,44.14 \\r\\n251.8 ,0.0 ,99.9 ,146.1 ,12.4 ,1006.0 ,899.8 ,100 ,45.37 \\r\\n249.1 ,0.0 ,98.8 ,158.1 ,12.8 ,987.8 ,889.0 ,3 ,15.36 \\r\\n249.1 ,0.0 ,98.8 ,158.1 ,12.8 ,987.8 ,889.0 ,14 ,28.68 \\r\\n249.1 ,0.0 ,98.8 ,158.1 ,12.8 ,987.8 ,889.0 ,28 ,30.85 \\r\\n249.1 ,0.0 ,98.8 ,158.1 ,12.8 ,987.8 ,889.0 ,56 ,42.03 \\r\\n249.1 ,0.0 ,98.8 ,158.1 ,12.8 ,987.8 ,889.0 ,100 ,51.06 \\r\\n252.3 ,0.0 ,98.8 ,146.3 ,14.2 ,987.8 ,889.0 ,3 ,21.78 \\r\\n252.3 ,0.0 ,98.8 ,146.3 ,14.2 ,987.8 ,889.0 ,14 ,42.29 \\r\\n252.3 ,0.0 ,98.8 ,146.3 ,14.2 ,987.8 ,889.0 ,28 ,50.60 \\r\\n252.3 ,0.0 ,98.8 ,146.3 ,14.2 ,987.8 ,889.0 ,56 ,55.83 \\r\\n252.3 ,0.0 ,98.8 ,146.3 ,14.2 ,987.8 ,889.0 ,100 ,60.95 \\r\\n246.8 ,0.0 ,125.1 ,143.3 ,12.0 ,1086.8 ,800.9 ,3 ,23.52 \\r\\n246.8 ,0.0 ,125.1 ,143.3 ,12.0 ,1086.8 ,800.9 ,14 ,42.22 \\r\\n246.8 ,0.0 ,125.1 ,143.3 ,12.0 ,1086.8 ,800.9 ,28 ,52.50 \\r\\n246.8 ,0.0 ,125.1 ,143.3 ,12.0 ,1086.8 ,800.9 ,56 ,60.32 \\r\\n246.8 ,0.0 ,125.1 ,143.3 ,12.0 ,1086.8 ,800.9 ,100 ,66.42 \\r\\n275.1 ,0.0 ,121.4 ,159.5 ,9.9 ,1053.6 ,777.5 ,3 ,23.80 \\r\\n275.1 ,0.0 ,121.4 ,159.5 ,9.9 ,1053.6 ,777.5 ,14 ,38.77 \\r\\n275.1 ,0.0 ,121.4 ,159.5 ,9.9 ,1053.6 ,777.5 ,28 ,51.33 \\r\\n275.1 ,0.0 ,121.4 ,159.5 ,9.9 ,1053.6 ,777.5 ,56 ,56.85 \\r\\n275.1 ,0.0 ,121.4 ,159.5 ,9.9 ,1053.6 ,777.5 ,100 ,58.61 \\r\\n297.2 ,0.0 ,117.5 ,174.8 ,9.5 ,1022.8 ,753.5 ,3 ,21.91 \\r\\n297.2 ,0.0 ,117.5 ,174.8 ,9.5 ,1022.8 ,753.5 ,14 ,36.99 \\r\\n297.2 ,0.0 ,117.5 ,174.8 ,9.5 ,1022.8 ,753.5 ,28 ,47.40 \\r\\n297.2 ,0.0 ,117.5 ,174.8 ,9.5 ,1022.8 ,753.5 ,56 ,51.96 \\r\\n297.2 ,0.0 ,117.5 ,174.8 ,9.5 ,1022.8 ,753.5 ,100 ,56.74 \\r\\n213.7 ,0.0 ,174.7 ,154.8 ,10.2 ,1053.5 ,776.4 ,3 ,17.57 \\r\\n213.7 ,0.0 ,174.7 ,154.8 ,10.2 ,1053.5 ,776.4 ,14 ,33.73 \\r\\n213.7 ,0.0 ,174.7 ,154.8 ,10.2 ,1053.5 ,776.4 ,28 ,40.15 \\r\\n213.7 ,0.0 ,174.7 ,154.8 ,10.2 ,1053.5 ,776.4 ,56 ,46.64 \\r\\n213.7 ,0.0 ,174.7 ,154.8 ,10.2 ,1053.5 ,776.4 ,100 ,50.08 \\r\\n213.5 ,0.0 ,174.2 ,154.6 ,11.7 ,1052.3 ,775.5 ,3 ,17.37 \\r\\n213.5 ,0.0 ,174.2 ,154.6 ,11.7 ,1052.3 ,775.5 ,14 ,33.70 \\r\\n213.5 ,0.0 ,174.2 ,154.6 ,11.7 ,1052.3 ,775.5 ,28 ,45.94 \\r\\n213.5 ,0.0 ,174.2 ,154.6 ,11.7 ,1052.3 ,775.5 ,56 ,51.43 \\r\\n213.5 ,0.0 ,174.2 ,154.6 ,11.7 ,1052.3 ,775.5 ,100 ,59.30 \\r\\n277.2 ,97.8 ,24.5 ,160.7 ,11.2 ,1061.7 ,782.5 ,3 ,30.45 \\r\\n277.2 ,97.8 ,24.5 ,160.7 ,11.2 ,1061.7 ,782.5 ,14 ,47.71 \\r\\n277.2 ,97.8 ,24.5 ,160.7 ,11.2 ,1061.7 ,782.5 ,28 ,63.14 \\r\\n277.2 ,97.8 ,24.5 ,160.7 ,11.2 ,1061.7 ,782.5 ,56 ,66.82 \\r\\n277.2 ,97.8 ,24.5 ,160.7 ,11.2 ,1061.7 ,782.5 ,100 ,66.95 \\r\\n218.2 ,54.6 ,123.8 ,140.8 ,11.9 ,1075.7 ,792.7 ,3 ,27.42 \\r\\n218.2 ,54.6 ,123.8 ,140.8 ,11.9 ,1075.7 ,792.7 ,14 ,35.96 \\r\\n218.2 ,54.6 ,123.8 ,140.8 ,11.9 ,1075.7 ,792.7 ,28 ,55.51 \\r\\n218.2 ,54.6 ,123.8 ,140.8 ,11.9 ,1075.7 ,792.7 ,56 ,61.99 \\r\\n218.2 ,54.6 ,123.8 ,140.8 ,11.9 ,1075.7 ,792.7 ,100 ,63.53 \\r\\n214.9 ,53.8 ,121.9 ,155.6 ,9.6 ,1014.3 ,780.6 ,3 ,18.02 \\r\\n214.9 ,53.8 ,121.9 ,155.6 ,9.6 ,1014.3 ,780.6 ,14 ,38.60 \\r\\n214.9 ,53.8 ,121.9 ,155.6 ,9.6 ,1014.3 ,780.6 ,28 ,52.20 \\r\\n214.9 ,53.8 ,121.9 ,155.6 ,9.6 ,1014.3 ,780.6 ,56 ,53.96 \\r\\n214.9 ,53.8 ,121.9 ,155.6 ,9.6 ,1014.3 ,780.6 ,100 ,56.63 \\r\\n218.9 ,0.0 ,124.1 ,158.5 ,11.3 ,1078.7 ,794.9 ,3 ,15.34 \\r\\n218.9 ,0.0 ,124.1 ,158.5 ,11.3 ,1078.7 ,794.9 ,14 ,26.05 \\r\\n218.9 ,0.0 ,124.1 ,158.5 ,11.3 ,1078.7 ,794.9 ,28 ,30.22 \\r\\n218.9 ,0.0 ,124.1 ,158.5 ,11.3 ,1078.7 ,794.9 ,56 ,37.27 \\r\\n218.9 ,0.0 ,124.1 ,158.5 ,11.3 ,1078.7 ,794.9 ,100 ,46.23 \\r\\n376.0 ,0.0 ,0.0 ,214.6 ,0.0 ,1003.5 ,762.4 ,3 ,16.28 \\r\\n376.0 ,0.0 ,0.0 ,214.6 ,0.0 ,1003.5 ,762.4 ,14 ,25.62 \\r\\n376.0 ,0.0 ,0.0 ,214.6 ,0.0 ,1003.5 ,762.4 ,28 ,31.97 \\r\\n376.0 ,0.0 ,0.0 ,214.6 ,0.0 ,1003.5 ,762.4 ,56 ,36.30 \\r\\n376.0 ,0.0 ,0.0 ,214.6 ,0.0 ,1003.5 ,762.4 ,100 ,43.06 \\r\\n500.0 ,0.0 ,0.0 ,140.0 ,4.0 ,966.0 ,853.0 ,28 ,67.57 \\r\\n475.0 ,0.0 ,59.0 ,142.0 ,1.9 ,1098.0 ,641.0 ,28 ,57.23 \\r\\n315.0 ,137.0 ,0.0 ,145.0 ,5.9 ,1130.0 ,745.0 ,28 ,81.75 \\r\\n505.0 ,0.0 ,60.0 ,195.0 ,0.0 ,1030.0 ,630.0 ,28 ,64.02 \\r\\n451.0 ,0.0 ,0.0 ,165.0 ,11.3 ,1030.0 ,745.0 ,28 ,78.80 \\r\\n516.0 ,0.0 ,0.0 ,162.0 ,8.2 ,801.0 ,802.0 ,28 ,41.37 \\r\\n520.0 ,0.0 ,0.0 ,170.0 ,5.2 ,855.0 ,855.0 ,28 ,60.28 \\r\\n528.0 ,0.0 ,0.0 ,185.0 ,6.9 ,920.0 ,720.0 ,28 ,56.83 \\r\\n520.0 ,0.0 ,0.0 ,175.0 ,5.2 ,870.0 ,805.0 ,28 ,51.02 \\r\\n385.0 ,0.0 ,136.0 ,158.0 ,20.0 ,903.0 ,768.0 ,28 ,55.55 \\r\\n500.1 ,0.0 ,0.0 ,200.0 ,3.0 ,1124.4 ,613.2 ,28 ,44.13 \\r\\n450.1 ,50.0 ,0.0 ,200.0 ,3.0 ,1124.4 ,613.2 ,28 ,39.38 \\r\\n397.0 ,17.2 ,158.0 ,167.0 ,20.8 ,967.0 ,633.0 ,28 ,55.65 \\r\\n333.0 ,17.5 ,163.0 ,167.0 ,17.9 ,996.0 ,652.0 ,28 ,47.28 \\r\\n334.0 ,17.6 ,158.0 ,189.0 ,15.3 ,967.0 ,633.0 ,28 ,44.33 \\r\\n405.0 ,0.0 ,0.0 ,175.0 ,0.0 ,1120.0 ,695.0 ,28 ,52.30 \\r\\n200.0 ,200.0 ,0.0 ,190.0 ,0.0 ,1145.0 ,660.0 ,28 ,49.25 \\r\\n516.0 ,0.0 ,0.0 ,162.0 ,8.3 ,801.0 ,802.0 ,28 ,41.37 \\r\\n145.0 ,116.0 ,119.0 ,184.0 ,5.7 ,833.0 ,880.0 ,28 ,29.16 \\r\\n160.0 ,128.0 ,122.0 ,182.0 ,6.4 ,824.0 ,879.0 ,28 ,39.40 \\r\\n234.0 ,156.0 ,0.0 ,189.0 ,5.9 ,981.0 ,760.0 ,28 ,39.30 \\r\\n250.0 ,180.0 ,95.0 ,159.0 ,9.5 ,860.0 ,800.0 ,28 ,67.87 \\r\\n475.0 ,0.0 ,0.0 ,162.0 ,9.5 ,1044.0 ,662.0 ,28 ,58.52 \\r\\n285.0 ,190.0 ,0.0 ,163.0 ,7.6 ,1031.0 ,685.0 ,28 ,53.58 \\r\\n356.0 ,119.0 ,0.0 ,160.0 ,9.0 ,1061.0 ,657.0 ,28 ,59.00 \\r\\n275.0 ,180.0 ,120.0 ,162.0 ,10.4 ,830.0 ,765.0 ,28 ,76.24 \\r\\n500.0 ,0.0 ,0.0 ,151.0 ,9.0 ,1033.0 ,655.0 ,28 ,69.84 \\r\\n165.0 ,0.0 ,143.6 ,163.8 ,0.0 ,1005.6 ,900.9 ,3 ,14.40 \\r\\n165.0 ,128.5 ,132.1 ,175.1 ,8.1 ,1005.8 ,746.6 ,3 ,19.42 \\r\\n178.0 ,129.8 ,118.6 ,179.9 ,3.6 ,1007.3 ,746.8 ,3 ,20.73 \\r\\n167.4 ,129.9 ,128.6 ,175.5 ,7.8 ,1006.3 ,746.6 ,3 ,14.94 \\r\\n172.4 ,13.6 ,172.4 ,156.8 ,4.1 ,1006.3 ,856.4 ,3 ,21.29 \\r\\n173.5 ,50.1 ,173.5 ,164.8 ,6.5 ,1006.2 ,793.5 ,3 ,23.08 \\r\\n167.0 ,75.4 ,167.0 ,164.0 ,7.9 ,1007.3 ,770.1 ,3 ,15.52 \\r\\n173.8 ,93.4 ,159.9 ,172.3 ,9.7 ,1007.2 ,746.6 ,3 ,15.82 \\r\\n190.3 ,0.0 ,125.2 ,166.6 ,9.9 ,1079.0 ,798.9 ,3 ,12.55 \\r\\n250.0 ,0.0 ,95.7 ,191.8 ,5.3 ,948.9 ,857.2 ,3 ,8.49 \\r\\n213.5 ,0.0 ,174.2 ,159.2 ,11.7 ,1043.6 ,771.9 ,3 ,15.61 \\r\\n194.7 ,0.0 ,100.5 ,170.2 ,7.5 ,998.0 ,901.8 ,3 ,12.18 \\r\\n251.4 ,0.0 ,118.3 ,192.9 ,5.8 ,1043.6 ,754.3 ,3 ,11.98 \\r\\n165.0 ,0.0 ,143.6 ,163.8 ,0.0 ,1005.6 ,900.9 ,14 ,16.88 \\r\\n165.0 ,128.5 ,132.1 ,175.1 ,8.1 ,1005.8 ,746.6 ,14 ,33.09 \\r\\n178.0 ,129.8 ,118.6 ,179.9 ,3.6 ,1007.3 ,746.8 ,14 ,34.24 \\r\\n167.4 ,129.9 ,128.6 ,175.5 ,7.8 ,1006.3 ,746.6 ,14 ,31.81 \\r\\n172.4 ,13.6 ,172.4 ,156.8 ,4.1 ,1006.3 ,856.4 ,14 ,29.75 \\r\\n173.5 ,50.1 ,173.5 ,164.8 ,6.5 ,1006.2 ,793.5 ,14 ,33.01 \\r\\n167.0 ,75.4 ,167.0 ,164.0 ,7.9 ,1007.3 ,770.1 ,14 ,32.90 \\r\\n173.8 ,93.4 ,159.9 ,172.3 ,9.7 ,1007.2 ,746.6 ,14 ,29.55 \\r\\n190.3 ,0.0 ,125.2 ,166.6 ,9.9 ,1079.0 ,798.9 ,14 ,19.42 \\r\\n250.0 ,0.0 ,95.7 ,191.8 ,5.3 ,948.9 ,857.2 ,14 ,24.66 \\r\\n213.5 ,0.0 ,174.2 ,159.2 ,11.7 ,1043.6 ,771.9 ,14 ,29.59 \\r\\n194.7 ,0.0 ,100.5 ,170.2 ,7.5 ,998.0 ,901.8 ,14 ,24.28 \\r\\n251.4 ,0.0 ,118.3 ,192.9 ,5.8 ,1043.6 ,754.3 ,14 ,20.73 \\r\\n165.0 ,0.0 ,143.6 ,163.8 ,0.0 ,1005.6 ,900.9 ,28 ,26.20 \\r\\n165.0 ,128.5 ,132.1 ,175.1 ,8.1 ,1005.8 ,746.6 ,28 ,46.39 \\r\\n178.0 ,129.8 ,118.6 ,179.9 ,3.6 ,1007.3 ,746.8 ,28 ,39.16 \\r\\n167.4 ,129.9 ,128.6 ,175.5 ,7.8 ,1006.3 ,746.6 ,28 ,41.20 \\r\\n172.4 ,13.6 ,172.4 ,156.8 ,4.1 ,1006.3 ,856.4 ,28 ,33.69 \\r\\n173.5 ,50.1 ,173.5 ,164.8 ,6.5 ,1006.2 ,793.5 ,28 ,38.20 \\r\\n167.0 ,75.4 ,167.0 ,164.0 ,7.9 ,1007.3 ,770.1 ,28 ,41.41 \\r\\n173.8 ,93.4 ,159.9 ,172.3 ,9.7 ,1007.2 ,746.6 ,28 ,37.81 \\r\\n190.3 ,0.0 ,125.2 ,166.6 ,9.9 ,1079.0 ,798.9 ,28 ,24.85 \\r\\n250.0 ,0.0 ,95.7 ,191.8 ,5.3 ,948.9 ,857.2 ,28 ,27.22 \\r\\n213.5 ,0.0 ,174.2 ,159.2 ,11.7 ,1043.6 ,771.9 ,28 ,44.64 \\r\\n194.7 ,0.0 ,100.5 ,170.2 ,7.5 ,998.0 ,901.8 ,28 ,37.27 \\r\\n251.4 ,0.0 ,118.3 ,192.9 ,5.8 ,1043.6 ,754.3 ,28 ,33.27 \\r\\n165.0 ,0.0 ,143.6 ,163.8 ,0.0 ,1005.6 ,900.9 ,56 ,36.56 \\r\\n165.0 ,128.5 ,132.1 ,175.1 ,8.1 ,1005.8 ,746.6 ,56 ,53.72 \\r\\n178.0 ,129.8 ,118.6 ,179.9 ,3.6 ,1007.3 ,746.8 ,56 ,48.59 \\r\\n167.4 ,129.9 ,128.6 ,175.5 ,7.8 ,1006.3 ,746.6 ,56 ,51.72 \\r\\n172.4 ,13.6 ,172.4 ,156.8 ,4.1 ,1006.3 ,856.4 ,56 ,35.85 \\r\\n173.5 ,50.1 ,173.5 ,164.8 ,6.5 ,1006.2 ,793.5 ,56 ,53.77 \\r\\n167.0 ,75.4 ,167.0 ,164.0 ,7.9 ,1007.3 ,770.1 ,56 ,53.46 \\r\\n173.8 ,93.4 ,159.9 ,172.3 ,9.7 ,1007.2 ,746.6 ,56 ,48.99 \\r\\n190.3 ,0.0 ,125.2 ,166.6 ,9.9 ,1079.0 ,798.9 ,56 ,31.72 \\r\\n250.0 ,0.0 ,95.7 ,191.8 ,5.3 ,948.9 ,857.2 ,56 ,39.64 \\r\\n213.5 ,0.0 ,174.2 ,159.2 ,11.7 ,1043.6 ,771.9 ,56 ,51.26 \\r\\n194.7 ,0.0 ,100.5 ,170.2 ,7.5 ,998.0 ,901.8 ,56 ,43.39 \\r\\n251.4 ,0.0 ,118.3 ,192.9 ,5.8 ,1043.6 ,754.3 ,56 ,39.27 \\r\\n165.0 ,0.0 ,143.6 ,163.8 ,0.0 ,1005.6 ,900.9 ,100 ,37.96 \\r\\n165.0 ,128.5 ,132.1 ,175.1 ,8.1 ,1005.8 ,746.6 ,100 ,55.02 \\r\\n178.0 ,129.8 ,118.6 ,179.9 ,3.6 ,1007.3 ,746.8 ,100 ,49.99 \\r\\n167.4 ,129.9 ,128.6 ,175.5 ,7.8 ,1006.3 ,746.6 ,100 ,53.66 \\r\\n172.4 ,13.6 ,172.4 ,156.8 ,4.1 ,1006.3 ,856.4 ,100 ,37.68 \\r\\n173.5 ,50.1 ,173.5 ,164.8 ,6.5 ,1006.2 ,793.5 ,100 ,56.06 \\r\\n167.0 ,75.4 ,167.0 ,164.0 ,7.9 ,1007.3 ,770.1 ,100 ,56.81 \\r\\n173.8 ,93.4 ,159.9 ,172.3 ,9.7 ,1007.2 ,746.6 ,100 ,50.94 \\r\\n190.3 ,0.0 ,125.2 ,166.6 ,9.9 ,1079.0 ,798.9 ,100 ,33.56 \\r\\n250.0 ,0.0 ,95.7 ,191.8 ,5.3 ,948.9 ,857.2 ,100 ,41.16 \\r\\n213.5 ,0.0 ,174.2 ,159.2 ,11.7 ,1043.6 ,771.9 ,100 ,52.96 \\r\\n194.7 ,0.0 ,100.5 ,170.2 ,7.5 ,998.0 ,901.8 ,100 ,44.28 \\r\\n251.4 ,0.0 ,118.3 ,192.9 ,5.8 ,1043.6 ,754.3 ,100 ,40.15 \\r\\n446.0 ,24.0 ,79.0 ,162.0 ,11.6 ,967.0 ,712.0 ,28 ,57.03 \\r\\n446.0 ,24.0 ,79.0 ,162.0 ,11.6 ,967.0 ,712.0 ,28 ,44.42 \\r\\n446.0 ,24.0 ,79.0 ,162.0 ,11.6 ,967.0 ,712.0 ,28 ,51.02 \\r\\n446.0 ,24.0 ,79.0 ,162.0 ,10.3 ,967.0 ,712.0 ,28 ,53.39 \\r\\n446.0 ,24.0 ,79.0 ,162.0 ,11.6 ,967.0 ,712.0 ,3 ,35.36 \\r\\n446.0 ,24.0 ,79.0 ,162.0 ,11.6 ,967.0 ,712.0 ,3 ,25.02 \\r\\n446.0 ,24.0 ,79.0 ,162.0 ,11.6 ,967.0 ,712.0 ,3 ,23.35 \\r\\n446.0 ,24.0 ,79.0 ,162.0 ,11.6 ,967.0 ,712.0 ,7 ,52.01 \\r\\n446.0 ,24.0 ,79.0 ,162.0 ,11.6 ,967.0 ,712.0 ,7 ,38.02 \\r\\n446.0 ,24.0 ,79.0 ,162.0 ,11.6 ,967.0 ,712.0 ,7 ,39.30 \\r\\n446.0 ,24.0 ,79.0 ,162.0 ,11.6 ,967.0 ,712.0 ,56 ,61.07 \\r\\n446.0 ,24.0 ,79.0 ,162.0 ,11.6 ,967.0 ,712.0 ,56 ,56.14 \\r\\n446.0 ,24.0 ,79.0 ,162.0 ,11.6 ,967.0 ,712.0 ,56 ,55.25 \\r\\n446.0 ,24.0 ,79.0 ,162.0 ,10.3 ,967.0 ,712.0 ,56 ,54.77 \\r\\n387.0 ,20.0 ,94.0 ,157.0 ,14.3 ,938.0 ,845.0 ,28 ,50.24 \\r\\n387.0 ,20.0 ,94.0 ,157.0 ,13.9 ,938.0 ,845.0 ,28 ,46.68 \\r\\n387.0 ,20.0 ,94.0 ,157.0 ,11.6 ,938.0 ,845.0 ,28 ,46.68 \\r\\n387.0 ,20.0 ,94.0 ,157.0 ,14.3 ,938.0 ,845.0 ,3 ,22.75 \\r\\n387.0 ,20.0 ,94.0 ,157.0 ,13.9 ,938.0 ,845.0 ,3 ,25.51 \\r\\n387.0 ,20.0 ,94.0 ,157.0 ,11.6 ,938.0 ,845.0 ,3 ,34.77 \\r\\n387.0 ,20.0 ,94.0 ,157.0 ,14.3 ,938.0 ,845.0 ,7 ,36.84 \\r\\n387.0 ,20.0 ,94.0 ,157.0 ,13.9 ,938.0 ,845.0 ,7 ,45.90 \\r\\n387.0 ,20.0 ,94.0 ,157.0 ,11.6 ,938.0 ,845.0 ,7 ,41.67 \\r\\n387.0 ,20.0 ,94.0 ,157.0 ,14.3 ,938.0 ,845.0 ,56 ,56.34 \\r\\n387.0 ,20.0 ,94.0 ,157.0 ,13.9 ,938.0 ,845.0 ,56 ,47.97 \\r\\n387.0 ,20.0 ,94.0 ,157.0 ,11.6 ,938.0 ,845.0 ,56 ,61.46 \\r\\n355.0 ,19.0 ,97.0 ,145.0 ,13.1 ,967.0 ,871.0 ,28 ,44.03 \\r\\n355.0 ,19.0 ,97.0 ,145.0 ,12.3 ,967.0 ,871.0 ,28 ,55.45 \\r\\n491.0 ,26.0 ,123.0 ,210.0 ,3.9 ,882.0 ,699.0 ,28 ,55.55 \\r\\n491.0 ,26.0 ,123.0 ,201.0 ,3.9 ,822.0 ,699.0 ,28 ,57.92 \\r\\n491.0 ,26.0 ,123.0 ,210.0 ,3.9 ,882.0 ,699.0 ,3 ,25.61 \\r\\n491.0 ,26.0 ,123.0 ,210.0 ,3.9 ,882.0 ,699.0 ,7 ,33.49 \\r\\n491.0 ,26.0 ,123.0 ,210.0 ,3.9 ,882.0 ,699.0 ,56 ,59.59 \\r\\n491.0 ,26.0 ,123.0 ,201.0 ,3.9 ,822.0 ,699.0 ,3 ,29.55 \\r\\n491.0 ,26.0 ,123.0 ,201.0 ,3.9 ,822.0 ,699.0 ,7 ,37.92 \\r\\n491.0 ,26.0 ,123.0 ,201.0 ,3.9 ,822.0 ,699.0 ,56 ,61.86 \\r\\n424.0 ,22.0 ,132.0 ,178.0 ,8.5 ,822.0 ,750.0 ,28 ,62.05 \\r\\n424.0 ,22.0 ,132.0 ,178.0 ,8.5 ,882.0 ,750.0 ,3 ,32.01 \\r\\n424.0 ,22.0 ,132.0 ,168.0 ,8.9 ,822.0 ,750.0 ,28 ,72.10 \\r\\n424.0 ,22.0 ,132.0 ,178.0 ,8.5 ,822.0 ,750.0 ,7 ,39.00 \\r\\n424.0 ,22.0 ,132.0 ,178.0 ,8.5 ,822.0 ,750.0 ,56 ,65.70 \\r\\n424.0 ,22.0 ,132.0 ,168.0 ,8.9 ,822.0 ,750.0 ,3 ,32.11 \\r\\n424.0 ,22.0 ,132.0 ,168.0 ,8.9 ,822.0 ,750.0 ,7 ,40.29 \\r\\n424.0 ,22.0 ,132.0 ,168.0 ,8.9 ,822.0 ,750.0 ,56 ,74.36 \\r\\n202.0 ,11.0 ,141.0 ,206.0 ,1.7 ,942.0 ,801.0 ,28 ,21.97 \\r\\n202.0 ,11.0 ,141.0 ,206.0 ,1.7 ,942.0 ,801.0 ,3 ,9.85 \\r\\n202.0 ,11.0 ,141.0 ,206.0 ,1.7 ,942.0 ,801.0 ,7 ,15.07 \\r\\n202.0 ,11.0 ,141.0 ,206.0 ,1.7 ,942.0 ,801.0 ,56 ,23.25 \\r\\n284.0 ,15.0 ,141.0 ,179.0 ,5.5 ,842.0 ,801.0 ,28 ,43.73 \\r\\n284.0 ,15.0 ,141.0 ,179.0 ,5.5 ,842.0 ,801.0 ,3 ,13.40 \\r\\n284.0 ,15.0 ,141.0 ,179.0 ,5.5 ,842.0 ,801.0 ,7 ,24.13 \\r\\n284.0 ,15.0 ,141.0 ,179.0 ,5.5 ,842.0 ,801.0 ,56 ,44.52 \\r\\n359.0 ,19.0 ,141.0 ,154.0 ,10.9 ,942.0 ,801.0 ,28 ,62.94 \\r\\n359.0 ,19.0 ,141.0 ,154.0 ,10.9 ,942.0 ,801.0 ,28 ,59.49 \\r\\n359.0 ,19.0 ,141.0 ,154.0 ,10.9 ,942.0 ,801.0 ,3 ,25.12 \\r\\n359.0 ,19.0 ,141.0 ,154.0 ,10.9 ,942.0 ,801.0 ,3 ,23.64 \\r\\n359.0 ,19.0 ,141.0 ,154.0 ,10.9 ,942.0 ,801.0 ,7 ,35.75 \\r\\n359.0 ,19.0 ,141.0 ,154.0 ,10.9 ,942.0 ,801.0 ,7 ,38.61 \\r\\n359.0 ,19.0 ,141.0 ,154.0 ,10.9 ,942.0 ,801.0 ,56 ,68.75 \\r\\n359.0 ,19.0 ,141.0 ,154.0 ,10.9 ,942.0 ,801.0 ,56 ,66.78 \\r\\n436.0 ,0.0 ,0.0 ,218.0 ,0.0 ,838.4 ,719.7 ,28 ,23.85 \\r\\n289.0 ,0.0 ,0.0 ,192.0 ,0.0 ,913.2 ,895.3 ,90 ,32.07 \\r\\n289.0 ,0.0 ,0.0 ,192.0 ,0.0 ,913.2 ,895.3 ,3 ,11.65 \\r\\n393.0 ,0.0 ,0.0 ,192.0 ,0.0 ,940.6 ,785.6 ,3 ,19.20 \\r\\n393.0 ,0.0 ,0.0 ,192.0 ,0.0 ,940.6 ,785.6 ,90 ,48.85 \\r\\n393.0 ,0.0 ,0.0 ,192.0 ,0.0 ,940.6 ,785.6 ,28 ,39.60 \\r\\n480.0 ,0.0 ,0.0 ,192.0 ,0.0 ,936.2 ,712.2 ,28 ,43.94 \\r\\n480.0 ,0.0 ,0.0 ,192.0 ,0.0 ,936.2 ,712.2 ,7 ,34.57 \\r\\n480.0 ,0.0 ,0.0 ,192.0 ,0.0 ,936.2 ,712.2 ,90 ,54.32 \\r\\n480.0 ,0.0 ,0.0 ,192.0 ,0.0 ,936.2 ,712.2 ,3 ,24.40 \\r\\n333.0 ,0.0 ,0.0 ,192.0 ,0.0 ,931.2 ,842.6 ,3 ,15.62 \\r\\n255.0 ,0.0 ,0.0 ,192.0 ,0.0 ,889.8 ,945.0 ,90 ,21.86 \\r\\n255.0 ,0.0 ,0.0 ,192.0 ,0.0 ,889.8 ,945.0 ,7 ,10.22 \\r\\n289.0 ,0.0 ,0.0 ,192.0 ,0.0 ,913.2 ,895.3 ,7 ,14.60 \\r\\n255.0 ,0.0 ,0.0 ,192.0 ,0.0 ,889.8 ,945.0 ,28 ,18.75 \\r\\n333.0 ,0.0 ,0.0 ,192.0 ,0.0 ,931.2 ,842.6 ,28 ,31.97 \\r\\n333.0 ,0.0 ,0.0 ,192.0 ,0.0 ,931.2 ,842.6 ,7 ,23.40 \\r\\n289.0 ,0.0 ,0.0 ,192.0 ,0.0 ,913.2 ,895.3 ,28 ,25.57 \\r\\n333.0 ,0.0 ,0.0 ,192.0 ,0.0 ,931.2 ,842.6 ,90 ,41.68 \\r\\n393.0 ,0.0 ,0.0 ,192.0 ,0.0 ,940.6 ,785.6 ,7 ,27.74 \\r\\n255.0 ,0.0 ,0.0 ,192.0 ,0.0 ,889.8 ,945.0 ,3 ,8.20 \\r\\n158.8 ,238.2 ,0.0 ,185.7 ,0.0 ,1040.6 ,734.3 ,7 ,9.62 \\r\\n239.6 ,359.4 ,0.0 ,185.7 ,0.0 ,941.6 ,664.3 ,7 ,25.42 \\r\\n238.2 ,158.8 ,0.0 ,185.7 ,0.0 ,1040.6 ,734.3 ,7 ,15.69 \\r\\n181.9 ,272.8 ,0.0 ,185.7 ,0.0 ,1012.4 ,714.3 ,28 ,27.94 \\r\\n193.5 ,290.2 ,0.0 ,185.7 ,0.0 ,998.2 ,704.3 ,28 ,32.63 \\r\\n255.5 ,170.3 ,0.0 ,185.7 ,0.0 ,1026.6 ,724.3 ,7 ,17.24 \\r\\n272.8 ,181.9 ,0.0 ,185.7 ,0.0 ,1012.4 ,714.3 ,7 ,19.77 \\r\\n239.6 ,359.4 ,0.0 ,185.7 ,0.0 ,941.6 ,664.3 ,28 ,39.44 \\r\\n220.8 ,147.2 ,0.0 ,185.7 ,0.0 ,1055.0 ,744.3 ,28 ,25.75 \\r\\n397.0 ,0.0 ,0.0 ,185.7 ,0.0 ,1040.6 ,734.3 ,28 ,33.08 \\r\\n382.5 ,0.0 ,0.0 ,185.7 ,0.0 ,1047.8 ,739.3 ,7 ,24.07 \\r\\n210.7 ,316.1 ,0.0 ,185.7 ,0.0 ,977.0 ,689.3 ,7 ,21.82 \\r\\n158.8 ,238.2 ,0.0 ,185.7 ,0.0 ,1040.6 ,734.3 ,28 ,21.07 \\r\\n295.8 ,0.0 ,0.0 ,185.7 ,0.0 ,1091.4 ,769.3 ,7 ,14.84 \\r\\n255.5 ,170.3 ,0.0 ,185.7 ,0.0 ,1026.6 ,724.3 ,28 ,32.05 \\r\\n203.5 ,135.7 ,0.0 ,185.7 ,0.0 ,1076.2 ,759.3 ,7 ,11.96 \\r\\n397.0 ,0.0 ,0.0 ,185.7 ,0.0 ,1040.6 ,734.3 ,7 ,25.45 \\r\\n381.4 ,0.0 ,0.0 ,185.7 ,0.0 ,1104.6 ,784.3 ,28 ,22.49 \\r\\n295.8 ,0.0 ,0.0 ,185.7 ,0.0 ,1091.4 ,769.3 ,28 ,25.22 \\r\\n228.0 ,342.1 ,0.0 ,185.7 ,0.0 ,955.8 ,674.3 ,28 ,39.70 \\r\\n220.8 ,147.2 ,0.0 ,185.7 ,0.0 ,1055.0 ,744.3 ,7 ,13.09 \\r\\n316.1 ,210.7 ,0.0 ,185.7 ,0.0 ,977.0 ,689.3 ,28 ,38.70 \\r\\n135.7 ,203.5 ,0.0 ,185.7 ,0.0 ,1076.2 ,759.3 ,7 ,7.51 \\r\\n238.1 ,0.0 ,0.0 ,185.7 ,0.0 ,1118.8 ,789.3 ,28 ,17.58 \\r\\n339.2 ,0.0 ,0.0 ,185.7 ,0.0 ,1069.2 ,754.3 ,7 ,21.18 \\r\\n135.7 ,203.5 ,0.0 ,185.7 ,0.0 ,1076.2 ,759.3 ,28 ,18.20 \\r\\n193.5 ,290.2 ,0.0 ,185.7 ,0.0 ,998.2 ,704.3 ,7 ,17.20 \\r\\n203.5 ,135.7 ,0.0 ,185.7 ,0.0 ,1076.2 ,759.3 ,28 ,22.63 \\r\\n290.2 ,193.5 ,0.0 ,185.7 ,0.0 ,998.2 ,704.3 ,7 ,21.86 \\r\\n181.9 ,272.8 ,0.0 ,185.7 ,0.0 ,1012.4 ,714.3 ,7 ,12.37 \\r\\n170.3 ,155.5 ,0.0 ,185.7 ,0.0 ,1026.6 ,724.3 ,28 ,25.73 \\r\\n210.7 ,316.1 ,0.0 ,185.7 ,0.0 ,977.0 ,689.3 ,28 ,37.81 \\r\\n228.0 ,342.1 ,0.0 ,185.7 ,0.0 ,955.8 ,674.3 ,7 ,21.92 \\r\\n290.2 ,193.5 ,0.0 ,185.7 ,0.0 ,998.2 ,704.3 ,28 ,33.04 \\r\\n381.4 ,0.0 ,0.0 ,185.7 ,0.0 ,1104.6 ,784.3 ,7 ,14.54 \\r\\n238.2 ,158.8 ,0.0 ,185.7 ,0.0 ,1040.6 ,734.3 ,28 ,26.91 \\r\\n186.2 ,124.1 ,0.0 ,185.7 ,0.0 ,1083.4 ,764.3 ,7 ,8.00 \\r\\n339.2 ,0.0 ,0.0 ,185.7 ,0.0 ,1069.2 ,754.3 ,28 ,31.90 \\r\\n238.1 ,0.0 ,0.0 ,185.7 ,0.0 ,1118.8 ,789.3 ,7 ,10.34 \\r\\n252.5 ,0.0 ,0.0 ,185.7 ,0.0 ,1111.6 ,784.3 ,28 ,19.77 \\r\\n382.5 ,0.0 ,0.0 ,185.7 ,0.0 ,1047.8 ,739.3 ,28 ,37.44 \\r\\n252.5 ,0.0 ,0.0 ,185.7 ,0.0 ,1111.6 ,784.3 ,7 ,11.48 \\r\\n316.1 ,210.7 ,0.0 ,185.7 ,0.0 ,977.0 ,689.3 ,7 ,24.44 \\r\\n186.2 ,124.1 ,0.0 ,185.7 ,0.0 ,1083.4 ,764.3 ,28 ,17.60 \\r\\n170.3 ,155.5 ,0.0 ,185.7 ,0.0 ,1026.6 ,724.3 ,7 ,10.73 \\r\\n272.8 ,181.9 ,0.0 ,185.7 ,0.0 ,1012.4 ,714.3 ,28 ,31.38 \\r\\n339.0 ,0.0 ,0.0 ,197.0 ,0.0 ,968.0 ,781.0 ,3 ,13.22 \\r\\n339.0 ,0.0 ,0.0 ,197.0 ,0.0 ,968.0 ,781.0 ,7 ,20.97 \\r\\n339.0 ,0.0 ,0.0 ,197.0 ,0.0 ,968.0 ,781.0 ,14 ,27.04 \\r\\n339.0 ,0.0 ,0.0 ,197.0 ,0.0 ,968.0 ,781.0 ,28 ,32.04 \\r\\n339.0 ,0.0 ,0.0 ,197.0 ,0.0 ,968.0 ,781.0 ,90 ,35.17 \\r\\n339.0 ,0.0 ,0.0 ,197.0 ,0.0 ,968.0 ,781.0 ,180 ,36.45 \\r\\n339.0 ,0.0 ,0.0 ,197.0 ,0.0 ,968.0 ,781.0 ,365 ,38.89 \\r\\n236.0 ,0.0 ,0.0 ,194.0 ,0.0 ,968.0 ,885.0 ,3 ,6.47 \\r\\n236.0 ,0.0 ,0.0 ,194.0 ,0.0 ,968.0 ,885.0 ,14 ,12.84 \\r\\n236.0 ,0.0 ,0.0 ,194.0 ,0.0 ,968.0 ,885.0 ,28 ,18.42 \\r\\n236.0 ,0.0 ,0.0 ,194.0 ,0.0 ,968.0 ,885.0 ,90 ,21.95 \\r\\n236.0 ,0.0 ,0.0 ,193.0 ,0.0 ,968.0 ,885.0 ,180 ,24.10 \\r\\n236.0 ,0.0 ,0.0 ,193.0 ,0.0 ,968.0 ,885.0 ,365 ,25.08 \\r\\n277.0 ,0.0 ,0.0 ,191.0 ,0.0 ,968.0 ,856.0 ,14 ,21.26 \\r\\n277.0 ,0.0 ,0.0 ,191.0 ,0.0 ,968.0 ,856.0 ,28 ,25.97 \\r\\n277.0 ,0.0 ,0.0 ,191.0 ,0.0 ,968.0 ,856.0 ,3 ,11.36 \\r\\n277.0 ,0.0 ,0.0 ,191.0 ,0.0 ,968.0 ,856.0 ,90 ,31.25 \\r\\n277.0 ,0.0 ,0.0 ,191.0 ,0.0 ,968.0 ,856.0 ,180 ,32.33 \\r\\n277.0 ,0.0 ,0.0 ,191.0 ,0.0 ,968.0 ,856.0 ,360 ,33.70 \\r\\n254.0 ,0.0 ,0.0 ,198.0 ,0.0 ,968.0 ,863.0 ,3 ,9.31 \\r\\n254.0 ,0.0 ,0.0 ,198.0 ,0.0 ,968.0 ,863.0 ,90 ,26.94 \\r\\n254.0 ,0.0 ,0.0 ,198.0 ,0.0 ,968.0 ,863.0 ,180 ,27.63 \\r\\n254.0 ,0.0 ,0.0 ,198.0 ,0.0 ,968.0 ,863.0 ,365 ,29.79 \\r\\n307.0 ,0.0 ,0.0 ,193.0 ,0.0 ,968.0 ,812.0 ,180 ,34.49 \\r\\n307.0 ,0.0 ,0.0 ,193.0 ,0.0 ,968.0 ,812.0 ,365 ,36.15 \\r\\n307.0 ,0.0 ,0.0 ,193.0 ,0.0 ,968.0 ,812.0 ,3 ,12.54 \\r\\n307.0 ,0.0 ,0.0 ,193.0 ,0.0 ,968.0 ,812.0 ,28 ,27.53 \\r\\n307.0 ,0.0 ,0.0 ,193.0 ,0.0 ,968.0 ,812.0 ,90 ,32.92 \\r\\n236.0 ,0.0 ,0.0 ,193.0 ,0.0 ,968.0 ,885.0 ,7 ,9.99 \\r\\n200.0 ,0.0 ,0.0 ,180.0 ,0.0 ,1125.0 ,845.0 ,7 ,7.84 \\r\\n200.0 ,0.0 ,0.0 ,180.0 ,0.0 ,1125.0 ,845.0 ,28 ,12.25 \\r\\n225.0 ,0.0 ,0.0 ,181.0 ,0.0 ,1113.0 ,833.0 ,7 ,11.17 \\r\\n225.0 ,0.0 ,0.0 ,181.0 ,0.0 ,1113.0 ,833.0 ,28 ,17.34 \\r\\n325.0 ,0.0 ,0.0 ,184.0 ,0.0 ,1063.0 ,783.0 ,7 ,17.54 \\r\\n325.0 ,0.0 ,0.0 ,184.0 ,0.0 ,1063.0 ,783.0 ,28 ,30.57 \\r\\n275.0 ,0.0 ,0.0 ,183.0 ,0.0 ,1088.0 ,808.0 ,7 ,14.20 \\r\\n275.0 ,0.0 ,0.0 ,183.0 ,0.0 ,1088.0 ,808.0 ,28 ,24.50 \\r\\n300.0 ,0.0 ,0.0 ,184.0 ,0.0 ,1075.0 ,795.0 ,7 ,15.58 \\r\\n300.0 ,0.0 ,0.0 ,184.0 ,0.0 ,1075.0 ,795.0 ,28 ,26.85 \\r\\n375.0 ,0.0 ,0.0 ,186.0 ,0.0 ,1038.0 ,758.0 ,7 ,26.06 \\r\\n375.0 ,0.0 ,0.0 ,186.0 ,0.0 ,1038.0 ,758.0 ,28 ,38.21 \\r\\n400.0 ,0.0 ,0.0 ,187.0 ,0.0 ,1025.0 ,745.0 ,28 ,43.70 \\r\\n400.0 ,0.0 ,0.0 ,187.0 ,0.0 ,1025.0 ,745.0 ,7 ,30.14 \\r\\n250.0 ,0.0 ,0.0 ,182.0 ,0.0 ,1100.0 ,820.0 ,7 ,12.73 \\r\\n250.0 ,0.0 ,0.0 ,182.0 ,0.0 ,1100.0 ,820.0 ,28 ,20.87 \\r\\n350.0 ,0.0 ,0.0 ,186.0 ,0.0 ,1050.0 ,770.0 ,7 ,20.28 \\r\\n350.0 ,0.0 ,0.0 ,186.0 ,0.0 ,1050.0 ,770.0 ,28 ,34.29 \\r\\n203.5 ,305.3 ,0.0 ,203.5 ,0.0 ,963.4 ,630.0 ,7 ,19.54 \\r\\n250.2 ,166.8 ,0.0 ,203.5 ,0.0 ,977.6 ,694.1 ,90 ,47.71 \\r\\n157.0 ,236.0 ,0.0 ,192.0 ,0.0 ,935.4 ,781.2 ,90 ,43.38 \\r\\n141.3 ,212.0 ,0.0 ,203.5 ,0.0 ,971.8 ,748.5 ,28 ,29.89 \\r\\n166.8 ,250.2 ,0.0 ,203.5 ,0.0 ,975.6 ,692.6 ,3 ,6.90 \\r\\n122.6 ,183.9 ,0.0 ,203.5 ,0.0 ,958.2 ,800.1 ,90 ,33.19 \\r\\n183.9 ,122.6 ,0.0 ,203.5 ,0.0 ,959.2 ,800.0 ,3 ,4.90 \\r\\n102.0 ,153.0 ,0.0 ,192.0 ,0.0 ,887.0 ,942.0 ,3 ,4.57 \\r\\n102.0 ,153.0 ,0.0 ,192.0 ,0.0 ,887.0 ,942.0 ,90 ,25.46 \\r\\n122.6 ,183.9 ,0.0 ,203.5 ,0.0 ,958.2 ,800.1 ,28 ,24.29 \\r\\n166.8 ,250.2 ,0.0 ,203.5 ,0.0 ,975.6 ,692.6 ,28 ,33.95 \\r\\n200.0 ,133.0 ,0.0 ,192.0 ,0.0 ,965.4 ,806.2 ,3 ,11.41 \\r\\n108.3 ,162.4 ,0.0 ,203.5 ,0.0 ,938.2 ,849.0 ,28 ,20.59 \\r\\n305.3 ,203.5 ,0.0 ,203.5 ,0.0 ,965.4 ,631.0 ,7 ,25.89 \\r\\n108.3 ,162.4 ,0.0 ,203.5 ,0.0 ,938.2 ,849.0 ,90 ,29.23 \\r\\n116.0 ,173.0 ,0.0 ,192.0 ,0.0 ,909.8 ,891.9 ,90 ,31.02 \\r\\n141.3 ,212.0 ,0.0 ,203.5 ,0.0 ,971.8 ,748.5 ,7 ,10.39 \\r\\n157.0 ,236.0 ,0.0 ,192.0 ,0.0 ,935.4 ,781.2 ,28 ,33.66 \\r\\n133.0 ,200.0 ,0.0 ,192.0 ,0.0 ,927.4 ,839.2 ,28 ,27.87 \\r\\n250.2 ,166.8 ,0.0 ,203.5 ,0.0 ,977.6 ,694.1 ,7 ,19.35 \\r\\n173.0 ,116.0 ,0.0 ,192.0 ,0.0 ,946.8 ,856.8 ,7 ,11.39 \\r\\n192.0 ,288.0 ,0.0 ,192.0 ,0.0 ,929.8 ,716.1 ,3 ,12.79 \\r\\n192.0 ,288.0 ,0.0 ,192.0 ,0.0 ,929.8 ,716.1 ,28 ,39.32 \\r\\n153.0 ,102.0 ,0.0 ,192.0 ,0.0 ,888.0 ,943.1 ,3 ,4.78 \\r\\n288.0 ,192.0 ,0.0 ,192.0 ,0.0 ,932.0 ,717.8 ,3 ,16.11 \\r\\n305.3 ,203.5 ,0.0 ,203.5 ,0.0 ,965.4 ,631.0 ,28 ,43.38 \\r\\n236.0 ,157.0 ,0.0 ,192.0 ,0.0 ,972.6 ,749.1 ,7 ,20.42 \\r\\n173.0 ,116.0 ,0.0 ,192.0 ,0.0 ,946.8 ,856.8 ,3 ,6.94 \\r\\n212.0 ,141.3 ,0.0 ,203.5 ,0.0 ,973.4 ,750.0 ,7 ,15.03 \\r\\n236.0 ,157.0 ,0.0 ,192.0 ,0.0 ,972.6 ,749.1 ,3 ,13.57 \\r\\n183.9 ,122.6 ,0.0 ,203.5 ,0.0 ,959.2 ,800.0 ,90 ,32.53 \\r\\n166.8 ,250.2 ,0.0 ,203.5 ,0.0 ,975.6 ,692.6 ,7 ,15.75 \\r\\n102.0 ,153.0 ,0.0 ,192.0 ,0.0 ,887.0 ,942.0 ,7 ,7.68 \\r\\n288.0 ,192.0 ,0.0 ,192.0 ,0.0 ,932.0 ,717.8 ,28 ,38.80 \\r\\n212.0 ,141.3 ,0.0 ,203.5 ,0.0 ,973.4 ,750.0 ,28 ,33.00 \\r\\n102.0 ,153.0 ,0.0 ,192.0 ,0.0 ,887.0 ,942.0 ,28 ,17.28 \\r\\n173.0 ,116.0 ,0.0 ,192.0 ,0.0 ,946.8 ,856.8 ,28 ,24.28 \\r\\n183.9 ,122.6 ,0.0 ,203.5 ,0.0 ,959.2 ,800.0 ,28 ,24.05 \\r\\n133.0 ,200.0 ,0.0 ,192.0 ,0.0 ,927.4 ,839.2 ,90 ,36.59 \\r\\n192.0 ,288.0 ,0.0 ,192.0 ,0.0 ,929.8 ,716.1 ,90 ,50.73 \\r\\n133.0 ,200.0 ,0.0 ,192.0 ,0.0 ,927.4 ,839.2 ,7 ,13.66 \\r\\n305.3 ,203.5 ,0.0 ,203.5 ,0.0 ,965.4 ,631.0 ,3 ,14.14 \\r\\n236.0 ,157.0 ,0.0 ,192.0 ,0.0 ,972.6 ,749.1 ,90 ,47.78 \\r\\n108.3 ,162.4 ,0.0 ,203.5 ,0.0 ,938.2 ,849.0 ,3 ,2.33 \\r\\n157.0 ,236.0 ,0.0 ,192.0 ,0.0 ,935.4 ,781.2 ,7 ,16.89 \\r\\n288.0 ,192.0 ,0.0 ,192.0 ,0.0 ,932.0 ,717.8 ,7 ,23.52 \\r\\n212.0 ,141.3 ,0.0 ,203.5 ,0.0 ,973.4 ,750.0 ,3 ,6.81 \\r\\n212.0 ,141.3 ,0.0 ,203.5 ,0.0 ,973.4 ,750.0 ,90 ,39.70 \\r\\n153.0 ,102.0 ,0.0 ,192.0 ,0.0 ,888.0 ,943.1 ,28 ,17.96 \\r\\n236.0 ,157.0 ,0.0 ,192.0 ,0.0 ,972.6 ,749.1 ,28 ,32.88 \\r\\n116.0 ,173.0 ,0.0 ,192.0 ,0.0 ,909.8 ,891.9 ,28 ,22.35 \\r\\n183.9 ,122.6 ,0.0 ,203.5 ,0.0 ,959.2 ,800.0 ,7 ,10.79 \\r\\n108.3 ,162.4 ,0.0 ,203.5 ,0.0 ,938.2 ,849.0 ,7 ,7.72 \\r\\n203.5 ,305.3 ,0.0 ,203.5 ,0.0 ,963.4 ,630.0 ,28 ,41.68 \\r\\n203.5 ,305.3 ,0.0 ,203.5 ,0.0 ,963.4 ,630.0 ,3 ,9.56 \\r\\n133.0 ,200.0 ,0.0 ,192.0 ,0.0 ,927.4 ,839.2 ,3 ,6.88 \\r\\n288.0 ,192.0 ,0.0 ,192.0 ,0.0 ,932.0 ,717.8 ,90 ,50.53 \\r\\n200.0 ,133.0 ,0.0 ,192.0 ,0.0 ,965.4 ,806.2 ,7 ,17.17 \\r\\n200.0 ,133.0 ,0.0 ,192.0 ,0.0 ,965.4 ,806.2 ,28 ,30.44 \\r\\n250.2 ,166.8 ,0.0 ,203.5 ,0.0 ,977.6 ,694.1 ,3 ,9.73 \\r\\n122.6 ,183.9 ,0.0 ,203.5 ,0.0 ,958.2 ,800.1 ,3 ,3.32 \\r\\n153.0 ,102.0 ,0.0 ,192.0 ,0.0 ,888.0 ,943.1 ,90 ,26.32 \\r\\n200.0 ,133.0 ,0.0 ,192.0 ,0.0 ,965.4 ,806.2 ,90 ,43.25 \\r\\n116.0 ,173.0 ,0.0 ,192.0 ,0.0 ,909.8 ,891.9 ,3 ,6.28 \\r\\n173.0 ,116.0 ,0.0 ,192.0 ,0.0 ,946.8 ,856.8 ,90 ,32.10 \\r\\n250.2 ,166.8 ,0.0 ,203.5 ,0.0 ,977.6 ,694.1 ,28 ,36.96 \\r\\n305.3 ,203.5 ,0.0 ,203.5 ,0.0 ,965.4 ,631.0 ,90 ,54.60 \\r\\n192.0 ,288.0 ,0.0 ,192.0 ,0.0 ,929.8 ,716.1 ,7 ,21.48 \\r\\n157.0 ,236.0 ,0.0 ,192.0 ,0.0 ,935.4 ,781.2 ,3 ,9.69 \\r\\n153.0 ,102.0 ,0.0 ,192.0 ,0.0 ,888.0 ,943.1 ,7 ,8.37 \\r\\n141.3 ,212.0 ,0.0 ,203.5 ,0.0 ,971.8 ,748.5 ,90 ,39.66 \\r\\n116.0 ,173.0 ,0.0 ,192.0 ,0.0 ,909.8 ,891.9 ,7 ,10.09 \\r\\n141.3 ,212.0 ,0.0 ,203.5 ,0.0 ,971.8 ,748.5 ,3 ,4.83 \\r\\n122.6 ,183.9 ,0.0 ,203.5 ,0.0 ,958.2 ,800.1 ,7 ,10.35 \\r\\n166.8 ,250.2 ,0.0 ,203.5 ,0.0 ,975.6 ,692.6 ,90 ,43.57 \\r\\n203.5 ,305.3 ,0.0 ,203.5 ,0.0 ,963.4 ,630.0 ,90 ,51.86 \\r\\n310.0 ,0.0 ,0.0 ,192.0 ,0.0 ,1012.0 ,830.0 ,3 ,11.85 \\r\\n310.0 ,0.0 ,0.0 ,192.0 ,0.0 ,1012.0 ,830.0 ,7 ,17.24 \\r\\n310.0 ,0.0 ,0.0 ,192.0 ,0.0 ,1012.0 ,830.0 ,28 ,27.83 \\r\\n310.0 ,0.0 ,0.0 ,192.0 ,0.0 ,1012.0 ,830.0 ,90 ,35.76 \\r\\n310.0 ,0.0 ,0.0 ,192.0 ,0.0 ,1012.0 ,830.0 ,120 ,38.70 \\r\\n331.0 ,0.0 ,0.0 ,192.0 ,0.0 ,1025.0 ,821.0 ,3 ,14.31 \\r\\n331.0 ,0.0 ,0.0 ,192.0 ,0.0 ,1025.0 ,821.0 ,7 ,17.44 \\r\\n331.0 ,0.0 ,0.0 ,192.0 ,0.0 ,1025.0 ,821.0 ,28 ,31.74 \\r\\n331.0 ,0.0 ,0.0 ,192.0 ,0.0 ,1025.0 ,821.0 ,90 ,37.91 \\r\\n331.0 ,0.0 ,0.0 ,192.0 ,0.0 ,1025.0 ,821.0 ,120 ,39.38 \\r\\n349.0 ,0.0 ,0.0 ,192.0 ,0.0 ,1056.0 ,809.0 ,3 ,15.87 \\r\\n349.0 ,0.0 ,0.0 ,192.0 ,0.0 ,1056.0 ,809.0 ,7 ,9.01 \\r\\n349.0 ,0.0 ,0.0 ,192.0 ,0.0 ,1056.0 ,809.0 ,28 ,33.61 \\r\\n349.0 ,0.0 ,0.0 ,192.0 ,0.0 ,1056.0 ,809.0 ,90 ,40.66 \\r\\n349.0 ,0.0 ,0.0 ,192.0 ,0.0 ,1056.0 ,809.0 ,120 ,40.86 \\r\\n238.0 ,0.0 ,0.0 ,186.0 ,0.0 ,1119.0 ,789.0 ,7 ,12.05 \\r\\n238.0 ,0.0 ,0.0 ,186.0 ,0.0 ,1119.0 ,789.0 ,28 ,17.54 \\r\\n296.0 ,0.0 ,0.0 ,186.0 ,0.0 ,1090.0 ,769.0 ,7 ,18.91 \\r\\n296.0 ,0.0 ,0.0 ,186.0 ,0.0 ,1090.0 ,769.0 ,28 ,25.18 \\r\\n297.0 ,0.0 ,0.0 ,186.0 ,0.0 ,1040.0 ,734.0 ,7 ,30.96 \\r\\n480.0 ,0.0 ,0.0 ,192.0 ,0.0 ,936.0 ,721.0 ,28 ,43.89 \\r\\n480.0 ,0.0 ,0.0 ,192.0 ,0.0 ,936.0 ,721.0 ,90 ,54.28 \\r\\n397.0 ,0.0 ,0.0 ,186.0 ,0.0 ,1040.0 ,734.0 ,28 ,36.94 \\r\\n281.0 ,0.0 ,0.0 ,186.0 ,0.0 ,1104.0 ,774.0 ,7 ,14.50 \\r\\n281.0 ,0.0 ,0.0 ,185.0 ,0.0 ,1104.0 ,774.0 ,28 ,22.44 \\r\\n500.0 ,0.0 ,0.0 ,200.0 ,0.0 ,1125.0 ,613.0 ,1 ,12.64 \\r\\n500.0 ,0.0 ,0.0 ,200.0 ,0.0 ,1125.0 ,613.0 ,3 ,26.06 \\r\\n500.0 ,0.0 ,0.0 ,200.0 ,0.0 ,1125.0 ,613.0 ,7 ,33.21 \\r\\n500.0 ,0.0 ,0.0 ,200.0 ,0.0 ,1125.0 ,613.0 ,14 ,36.94 \\r\\n500.0 ,0.0 ,0.0 ,200.0 ,0.0 ,1125.0 ,613.0 ,28 ,44.09 \\r\\n540.0 ,0.0 ,0.0 ,173.0 ,0.0 ,1125.0 ,613.0 ,7 ,52.61 \\r\\n540.0 ,0.0 ,0.0 ,173.0 ,0.0 ,1125.0 ,613.0 ,14 ,59.76 \\r\\n540.0 ,0.0 ,0.0 ,173.0 ,0.0 ,1125.0 ,613.0 ,28 ,67.31 \\r\\n540.0 ,0.0 ,0.0 ,173.0 ,0.0 ,1125.0 ,613.0 ,90 ,69.66 \\r\\n540.0 ,0.0 ,0.0 ,173.0 ,0.0 ,1125.0 ,613.0 ,180 ,71.62 \\r\\n540.0 ,0.0 ,0.0 ,173.0 ,0.0 ,1125.0 ,613.0 ,270 ,74.17 \\r\\n350.0 ,0.0 ,0.0 ,203.0 ,0.0 ,974.0 ,775.0 ,7 ,18.13 \\r\\n350.0 ,0.0 ,0.0 ,203.0 ,0.0 ,974.0 ,775.0 ,14 ,22.53 \\r\\n350.0 ,0.0 ,0.0 ,203.0 ,0.0 ,974.0 ,775.0 ,28 ,27.34 \\r\\n350.0 ,0.0 ,0.0 ,203.0 ,0.0 ,974.0 ,775.0 ,56 ,29.98 \\r\\n350.0 ,0.0 ,0.0 ,203.0 ,0.0 ,974.0 ,775.0 ,90 ,31.35 \\r\\n350.0 ,0.0 ,0.0 ,203.0 ,0.0 ,974.0 ,775.0 ,180 ,32.72 \\r\\n385.0 ,0.0 ,0.0 ,186.0 ,0.0 ,966.0 ,763.0 ,1 ,6.27 \\r\\n385.0 ,0.0 ,0.0 ,186.0 ,0.0 ,966.0 ,763.0 ,3 ,14.70 \\r\\n385.0 ,0.0 ,0.0 ,186.0 ,0.0 ,966.0 ,763.0 ,7 ,23.22 \\r\\n385.0 ,0.0 ,0.0 ,186.0 ,0.0 ,966.0 ,763.0 ,14 ,27.92 \\r\\n385.0 ,0.0 ,0.0 ,186.0 ,0.0 ,966.0 ,763.0 ,28 ,31.35 \\r\\n331.0 ,0.0 ,0.0 ,192.0 ,0.0 ,978.0 ,825.0 ,180 ,39.00 \\r\\n331.0 ,0.0 ,0.0 ,192.0 ,0.0 ,978.0 ,825.0 ,360 ,41.24 \\r\\n349.0 ,0.0 ,0.0 ,192.0 ,0.0 ,1047.0 ,806.0 ,3 ,14.99 \\r\\n331.0 ,0.0 ,0.0 ,192.0 ,0.0 ,978.0 ,825.0 ,3 ,13.52 \\r\\n382.0 ,0.0 ,0.0 ,186.0 ,0.0 ,1047.0 ,739.0 ,7 ,24.00 \\r\\n382.0 ,0.0 ,0.0 ,186.0 ,0.0 ,1047.0 ,739.0 ,28 ,37.42 \\r\\n382.0 ,0.0 ,0.0 ,186.0 ,0.0 ,1111.0 ,784.0 ,7 ,11.47 \\r\\n281.0 ,0.0 ,0.0 ,186.0 ,0.0 ,1104.0 ,774.0 ,28 ,22.44 \\r\\n339.0 ,0.0 ,0.0 ,185.0 ,0.0 ,1069.0 ,754.0 ,7 ,21.16 \\r\\n339.0 ,0.0 ,0.0 ,185.0 ,0.0 ,1069.0 ,754.0 ,28 ,31.84 \\r\\n295.0 ,0.0 ,0.0 ,185.0 ,0.0 ,1069.0 ,769.0 ,7 ,14.80 \\r\\n295.0 ,0.0 ,0.0 ,185.0 ,0.0 ,1069.0 ,769.0 ,28 ,25.18 \\r\\n238.0 ,0.0 ,0.0 ,185.0 ,0.0 ,1118.0 ,789.0 ,28 ,17.54 \\r\\n296.0 ,0.0 ,0.0 ,192.0 ,0.0 ,1085.0 ,765.0 ,7 ,14.20 \\r\\n296.0 ,0.0 ,0.0 ,192.0 ,0.0 ,1085.0 ,765.0 ,28 ,21.65 \\r\\n296.0 ,0.0 ,0.0 ,192.0 ,0.0 ,1085.0 ,765.0 ,90 ,29.39 \\r\\n331.0 ,0.0 ,0.0 ,192.0 ,0.0 ,879.0 ,825.0 ,3 ,13.52 \\r\\n331.0 ,0.0 ,0.0 ,192.0 ,0.0 ,978.0 ,825.0 ,7 ,16.26 \\r\\n331.0 ,0.0 ,0.0 ,192.0 ,0.0 ,978.0 ,825.0 ,28 ,31.45 \\r\\n331.0 ,0.0 ,0.0 ,192.0 ,0.0 ,978.0 ,825.0 ,90 ,37.23 \\r\\n349.0 ,0.0 ,0.0 ,192.0 ,0.0 ,1047.0 ,806.0 ,7 ,18.13 \\r\\n349.0 ,0.0 ,0.0 ,192.0 ,0.0 ,1047.0 ,806.0 ,28 ,32.72 \\r\\n349.0 ,0.0 ,0.0 ,192.0 ,0.0 ,1047.0 ,806.0 ,90 ,39.49 \\r\\n349.0 ,0.0 ,0.0 ,192.0 ,0.0 ,1047.0 ,806.0 ,180 ,41.05 \\r\\n349.0 ,0.0 ,0.0 ,192.0 ,0.0 ,1047.0 ,806.0 ,360 ,42.13 \\r\\n302.0 ,0.0 ,0.0 ,203.0 ,0.0 ,974.0 ,817.0 ,14 ,18.13 \\r\\n302.0 ,0.0 ,0.0 ,203.0 ,0.0 ,974.0 ,817.0 ,180 ,26.74 \\r\\n525.0 ,0.0 ,0.0 ,189.0 ,0.0 ,1125.0 ,613.0 ,180 ,61.92 \\r\\n500.0 ,0.0 ,0.0 ,200.0 ,0.0 ,1125.0 ,613.0 ,90 ,47.22 \\r\\n500.0 ,0.0 ,0.0 ,200.0 ,0.0 ,1125.0 ,613.0 ,180 ,51.04 \\r\\n500.0 ,0.0 ,0.0 ,200.0 ,0.0 ,1125.0 ,613.0 ,270 ,55.16 \\r\\n540.0 ,0.0 ,0.0 ,173.0 ,0.0 ,1125.0 ,613.0 ,3 ,41.64 \\r\\n252.0 ,0.0 ,0.0 ,185.0 ,0.0 ,1111.0 ,784.0 ,7 ,13.71 \\r\\n252.0 ,0.0 ,0.0 ,185.0 ,0.0 ,1111.0 ,784.0 ,28 ,19.69 \\r\\n339.0 ,0.0 ,0.0 ,185.0 ,0.0 ,1060.0 ,754.0 ,28 ,31.65 \\r\\n393.0 ,0.0 ,0.0 ,192.0 ,0.0 ,940.0 ,758.0 ,3 ,19.11 \\r\\n393.0 ,0.0 ,0.0 ,192.0 ,0.0 ,940.0 ,758.0 ,28 ,39.58 \\r\\n393.0 ,0.0 ,0.0 ,192.0 ,0.0 ,940.0 ,758.0 ,90 ,48.79 \\r\\n382.0 ,0.0 ,0.0 ,185.0 ,0.0 ,1047.0 ,739.0 ,7 ,24.00 \\r\\n382.0 ,0.0 ,0.0 ,185.0 ,0.0 ,1047.0 ,739.0 ,28 ,37.42 \\r\\n252.0 ,0.0 ,0.0 ,186.0 ,0.0 ,1111.0 ,784.0 ,7 ,11.47 \\r\\n252.0 ,0.0 ,0.0 ,185.0 ,0.0 ,1111.0 ,784.0 ,28 ,19.69 \\r\\n310.0 ,0.0 ,0.0 ,192.0 ,0.0 ,970.0 ,850.0 ,7 ,14.99 \\r\\n310.0 ,0.0 ,0.0 ,192.0 ,0.0 ,970.0 ,850.0 ,28 ,27.92 \\r\\n310.0 ,0.0 ,0.0 ,192.0 ,0.0 ,970.0 ,850.0 ,90 ,34.68 \\r\\n310.0 ,0.0 ,0.0 ,192.0 ,0.0 ,970.0 ,850.0 ,180 ,37.33 \\r\\n310.0 ,0.0 ,0.0 ,192.0 ,0.0 ,970.0 ,850.0 ,360 ,38.11 \\r\\n525.0 ,0.0 ,0.0 ,189.0 ,0.0 ,1125.0 ,613.0 ,3 ,33.80 \\r\\n525.0 ,0.0 ,0.0 ,189.0 ,0.0 ,1125.0 ,613.0 ,7 ,42.42 \\r\\n525.0 ,0.0 ,0.0 ,189.0 ,0.0 ,1125.0 ,613.0 ,14 ,48.40 \\r\\n525.0 ,0.0 ,0.0 ,189.0 ,0.0 ,1125.0 ,613.0 ,28 ,55.94 \\r\\n525.0 ,0.0 ,0.0 ,189.0 ,0.0 ,1125.0 ,613.0 ,90 ,58.78 \\r\\n525.0 ,0.0 ,0.0 ,189.0 ,0.0 ,1125.0 ,613.0 ,270 ,67.11 \\r\\n322.0 ,0.0 ,0.0 ,203.0 ,0.0 ,974.0 ,800.0 ,14 ,20.77 \\r\\n322.0 ,0.0 ,0.0 ,203.0 ,0.0 ,974.0 ,800.0 ,28 ,25.18 \\r\\n322.0 ,0.0 ,0.0 ,203.0 ,0.0 ,974.0 ,800.0 ,180 ,29.59 \\r\\n302.0 ,0.0 ,0.0 ,203.0 ,0.0 ,974.0 ,817.0 ,28 ,21.75 \\r\\n397.0 ,0.0 ,0.0 ,185.0 ,0.0 ,1040.0 ,734.0 ,28 ,39.09 \\r\\n480.0 ,0.0 ,0.0 ,192.0 ,0.0 ,936.0 ,721.0 ,3 ,24.39 \\r\\n522.0 ,0.0 ,0.0 ,146.0 ,0.0 ,896.0 ,896.0 ,7 ,50.51 \\r\\n522.0 ,0.0 ,0.0 ,146.0 ,0.0 ,896.0 ,896.0 ,28 ,74.99 \\r\\n273.0 ,105.0 ,82.0 ,210.0 ,9.0 ,904.0 ,680.0 ,28 ,37.17 \\r\\n162.0 ,190.0 ,148.0 ,179.0 ,19.0 ,838.0 ,741.0 ,28 ,33.76 \\r\\n154.0 ,144.0 ,112.0 ,220.0 ,10.0 ,923.0 ,658.0 ,28 ,16.50 \\r\\n147.0 ,115.0 ,89.0 ,202.0 ,9.0 ,860.0 ,829.0 ,28 ,19.99 \\r\\n152.0 ,178.0 ,139.0 ,168.0 ,18.0 ,944.0 ,695.0 ,28 ,36.35 \\r\\n310.0 ,143.0 ,111.0 ,168.0 ,22.0 ,914.0 ,651.0 ,28 ,33.69 \\r\\n144.0 ,0.0 ,175.0 ,158.0 ,18.0 ,943.0 ,844.0 ,28 ,15.42 \\r\\n304.0 ,140.0 ,0.0 ,214.0 ,6.0 ,895.0 ,722.0 ,28 ,33.42 \\r\\n374.0 ,0.0 ,0.0 ,190.0 ,7.0 ,1013.0 ,730.0 ,28 ,39.05 \\r\\n159.0 ,149.0 ,116.0 ,175.0 ,15.0 ,953.0 ,720.0 ,28 ,27.68 \\r\\n153.0 ,239.0 ,0.0 ,200.0 ,6.0 ,1002.0 ,684.0 ,28 ,26.86 \\r\\n310.0 ,143.0 ,0.0 ,168.0 ,10.0 ,914.0 ,804.0 ,28 ,45.30 \\r\\n305.0 ,0.0 ,100.0 ,196.0 ,10.0 ,959.0 ,705.0 ,28 ,30.12 \\r\\n151.0 ,0.0 ,184.0 ,167.0 ,12.0 ,991.0 ,772.0 ,28 ,15.57 \\r\\n142.0 ,167.0 ,130.0 ,174.0 ,11.0 ,883.0 ,785.0 ,28 ,44.61 \\r\\n298.0 ,137.0 ,107.0 ,201.0 ,6.0 ,878.0 ,655.0 ,28 ,53.52 \\r\\n321.0 ,164.0 ,0.0 ,190.0 ,5.0 ,870.0 ,774.0 ,28 ,57.21 \\r\\n366.0 ,187.0 ,0.0 ,191.0 ,7.0 ,824.0 ,757.0 ,28 ,65.91 \\r\\n280.0 ,129.0 ,100.0 ,172.0 ,9.0 ,825.0 ,805.0 ,28 ,52.82 \\r\\n252.0 ,97.0 ,76.0 ,194.0 ,8.0 ,835.0 ,821.0 ,28 ,33.40 \\r\\n165.0 ,0.0 ,150.0 ,182.0 ,12.0 ,1023.0 ,729.0 ,28 ,18.03 \\r\\n156.0 ,243.0 ,0.0 ,180.0 ,11.0 ,1022.0 ,698.0 ,28 ,37.36 \\r\\n160.0 ,188.0 ,146.0 ,203.0 ,11.0 ,829.0 ,710.0 ,28 ,32.84 \\r\\n298.0 ,0.0 ,107.0 ,186.0 ,6.0 ,879.0 ,815.0 ,28 ,42.64 \\r\\n318.0 ,0.0 ,126.0 ,210.0 ,6.0 ,861.0 ,737.0 ,28 ,40.06 \\r\\n287.0 ,121.0 ,94.0 ,188.0 ,9.0 ,904.0 ,696.0 ,28 ,41.94 \\r\\n326.0 ,166.0 ,0.0 ,174.0 ,9.0 ,882.0 ,790.0 ,28 ,61.23 \\r\\n356.0 ,0.0 ,142.0 ,193.0 ,11.0 ,801.0 ,778.0 ,28 ,40.87 \\r\\n132.0 ,207.0 ,161.0 ,179.0 ,5.0 ,867.0 ,736.0 ,28 ,33.30 \\r\\n322.0 ,149.0 ,0.0 ,186.0 ,8.0 ,951.0 ,709.0 ,28 ,52.42 \\r\\n164.0 ,0.0 ,200.0 ,181.0 ,13.0 ,849.0 ,846.0 ,28 ,15.09 \\r\\n314.0 ,0.0 ,113.0 ,170.0 ,10.0 ,925.0 ,783.0 ,28 ,38.46 \\r\\n321.0 ,0.0 ,128.0 ,182.0 ,11.0 ,870.0 ,780.0 ,28 ,37.26 \\r\\n140.0 ,164.0 ,128.0 ,237.0 ,6.0 ,869.0 ,656.0 ,28 ,35.23 \\r\\n288.0 ,121.0 ,0.0 ,177.0 ,7.0 ,908.0 ,829.0 ,28 ,42.13 \\r\\n298.0 ,0.0 ,107.0 ,210.0 ,11.0 ,880.0 ,744.0 ,28 ,31.87 \\r\\n265.0 ,111.0 ,86.0 ,195.0 ,6.0 ,833.0 ,790.0 ,28 ,41.54 \\r\\n160.0 ,250.0 ,0.0 ,168.0 ,12.0 ,1049.0 ,688.0 ,28 ,39.45 \\r\\n166.0 ,260.0 ,0.0 ,183.0 ,13.0 ,859.0 ,827.0 ,28 ,37.91 \\r\\n276.0 ,116.0 ,90.0 ,180.0 ,9.0 ,870.0 ,768.0 ,28 ,44.28 \\r\\n322.0 ,0.0 ,116.0 ,196.0 ,10.0 ,818.0 ,813.0 ,28 ,31.18 \\r\\n149.0 ,139.0 ,109.0 ,193.0 ,6.0 ,892.0 ,780.0 ,28 ,23.69 \\r\\n159.0 ,187.0 ,0.0 ,176.0 ,11.0 ,990.0 ,789.0 ,28 ,32.76 \\r\\n261.0 ,100.0 ,78.0 ,201.0 ,9.0 ,864.0 ,761.0 ,28 ,32.40 \\r\\n237.0 ,92.0 ,71.0 ,247.0 ,6.0 ,853.0 ,695.0 ,28 ,28.63 \\r\\n313.0 ,0.0 ,113.0 ,178.0 ,8.0 ,1002.0 ,689.0 ,28 ,36.80 \\r\\n155.0 ,183.0 ,0.0 ,193.0 ,9.0 ,1047.0 ,697.0 ,28 ,18.28 \\r\\n146.0 ,230.0 ,0.0 ,202.0 ,3.0 ,827.0 ,872.0 ,28 ,33.06 \\r\\n296.0 ,0.0 ,107.0 ,221.0 ,11.0 ,819.0 ,778.0 ,28 ,31.42 \\r\\n133.0 ,210.0 ,0.0 ,196.0 ,3.0 ,949.0 ,795.0 ,28 ,31.03 \\r\\n313.0 ,145.0 ,0.0 ,178.0 ,8.0 ,867.0 ,824.0 ,28 ,44.39 \\r\\n152.0 ,0.0 ,112.0 ,184.0 ,8.0 ,992.0 ,816.0 ,28 ,12.18 \\r\\n153.0 ,145.0 ,113.0 ,178.0 ,8.0 ,1002.0 ,689.0 ,28 ,25.56 \\r\\n140.0 ,133.0 ,103.0 ,200.0 ,7.0 ,916.0 ,753.0 ,28 ,36.44 \\r\\n149.0 ,236.0 ,0.0 ,176.0 ,13.0 ,847.0 ,893.0 ,28 ,32.96 \\r\\n300.0 ,0.0 ,120.0 ,212.0 ,10.0 ,878.0 ,728.0 ,28 ,23.84 \\r\\n153.0 ,145.0 ,113.0 ,178.0 ,8.0 ,867.0 ,824.0 ,28 ,26.23 \\r\\n148.0 ,0.0 ,137.0 ,158.0 ,16.0 ,1002.0 ,830.0 ,28 ,17.95 \\r\\n326.0 ,0.0 ,138.0 ,199.0 ,11.0 ,801.0 ,792.0 ,28 ,40.68 \\r\\n153.0 ,145.0 ,0.0 ,178.0 ,8.0 ,1000.0 ,822.0 ,28 ,19.01 \\r\\n262.0 ,111.0 ,86.0 ,195.0 ,5.0 ,895.0 ,733.0 ,28 ,33.72 \\r\\n158.0 ,0.0 ,195.0 ,220.0 ,11.0 ,898.0 ,713.0 ,28 ,8.54 \\r\\n151.0 ,0.0 ,185.0 ,167.0 ,16.0 ,1074.0 ,678.0 ,28 ,13.46 \\r\\n273.0 ,0.0 ,90.0 ,199.0 ,11.0 ,931.0 ,762.0 ,28 ,32.24 \\r\\n149.0 ,118.0 ,92.0 ,183.0 ,7.0 ,953.0 ,780.0 ,28 ,23.52 \\r\\n143.0 ,169.0 ,143.0 ,191.0 ,8.0 ,967.0 ,643.0 ,28 ,29.72 \\r\\n260.0 ,101.0 ,78.0 ,171.0 ,10.0 ,936.0 ,763.0 ,28 ,49.77 \\r\\n313.0 ,161.0 ,0.0 ,178.0 ,10.0 ,917.0 ,759.0 ,28 ,52.44 \\r\\n284.0 ,120.0 ,0.0 ,168.0 ,7.0 ,970.0 ,794.0 ,28 ,40.93 \\r\\n336.0 ,0.0 ,0.0 ,182.0 ,3.0 ,986.0 ,817.0 ,28 ,44.86 \\r\\n145.0 ,0.0 ,134.0 ,181.0 ,11.0 ,979.0 ,812.0 ,28 ,13.20 \\r\\n150.0 ,237.0 ,0.0 ,174.0 ,12.0 ,1069.0 ,675.0 ,28 ,37.43 \\r\\n144.0 ,170.0 ,133.0 ,192.0 ,8.0 ,814.0 ,805.0 ,28 ,29.87 \\r\\n331.0 ,170.0 ,0.0 ,195.0 ,8.0 ,811.0 ,802.0 ,28 ,56.61 \\r\\n155.0 ,0.0 ,143.0 ,193.0 ,9.0 ,1047.0 ,697.0 ,28 ,12.46 \\r\\n155.0 ,183.0 ,0.0 ,193.0 ,9.0 ,877.0 ,868.0 ,28 ,23.79 \\r\\n135.0 ,0.0 ,166.0 ,180.0 ,10.0 ,961.0 ,805.0 ,28 ,13.29 \\r\\n266.0 ,112.0 ,87.0 ,178.0 ,10.0 ,910.0 ,745.0 ,28 ,39.42 \\r\\n314.0 ,145.0 ,113.0 ,179.0 ,8.0 ,869.0 ,690.0 ,28 ,46.23 \\r\\n313.0 ,145.0 ,0.0 ,127.0 ,8.0 ,1000.0 ,822.0 ,28 ,44.52 \\r\\n146.0 ,173.0 ,0.0 ,182.0 ,3.0 ,986.0 ,817.0 ,28 ,23.74 \\r\\n144.0 ,136.0 ,106.0 ,178.0 ,7.0 ,941.0 ,774.0 ,28 ,26.14 \\r\\n148.0 ,0.0 ,182.0 ,181.0 ,15.0 ,839.0 ,884.0 ,28 ,15.52 \\r\\n277.0 ,117.0 ,91.0 ,191.0 ,7.0 ,946.0 ,666.0 ,28 ,43.57 \\r\\n298.0 ,0.0 ,107.0 ,164.0 ,13.0 ,953.0 ,784.0 ,28 ,35.86 \\r\\n313.0 ,145.0 ,0.0 ,178.0 ,8.0 ,1002.0 ,689.0 ,28 ,41.05 \\r\\n155.0 ,184.0 ,143.0 ,194.0 ,9.0 ,880.0 ,699.0 ,28 ,28.99 \\r\\n289.0 ,134.0 ,0.0 ,195.0 ,6.0 ,924.0 ,760.0 ,28 ,46.24 \\r\\n148.0 ,175.0 ,0.0 ,171.0 ,2.0 ,1000.0 ,828.0 ,28 ,26.92 \\r\\n145.0 ,0.0 ,179.0 ,202.0 ,8.0 ,824.0 ,869.0 ,28 ,10.54 \\r\\n313.0 ,0.0 ,0.0 ,178.0 ,8.0 ,1000.0 ,822.0 ,28 ,25.10 \\r\\n136.0 ,162.0 ,126.0 ,172.0 ,10.0 ,923.0 ,764.0 ,28 ,29.07 \\r\\n155.0 ,0.0 ,143.0 ,193.0 ,9.0 ,877.0 ,868.0 ,28 ,9.74 \\r\\n255.0 ,99.0 ,77.0 ,189.0 ,6.0 ,919.0 ,749.0 ,28 ,33.80 \\r\\n162.0 ,207.0 ,172.0 ,216.0 ,10.0 ,822.0 ,638.0 ,28 ,39.84 \\r\\n136.0 ,196.0 ,98.0 ,199.0 ,6.0 ,847.0 ,783.0 ,28 ,26.97 \\r\\n164.0 ,163.0 ,128.0 ,197.0 ,8.0 ,961.0 ,641.0 ,28 ,27.23 \\r\\n162.0 ,214.0 ,164.0 ,202.0 ,10.0 ,820.0 ,680.0 ,28 ,30.65 \\r\\n157.0 ,214.0 ,152.0 ,200.0 ,9.0 ,819.0 ,704.0 ,28 ,33.05 \\r\\n149.0 ,153.0 ,194.0 ,192.0 ,8.0 ,935.0 ,623.0 ,28 ,24.58 \\r\\n135.0 ,105.0 ,193.0 ,196.0 ,6.0 ,965.0 ,643.0 ,28 ,21.91 \\r\\n159.0 ,209.0 ,161.0 ,201.0 ,7.0 ,848.0 ,669.0 ,28 ,30.88 \\r\\n144.0 ,15.0 ,195.0 ,176.0 ,6.0 ,1021.0 ,709.0 ,28 ,15.34 \\r\\n154.0 ,174.0 ,185.0 ,228.0 ,7.0 ,845.0 ,612.0 ,28 ,24.34 \\r\\n167.0 ,187.0 ,195.0 ,185.0 ,7.0 ,898.0 ,636.0 ,28 ,23.89 \\r\\n184.0 ,86.0 ,190.0 ,213.0 ,6.0 ,923.0 ,623.0 ,28 ,22.93 \\r\\n156.0 ,178.0 ,187.0 ,221.0 ,7.0 ,854.0 ,614.0 ,28 ,29.41 \\r\\n236.9 ,91.7 ,71.5 ,246.9 ,6.0 ,852.9 ,695.4 ,28 ,28.63 \\r\\n313.3 ,0.0 ,113.0 ,178.5 ,8.0 ,1001.9 ,688.7 ,28 ,36.80 \\r\\n154.8 ,183.4 ,0.0 ,193.3 ,9.1 ,1047.4 ,696.7 ,28 ,18.29 \\r\\n145.9 ,230.5 ,0.0 ,202.5 ,3.4 ,827.0 ,871.8 ,28 ,32.72 \\r\\n296.0 ,0.0 ,106.7 ,221.4 ,10.5 ,819.2 ,778.4 ,28 ,31.42 \\r\\n133.1 ,210.2 ,0.0 ,195.7 ,3.1 ,949.4 ,795.3 ,28 ,28.94 \\r\\n313.3 ,145.0 ,0.0 ,178.5 ,8.0 ,867.2 ,824.0 ,28 ,40.93 \\r\\n151.6 ,0.0 ,111.9 ,184.4 ,7.9 ,992.0 ,815.9 ,28 ,12.18 \\r\\n153.1 ,145.0 ,113.0 ,178.5 ,8.0 ,1001.9 ,688.7 ,28 ,25.56 \\r\\n139.9 ,132.6 ,103.3 ,200.3 ,7.4 ,916.0 ,753.4 ,28 ,36.44 \\r\\n149.5 ,236.0 ,0.0 ,175.8 ,12.6 ,846.8 ,892.7 ,28 ,32.96 \\r\\n299.8 ,0.0 ,119.8 ,211.5 ,9.9 ,878.2 ,727.6 ,28 ,23.84 \\r\\n153.1 ,145.0 ,113.0 ,178.5 ,8.0 ,867.2 ,824.0 ,28 ,26.23 \\r\\n148.1 ,0.0 ,136.6 ,158.1 ,16.1 ,1001.8 ,830.1 ,28 ,17.96 \\r\\n326.5 ,0.0 ,137.9 ,199.0 ,10.8 ,801.1 ,792.5 ,28 ,38.63 \\r\\n152.7 ,144.7 ,0.0 ,178.1 ,8.0 ,999.7 ,822.2 ,28 ,19.01 \\r\\n261.9 ,110.5 ,86.1 ,195.4 ,5.0 ,895.2 ,732.6 ,28 ,33.72 \\r\\n158.4 ,0.0 ,194.9 ,219.7 ,11.0 ,897.7 ,712.9 ,28 ,8.54 \\r\\n150.7 ,0.0 ,185.3 ,166.7 ,15.6 ,1074.5 ,678.0 ,28 ,13.46 \\r\\n272.6 ,0.0 ,89.6 ,198.7 ,10.6 ,931.3 ,762.2 ,28 ,32.25 \\r\\n149.0 ,117.6 ,91.7 ,182.9 ,7.1 ,953.4 ,780.3 ,28 ,23.52 \\r\\n143.0 ,169.4 ,142.7 ,190.7 ,8.4 ,967.4 ,643.5 ,28 ,29.73 \\r\\n259.9 ,100.6 ,78.4 ,170.6 ,10.4 ,935.7 ,762.9 ,28 ,49.77 \\r\\n312.9 ,160.5 ,0.0 ,177.6 ,9.6 ,916.6 ,759.5 ,28 ,52.45 \\r\\n284.0 ,119.7 ,0.0 ,168.3 ,7.2 ,970.4 ,794.2 ,28 ,40.93 \\r\\n336.5 ,0.0 ,0.0 ,181.9 ,3.4 ,985.8 ,816.8 ,28 ,44.87 \\r\\n144.8 ,0.0 ,133.6 ,180.8 ,11.1 ,979.5 ,811.5 ,28 ,13.20 \\r\\n150.0 ,236.8 ,0.0 ,173.8 ,11.9 ,1069.3 ,674.8 ,28 ,37.43 \\r\\n143.7 ,170.2 ,132.6 ,191.6 ,8.5 ,814.1 ,805.3 ,28 ,29.87 \\r\\n330.5 ,169.6 ,0.0 ,194.9 ,8.1 ,811.0 ,802.3 ,28 ,56.62 \\r\\n154.8 ,0.0 ,142.8 ,193.3 ,9.1 ,1047.4 ,696.7 ,28 ,12.46 \\r\\n154.8 ,183.4 ,0.0 ,193.3 ,9.1 ,877.2 ,867.7 ,28 ,23.79 \\r\\n134.7 ,0.0 ,165.7 ,180.2 ,10.0 ,961.0 ,804.9 ,28 ,13.29 \\r\\n266.2 ,112.3 ,87.5 ,177.9 ,10.4 ,909.7 ,744.5 ,28 ,39.42 \\r\\n314.0 ,145.3 ,113.2 ,178.9 ,8.0 ,869.1 ,690.2 ,28 ,46.23 \\r\\n312.7 ,144.7 ,0.0 ,127.3 ,8.0 ,999.7 ,822.2 ,28 ,44.52 \\r\\n145.7 ,172.6 ,0.0 ,181.9 ,3.4 ,985.8 ,816.8 ,28 ,23.74 \\r\\n143.8 ,136.3 ,106.2 ,178.1 ,7.5 ,941.5 ,774.3 ,28 ,26.15 \\r\\n148.1 ,0.0 ,182.1 ,181.4 ,15.0 ,838.9 ,884.3 ,28 ,15.53 \\r\\n277.0 ,116.8 ,91.0 ,190.6 ,7.0 ,946.5 ,665.6 ,28 ,43.58 \\r\\n298.1 ,0.0 ,107.5 ,163.6 ,12.8 ,953.2 ,784.0 ,28 ,35.87 \\r\\n313.3 ,145.0 ,0.0 ,178.5 ,8.0 ,1001.9 ,688.7 ,28 ,41.05 \\r\\n155.2 ,183.9 ,143.2 ,193.8 ,9.2 ,879.6 ,698.5 ,28 ,28.99 \\r\\n289.0 ,133.7 ,0.0 ,194.9 ,5.5 ,924.1 ,760.1 ,28 ,46.25 \\r\\n147.8 ,175.1 ,0.0 ,171.2 ,2.2 ,1000.0 ,828.5 ,28 ,26.92 \\r\\n145.4 ,0.0 ,178.9 ,201.7 ,7.8 ,824.0 ,868.7 ,28 ,10.54 \\r\\n312.7 ,0.0 ,0.0 ,178.1 ,8.0 ,999.7 ,822.2 ,28 ,25.10 \\r\\n136.4 ,161.6 ,125.8 ,171.6 ,10.4 ,922.6 ,764.4 ,28 ,29.07 \\r\\n154.8 ,0.0 ,142.8 ,193.3 ,9.1 ,877.2 ,867.7 ,28 ,9.74 \\r\\n255.3 ,98.8 ,77.0 ,188.6 ,6.5 ,919.0 ,749.3 ,28 ,33.80 \\r\\n272.8 ,105.1 ,81.8 ,209.7 ,9.0 ,904.0 ,679.7 ,28 ,37.17 \\r\\n162.0 ,190.1 ,148.1 ,178.8 ,18.8 ,838.1 ,741.4 ,28 ,33.76 \\r\\n153.6 ,144.2 ,112.3 ,220.1 ,10.1 ,923.2 ,657.9 ,28 ,16.50 \\r\\n146.5 ,114.6 ,89.3 ,201.9 ,8.8 ,860.0 ,829.5 ,28 ,19.99 \\r\\n151.8 ,178.1 ,138.7 ,167.5 ,18.3 ,944.0 ,694.6 ,28 ,36.35 \\r\\n309.9 ,142.8 ,111.2 ,167.8 ,22.1 ,913.9 ,651.2 ,28 ,38.22 \\r\\n143.6 ,0.0 ,174.9 ,158.4 ,17.9 ,942.7 ,844.5 ,28 ,15.42 \\r\\n303.6 ,139.9 ,0.0 ,213.5 ,6.2 ,895.5 ,722.5 ,28 ,33.42 \\r\\n374.3 ,0.0 ,0.0 ,190.2 ,6.7 ,1013.2 ,730.4 ,28 ,39.06 \\r\\n158.6 ,148.9 ,116.0 ,175.1 ,15.0 ,953.3 ,719.7 ,28 ,27.68 \\r\\n152.6 ,238.7 ,0.0 ,200.0 ,6.3 ,1001.8 ,683.9 ,28 ,26.86 \\r\\n310.0 ,142.8 ,0.0 ,167.9 ,10.0 ,914.3 ,804.0 ,28 ,45.30 \\r\\n304.8 ,0.0 ,99.6 ,196.0 ,9.8 ,959.4 ,705.2 ,28 ,30.12 \\r\\n150.9 ,0.0 ,183.9 ,166.6 ,11.6 ,991.2 ,772.2 ,28 ,15.57 \\r\\n141.9 ,166.6 ,129.7 ,173.5 ,10.9 ,882.6 ,785.3 ,28 ,44.61 \\r\\n297.8 ,137.2 ,106.9 ,201.3 ,6.0 ,878.4 ,655.3 ,28 ,53.52 \\r\\n321.3 ,164.2 ,0.0 ,190.5 ,4.6 ,870.0 ,774.0 ,28 ,57.22 \\r\\n366.0 ,187.0 ,0.0 ,191.3 ,6.6 ,824.3 ,756.9 ,28 ,65.91 \\r\\n279.8 ,128.9 ,100.4 ,172.4 ,9.5 ,825.1 ,804.9 ,28 ,52.83 \\r\\n252.1 ,97.1 ,75.6 ,193.8 ,8.3 ,835.5 ,821.4 ,28 ,33.40 \\r\\n164.6 ,0.0 ,150.4 ,181.6 ,11.7 ,1023.3 ,728.9 ,28 ,18.03 \\r\\n155.6 ,243.5 ,0.0 ,180.3 ,10.7 ,1022.0 ,697.7 ,28 ,37.36 \\r\\n160.2 ,188.0 ,146.4 ,203.2 ,11.3 ,828.7 ,709.7 ,28 ,35.31 \\r\\n298.1 ,0.0 ,107.0 ,186.4 ,6.1 ,879.0 ,815.2 ,28 ,42.64 \\r\\n317.9 ,0.0 ,126.5 ,209.7 ,5.7 ,860.5 ,736.6 ,28 ,40.06 \\r\\n287.3 ,120.5 ,93.9 ,187.6 ,9.2 ,904.4 ,695.9 ,28 ,43.80 \\r\\n325.6 ,166.4 ,0.0 ,174.0 ,8.9 ,881.6 ,790.0 ,28 ,61.24 \\r\\n355.9 ,0.0 ,141.6 ,193.3 ,11.0 ,801.4 ,778.4 ,28 ,40.87 \\r\\n132.0 ,206.5 ,160.9 ,178.9 ,5.5 ,866.9 ,735.6 ,28 ,33.31 \\r\\n322.5 ,148.6 ,0.0 ,185.8 ,8.5 ,951.0 ,709.5 ,28 ,52.43 \\r\\n164.2 ,0.0 ,200.1 ,181.2 ,12.6 ,849.3 ,846.0 ,28 ,15.09 \\r\\n313.8 ,0.0 ,112.6 ,169.9 ,10.1 ,925.3 ,782.9 ,28 ,38.46 \\r\\n321.4 ,0.0 ,127.9 ,182.5 ,11.5 ,870.1 ,779.7 ,28 ,37.27 \\r\\n139.7 ,163.9 ,127.7 ,236.7 ,5.8 ,868.6 ,655.6 ,28 ,35.23 \\r\\n288.4 ,121.0 ,0.0 ,177.4 ,7.0 ,907.9 ,829.5 ,28 ,42.14 \\r\\n298.2 ,0.0 ,107.0 ,209.7 ,11.1 ,879.6 ,744.2 ,28 ,31.88 \\r\\n264.5 ,111.0 ,86.5 ,195.5 ,5.9 ,832.6 ,790.4 ,28 ,41.54 \\r\\n159.8 ,250.0 ,0.0 ,168.4 ,12.2 ,1049.3 ,688.2 ,28 ,39.46 \\r\\n166.0 ,259.7 ,0.0 ,183.2 ,12.7 ,858.8 ,826.8 ,28 ,37.92 \\r\\n276.4 ,116.0 ,90.3 ,179.6 ,8.9 ,870.1 ,768.3 ,28 ,44.28 \\r\\n322.2 ,0.0 ,115.6 ,196.0 ,10.4 ,817.9 ,813.4 ,28 ,31.18 \\r\\n148.5 ,139.4 ,108.6 ,192.7 ,6.1 ,892.4 ,780.0 ,28 ,23.70 \\r\\n159.1 ,186.7 ,0.0 ,175.6 ,11.3 ,989.6 ,788.9 ,28 ,32.77 \\r\\n260.9 ,100.5 ,78.3 ,200.6 ,8.6 ,864.5 ,761.5 ,28 ,32.40 '}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5F_jljvskNkM"
      },
      "source": [
        "# Looking at Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "KvzNjzgDkKDc",
        "outputId": "565b19f1-a6fb-493b-aa61-968d467486cf"
      },
      "source": [
        "df = pd.read_csv(\"/content/concrete_data.csv\")\n",
        "df.head()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Cement</th>\n",
              "      <th>Blast Furnace Slag</th>\n",
              "      <th>Fly Ash</th>\n",
              "      <th>Water</th>\n",
              "      <th>Superplasticizer</th>\n",
              "      <th>Coarse Aggregate</th>\n",
              "      <th>Fine Aggregate</th>\n",
              "      <th>Age</th>\n",
              "      <th>Strength</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>540.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>162.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>1040.0</td>\n",
              "      <td>676.0</td>\n",
              "      <td>28</td>\n",
              "      <td>79.99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>540.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>162.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>1055.0</td>\n",
              "      <td>676.0</td>\n",
              "      <td>28</td>\n",
              "      <td>61.89</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>332.5</td>\n",
              "      <td>142.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>228.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>932.0</td>\n",
              "      <td>594.0</td>\n",
              "      <td>270</td>\n",
              "      <td>40.27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>332.5</td>\n",
              "      <td>142.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>228.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>932.0</td>\n",
              "      <td>594.0</td>\n",
              "      <td>365</td>\n",
              "      <td>41.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>198.6</td>\n",
              "      <td>132.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>192.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>978.4</td>\n",
              "      <td>825.5</td>\n",
              "      <td>360</td>\n",
              "      <td>44.30</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Cement  Blast Furnace Slag  Fly Ash  ...  Fine Aggregate  Age  Strength\n",
              "0   540.0                 0.0      0.0  ...           676.0   28     79.99\n",
              "1   540.0                 0.0      0.0  ...           676.0   28     61.89\n",
              "2   332.5               142.5      0.0  ...           594.0  270     40.27\n",
              "3   332.5               142.5      0.0  ...           594.0  365     41.05\n",
              "4   198.6               132.4      0.0  ...           825.5  360     44.30\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMcbBlrqob3c",
        "outputId": "eb16ce64-613b-4445-c9ce-486c291da3e4"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1030, 9)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygw0AArK6BHk"
      },
      "source": [
        "#Standardizing Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "QeVdeKpx6AZp",
        "outputId": "7dc05f15-b8fe-4a02-9967-3997111d3222"
      },
      "source": [
        "label_cols = ['Age', 'Strength']\n",
        "columns_to_standardize = df.drop(columns=label_cols).columns\n",
        "means = df.loc[:,columns_to_standardize].mean()\n",
        "stds = df.loc[:,columns_to_standardize].std()\n",
        "df.loc[:,columns_to_standardize] = (df.loc[:,columns_to_standardize]-means)/stds\n",
        "df.head()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Cement</th>\n",
              "      <th>Blast Furnace Slag</th>\n",
              "      <th>Fly Ash</th>\n",
              "      <th>Water</th>\n",
              "      <th>Superplasticizer</th>\n",
              "      <th>Coarse Aggregate</th>\n",
              "      <th>Fine Aggregate</th>\n",
              "      <th>Age</th>\n",
              "      <th>Strength</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.476712</td>\n",
              "      <td>-0.856472</td>\n",
              "      <td>-0.846733</td>\n",
              "      <td>-0.916319</td>\n",
              "      <td>-0.620147</td>\n",
              "      <td>0.862735</td>\n",
              "      <td>-1.217079</td>\n",
              "      <td>28</td>\n",
              "      <td>79.99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.476712</td>\n",
              "      <td>-0.856472</td>\n",
              "      <td>-0.846733</td>\n",
              "      <td>-0.916319</td>\n",
              "      <td>-0.620147</td>\n",
              "      <td>1.055651</td>\n",
              "      <td>-1.217079</td>\n",
              "      <td>28</td>\n",
              "      <td>61.89</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.491187</td>\n",
              "      <td>0.795140</td>\n",
              "      <td>-0.846733</td>\n",
              "      <td>2.174405</td>\n",
              "      <td>-1.038638</td>\n",
              "      <td>-0.526262</td>\n",
              "      <td>-2.239829</td>\n",
              "      <td>270</td>\n",
              "      <td>40.27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.491187</td>\n",
              "      <td>0.795140</td>\n",
              "      <td>-0.846733</td>\n",
              "      <td>2.174405</td>\n",
              "      <td>-1.038638</td>\n",
              "      <td>-0.526262</td>\n",
              "      <td>-2.239829</td>\n",
              "      <td>365</td>\n",
              "      <td>41.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.790075</td>\n",
              "      <td>0.678079</td>\n",
              "      <td>-0.846733</td>\n",
              "      <td>0.488555</td>\n",
              "      <td>-1.038638</td>\n",
              "      <td>0.070492</td>\n",
              "      <td>0.647569</td>\n",
              "      <td>360</td>\n",
              "      <td>44.30</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Cement  Blast Furnace Slag   Fly Ash  ...  Fine Aggregate  Age  Strength\n",
              "0  2.476712           -0.856472 -0.846733  ...       -1.217079   28     79.99\n",
              "1  2.476712           -0.856472 -0.846733  ...       -1.217079   28     61.89\n",
              "2  0.491187            0.795140 -0.846733  ...       -2.239829  270     40.27\n",
              "3  0.491187            0.795140 -0.846733  ...       -2.239829  365     41.05\n",
              "4 -0.790075            0.678079 -0.846733  ...        0.647569  360     44.30\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZXafBZkkXV9"
      },
      "source": [
        "#Separating our features and targets\n",
        "\n",
        "Age and Strength are targets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXV8daqJkiFL"
      },
      "source": [
        "features = df.drop(columns=['Age', 'Strength'])\n",
        "targets = df.loc[:, [\"Age\", \"Strength\"]]"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VFpSdPf-Quv",
        "outputId": "daafe744-575b-489c-bbcb-f9ecfaf3f32a"
      },
      "source": [
        "features.mean().values"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 2.43222379e-15, -8.51368598e-16,  3.83781464e-16,  1.84674282e-15,\n",
              "       -9.64115519e-16,  6.81870957e-15,  1.23257122e-14])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrADro_Clvga"
      },
      "source": [
        "# Creating model using PyTorch\n",
        "\n",
        "When implementing the active learning algorithm, it will use PyTorch to retrain the model with newly sampled data. This will be a simple neural network, with no hidden layers. This really constructs two Linear Regression models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbJs6UnUl1Fd",
        "outputId": "559c6898-d8ea-4b55-9e9f-d1bb6e729eb3"
      },
      "source": [
        "class neural_network(nn.Module):\n",
        "    def __init__(self, num_features, num_labels):\n",
        "        super(neural_network, self).__init__()\n",
        "        self.fc = nn.Linear(num_features, num_labels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Takes input x, and passes that through the network\n",
        "        return self.fc(x)\n",
        "\n",
        "# THis gets the number of columns in the features df\n",
        "num_features = features.shape[1]\n",
        "\n",
        "# This gets the number of targets\n",
        "num_targets = targets.shape[1]\n",
        "\n",
        "model = neural_network(num_features, num_targets)\n",
        "\n",
        "print(model)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "neural_network(\n",
            "  (fc): Linear(in_features=7, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leoE-F_Dnyyi"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Defining Loss function\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "# Defining Optimizer with l2 regularization since the dataset is quite small\n",
        "optimizer = optim.Adam(model.parameters(), weight_decay=0.001, lr=0.001)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nueJjJlHpeR_"
      },
      "source": [
        "# Function to train and optimize model\n",
        "def train_model(model, num_epochs, loss_fn, optimizer, training_features, training_labels):\n",
        "    for epoch in range(1, num_epochs+1):\n",
        "        # Setting optimizer gradients to zero\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Getting predictions\n",
        "        preds = model(training_features)\n",
        "\n",
        "        # Calculating loss with L2 regularization\n",
        "        loss = loss_fn(preds, training_labels)\n",
        "\n",
        "        # Calculating Gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # Taking a gradient descent step\n",
        "        optimizer.step()\n",
        "\n",
        "        #print('Epoch:{}\\t MSE_Loss:{}'.format(epoch, loss.item()))"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2UvwBBbk0wh"
      },
      "source": [
        "# Active Learning Sampling Technique\n",
        "\n",
        "I will be implementing all the alogorithms from this paper: https://arxiv.org/pdf/1808.04245.pdf\n",
        "\n",
        "Especially *Improved Greedy Sampling* (iGS) technique from section 2.3 of the paper."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJkicXeDaKEB"
      },
      "source": [
        "## GSx Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYch8XwmlNJJ"
      },
      "source": [
        "from sklearn.metrics import pairwise_distances_argmin_min\n",
        "\n",
        "# This is the function that computes distance for GSx algorithm (Equation 2 in Section 2.1)\n",
        "def distance_GSx(unlabeled_sample, K_list):\n",
        "    \"\"\"\n",
        "    This function is meant to select the current unlabelled sample and calculate it's distance to all the other \n",
        "    samples in K, where the samples in K are all the samples that will be used to label, then construct a model\n",
        "    to label the other unlabelled data.\n",
        "\n",
        "    In this function we will select minimum distance from the unlabelled samples to all selected samples in K.\n",
        "    This minimum distance will be returned for all non-selected unlabelled distance, which then it will select\n",
        "    the unlabelled sample with the MAXIMUM of the minimum distances, and that unlabelled sample will be selected to\n",
        "    be in K. So K will contain the samples that are incremently farthest away from eachother, therefore K will contain\n",
        "    a good variance that will represent the dataset. Then all K values will be labelled by hand.\n",
        "\n",
        "    params:\n",
        "        unlabeled_sample (np.array) - The unlabelled sample that is used to find the closest sampled K value\n",
        "        K_list (list) - A list of all current chosen samples that will be labelled.\n",
        "\n",
        "    returns:\n",
        "        The minimum distance of all the distances between the unlabelled sample and all K samples \n",
        "    \"\"\"\n",
        "    distances = []\n",
        "    for k_sample in K_list:\n",
        "\n",
        "        # This takes the euchilidean distance from the labeled sample(k) and unlabeled sample(m)\n",
        "        distance = np.linalg.norm(k_sample - unlabeled_sample)\n",
        "        distances.append(distance)\n",
        "    return min(distances)\n",
        "\n",
        "# This will use GSx method from paper to get the first batch of data to then label\n",
        "def GSx_Sampling(features, K, true_dataset=None, label_cols=None):\n",
        "    \"\"\"\n",
        "    params:\n",
        "        features (pd.DataFrame) - This is the dataset that is unlabelled, which will provide samples to hand label\n",
        "                                before constructing a model to then use predictions to label\n",
        "        K (int) - This is the total amount of samples that would be hand labelled before constructing a model\n",
        "        true_dataset (pd.DataFrame) - This is the real dataset to sample labels from since this is for simulation\n",
        "        label_cols (list) - A list of all label column names in true_dataset to label the K samples\n",
        "\n",
        "    returns:\n",
        "        This will return a labelled dataset of the samples that were selected and unlabelled dataset if the real dataset is given.\n",
        "        If not, it will provide the features that were selected.\n",
        "    \"\"\"\n",
        "\n",
        "    # Dictionary that is going to hold the chosen samples and their indices\n",
        "    K_set = {'samples':[], 'indices':[]}\n",
        "\n",
        "    # Finding the centroid by taking the mean of all variables\n",
        "    centroid = features.mean().values\n",
        "\n",
        "    # using sklearn, finding closest sample to centroid\n",
        "    closest_idx, _ = pairwise_distances_argmin_min(centroid.reshape(1, -1), features)\n",
        "    \n",
        "    # Grabbing the index integer in the array \n",
        "    closest_idx = closest_idx[0]\n",
        "\n",
        "    # Placing first k sample and index into K_set\n",
        "    first_k = features.iloc[closest_idx].values\n",
        "    K_set['samples'].append(first_k)\n",
        "    K_set['indices'].append(closest_idx)\n",
        "    \n",
        "    \n",
        "    # Removing first_k from unlabeled features\n",
        "    features = features.drop(closest_idx, axis=0)\n",
        "\n",
        "    # This will run K-1 times to select K-1 samples for K\n",
        "    for k in range(1, K):\n",
        "        # This will save all the distances for this iteration\n",
        "        GSx_distances = []\n",
        "        \n",
        "        for sample_idx in features.index:\n",
        "            \n",
        "            # Computing the minimum distances of current unlabelled sample and all K samples\n",
        "            min_distance = distance_GSx(features.loc[sample_idx].values, K_set['samples'])\n",
        "                \n",
        "            # Appending the minimum distance\n",
        "            GSx_distances.append(min_distance)\n",
        "\n",
        "        # Grabbing the sample idx with the largest minimum distance to a single k value\n",
        "        new_k_sample_idx = features.index[np.argmax(GSx_distances)]\n",
        "        \n",
        "        # Placing new k sample and index into K_set\n",
        "        new_k_sample = features.loc[new_k_sample_idx].values\n",
        "        K_set['samples'].append(new_k_sample)\n",
        "        K_set['indices'].append(new_k_sample_idx)\n",
        "\n",
        "        # Delecting the new k sample from features\n",
        "        features = features.drop(new_k_sample_idx, axis=0)\n",
        "    \n",
        "    # Creating a dataframe of K samples\n",
        "    K_df = pd.DataFrame(K_set['samples'], columns=features.columns, index=K_set['indices'])\n",
        "    \n",
        "    # This will return K samples and their labels if they are given\n",
        "    if type(true_dataset) == pd.core.frame.DataFrame and type(label_cols) == list:\n",
        "        labels = true_dataset.loc[K_set['indices'], label_cols]\n",
        "        return K_df, labels\n",
        "\n",
        "    # This will only return the K samples if the true dataset was not given\n",
        "    return K_df\n",
        "\n",
        "def GSx_Algorithm(features, K, model, true_dataset=None, label_cols=None, epochs=None):\n",
        "    \"\"\"\n",
        "    params:\n",
        "        features (pd.DataFrame) - This is the dataset that is unlabelled, which will provide samples to hand label\n",
        "                                before constructing a model to then use predictions to label\n",
        "        K (int) - This is the total amount of samples that would be hand labelled before constructing a model\n",
        "        model (PyTorch Model) - This is the model that will be trained with the selected K samples\n",
        "        true_dataset (pd.DataFrame) - This is the real dataset to sample labels from since this is for simulation\n",
        "        label_cols (list) - A list of all label column names in true_dataset to label the K samples\n",
        "\n",
        "    returns:\n",
        "        This will return a labelled dataset of the samples that were selected and unlabelled dataset if the real dataset is given.\n",
        "        If not, it will provide the features that were selected.\n",
        "    \"\"\"\n",
        "\n",
        "    # Dictionary that is going to hold the chosen samples and their indices\n",
        "    K_set = {'samples':[], 'indices':[]}\n",
        "\n",
        "    # Finding the centroid by taking the mean of all variables\n",
        "    centroid = features.mean().values\n",
        "\n",
        "    # using sklearn, finding closest sample to centroid\n",
        "    closest_idx, _ = pairwise_distances_argmin_min(centroid.reshape(1, -1), features)\n",
        "    \n",
        "    # Grabbing the index integer in the array \n",
        "    closest_idx = closest_idx[0]\n",
        "\n",
        "    # Placing first k sample and index into K_set\n",
        "    first_k = features.iloc[closest_idx].values\n",
        "    K_set['samples'].append(first_k)\n",
        "    K_set['indices'].append(closest_idx)\n",
        "    \n",
        "    \n",
        "    # Removing first_k from unlabeled features\n",
        "    features = features.drop(closest_idx, axis=0)\n",
        "\n",
        "    # This will run K-1 times to select K-1 samples for K\n",
        "    for k in range(1, K):\n",
        "        print('Getting k={} sample'.format(k))\n",
        "        # This will save all the distances for this iteration\n",
        "        GSx_distances = []\n",
        "        \n",
        "        for sample_idx in features.index:\n",
        "            \n",
        "            # Computing the minimum distances of current unlabelled sample and all K samples\n",
        "            min_distance = distance_GSx(features.loc[sample_idx].values, K_set['samples'])\n",
        "                \n",
        "            # Appending the minimum distance\n",
        "            GSx_distances.append(min_distance)\n",
        "\n",
        "        # Grabbing the sample idx with the largest minimum distance to a single k value\n",
        "        new_k_sample_idx = features.index[np.argmax(GSx_distances)]\n",
        "        \n",
        "        # Placing new k sample and index into K_set\n",
        "        new_k_sample = features.loc[new_k_sample_idx].values\n",
        "        K_set['samples'].append(new_k_sample)\n",
        "        K_set['indices'].append(new_k_sample_idx)\n",
        "\n",
        "        # Delecting the new k sample from features\n",
        "        features = features.drop(new_k_sample_idx, axis=0)\n",
        "    \n",
        "    # Creating a dataframe of K samples\n",
        "    K_features = pd.DataFrame(K_set['samples'], columns=features.columns, index=K_set['indices'])\n",
        "    \n",
        "    # Getting K_labels\n",
        "    K_labels = true_dataset.loc[K_set['indices'], label_cols]\n",
        "\n",
        "    # Converting K features and labels to PyTorch tensors\n",
        "    K_torch_features = torch.tensor(K_features.values, dtype=torch.float)\n",
        "    K_torch_labels = torch.tensor(K_labels.values, dtype=torch.float)\n",
        "\n",
        "    print('Training Model with all K values')\n",
        "    \n",
        "    train_model(model, epochs, loss_fn, optimizer, K_torch_features, K_torch_labels)\n",
        "    K_data = pd.concat([K_features, K_labels], axis=1)\n",
        "    return model, K_data, features"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeraNIDwO2Nw",
        "outputId": "6817af88-3814-40d7-c4b1-0850072ab193"
      },
      "source": [
        "GSx_K, GSx_data, unlabeled_data = GSx_Algorithm(features, K=100, model=model, true_dataset=df, label_cols=['Age', 'Strength'], epochs=50)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting k=1 sample\n",
            "Getting k=2 sample\n",
            "Getting k=3 sample\n",
            "Getting k=4 sample\n",
            "Getting k=5 sample\n",
            "Getting k=6 sample\n",
            "Getting k=7 sample\n",
            "Getting k=8 sample\n",
            "Getting k=9 sample\n",
            "Getting k=10 sample\n",
            "Getting k=11 sample\n",
            "Getting k=12 sample\n",
            "Getting k=13 sample\n",
            "Getting k=14 sample\n",
            "Getting k=15 sample\n",
            "Getting k=16 sample\n",
            "Getting k=17 sample\n",
            "Getting k=18 sample\n",
            "Getting k=19 sample\n",
            "Training Model with all K values\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y30QJAyYaoij"
      },
      "source": [
        "## GSy Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wkq7joxpaqyk"
      },
      "source": [
        "# This is the function that computes distance for GSy algorithm (Equation 2 in Section 2.2)\n",
        "def distance_GSy(unlabeled_prediction, K_0_labels):\n",
        "    \"\"\"\n",
        "    This function is meant to select the current unlabelled sample models prediction, f(x), and calculate the distance\n",
        "    of each label of the K samples. The model f(x), is trained on the samples and labels of K_0 sampled using GSx.\n",
        "\n",
        "    In this function we will select minimum distance from predictions from the unlabelled samples to all lables in K.\n",
        "    This minimum distance will be returned for all non-selected unlabelled samples, which then it will select\n",
        "    the unlabelled sample with the MAXIMUM of the minimum distances, and that unlabelled sample will be selected to\n",
        "    be in K. So K will contain the samples that have higher variation in the target space.\n",
        "\n",
        "    params:\n",
        "        unlabeled_sample (np.array) - The unlabelled prediction that is used to find the closest sampled K label\n",
        "        K_0_labels (pd.Series or pd.DataFrame) - A Series or DataFrame of all labels collected for K_0 labels from GSx algo\n",
        "\n",
        "    returns:\n",
        "        The minimum distance of all the distances between the unlabelled sample's prediction and all K samples' predictions\n",
        "    \"\"\"\n",
        "    distances = []\n",
        "\n",
        "    # If there are multiple targets, will use euchlidean distance\n",
        "    if K_0_labels.values.ndim > 1:\n",
        "        for k_label in K_0_labels.values:\n",
        "\n",
        "            # This takes the euchilidean distance from the labeled sample(k) and unlabeled sample(m)\n",
        "            distance = np.linalg.norm(unlabeled_prediction - k_label)\n",
        "            distances.append(distance)\n",
        "    # If the targets are 1-dimensional then you will just take the absolute value difference\n",
        "    else:\n",
        "        for k_label in K_0_labels.values:\n",
        "            distances.append(np.abs(unlabeled_prediction - k_label))\n",
        "\n",
        "    return min(distances)\n",
        "\n",
        "# This will use GSy method from paper to get a model able to make predictions from \n",
        "def GSy_Algorithm(features, K, model, true_dataset=None, label_cols=None, K_0=None, epochs=None):\n",
        "    \"\"\"\n",
        "    params:\n",
        "        features (pd.DataFrame) - This is the dataset that is unlabelled, which will provide samples to hand label\n",
        "                                before constructing a model to then use predictions to label\n",
        "        K (int) - This is the total amount of samples that would be hand labelled before constructing a model\n",
        "        model (PyTorch Model) - This is the model that will be trained overtime\n",
        "        true_dataset (pd.DataFrame) - This is the real dataset to sample labels from since this is for simulation\n",
        "        label_cols (list) - A list of all label column names in true_dataset to label the K samples\n",
        "        K_0 (int) - The number of initial k samples that are sampled from the GSx algorithm to then train the model.\n",
        "                    According to the authors, it is set to the number of features in the dataset.\n",
        "        epochs (int) - The number of epochs to train the model for each update\n",
        "\n",
        "    returns:\n",
        "        This will return a PyTorch model trained on the K labelled Samples, the leftover unlabelled data to predict,\n",
        "        and the Final K dataset\n",
        "    \"\"\"\n",
        "    \n",
        "    # Initializing K_0 to the number of features\n",
        "    if K_0 == None:\n",
        "        K_0 = features.shape[1]\n",
        "\n",
        "    print('Running GSx Algorithm to get K_0 data')\n",
        "    # Running GSx algorithm to collect K_0 samples\n",
        "    K_0_features, K_0_labels = GSx_Sampling(features, K=K_0, true_dataset=true_dataset, label_cols=label_cols)\n",
        "\n",
        "    # Removing labeled K_0 data from features\n",
        "    features = features.drop(index = K_0_features.index)\n",
        "\n",
        "    # Creating Torch tensors from K_0 data to train\n",
        "    K_torch_features = torch.tensor(K_0_features.values, dtype=torch.float)\n",
        "    K_torch_labels = torch.tensor(K_0_labels.values, dtype=torch.float)\n",
        "\n",
        "    # Training the PyTorch model with K_0 data\n",
        "    print('Training Model on K_0 Data')\n",
        "    train_model(model, epochs, loss_fn, optimizer, K_torch_features, K_torch_labels)\n",
        "\n",
        "    print('\\n\\nNow Finding K-K_0 Samples')\n",
        "\n",
        "    # Renaming K_0 data to K data since we will be adding to it\n",
        "    K_features, K_labels= K_0_features, K_0_labels\n",
        "\n",
        "    # Sampling the K-K_0 samples\n",
        "    #K_0, K+1\n",
        "    for k in range(K_0+1, K+1):\n",
        "        # This will save all the distances for this iteration\n",
        "        GSy_distances = []\n",
        "        \n",
        "        for sample_idx in features.index:\n",
        "\n",
        "            unlabeled_sample = features.loc[sample_idx].values\n",
        "            unlabeled_prediction = model(torch.tensor(unlabeled_sample, dtype=torch.float))\n",
        "\n",
        "            # Computing the minimum distances of current unlabelled sample's prediction and all K samples labels\n",
        "            min_distance = distance_GSy(unlabeled_prediction.detach().numpy(), K_labels)\n",
        "                \n",
        "            # Appending the minimum distance\n",
        "            GSy_distances.append(min_distance)\n",
        "\n",
        "        # Grabbing the sample idx with the largest minimum GSy distance\n",
        "        new_k_sample_idx = features.index[np.argmax(GSy_distances)]\n",
        "        \n",
        "        # Placing new k sample and label into the K dataframes\n",
        "        new_k_sample = features.loc[new_k_sample_idx]\n",
        "        new_k_label = true_dataset.loc[new_k_sample_idx, label_cols]\n",
        "        K_features = K_features.append(new_k_sample)\n",
        "        K_labels = K_labels.append(new_k_label)\n",
        "\n",
        "        # Delecting the new k sample from features\n",
        "        features = features.drop(new_k_sample_idx, axis=0)\n",
        "        \n",
        "        # Retraining PyTorch model with new value\n",
        "        print('Retraining PyTorch model for new k sample, k={}'.format(k))\n",
        "        K_torch_features = torch.tensor(K_features.values, dtype=torch.float)\n",
        "        K_torch_labels = torch.tensor(K_labels.values, dtype=torch.float)\n",
        "        train_model(model, epochs, loss_fn, optimizer, K_torch_features, K_torch_labels)\n",
        "\n",
        "    return model, features, pd.concat([K_features, K_labels], axis=1)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "runxRsZ2sduw",
        "outputId": "b4ad323b-4400-4f07-f246-84355ae712f9"
      },
      "source": [
        "# K can be a subset of the original data, I am choosing 20% of original data\n",
        "K=int(df.shape[0] * 0.2)\n",
        "output = GSy_Algorithm(features, K, model, true_dataset=df, label_cols=['Age', 'Strength'], epochs=50)\n",
        "model, unlabeled_data, labeled_data = output"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running GSx Algorithm to get K_0 data\n",
            "Training Model on K_0 Data\n",
            "\n",
            "\n",
            "Now Finding K-K_0 Samples\n",
            "Retraining PyTorch model for new k sample, k=8\n",
            "Retraining PyTorch model for new k sample, k=9\n",
            "Retraining PyTorch model for new k sample, k=10\n",
            "Retraining PyTorch model for new k sample, k=11\n",
            "Retraining PyTorch model for new k sample, k=12\n",
            "Retraining PyTorch model for new k sample, k=13\n",
            "Retraining PyTorch model for new k sample, k=14\n",
            "Retraining PyTorch model for new k sample, k=15\n",
            "Retraining PyTorch model for new k sample, k=16\n",
            "Retraining PyTorch model for new k sample, k=17\n",
            "Retraining PyTorch model for new k sample, k=18\n",
            "Retraining PyTorch model for new k sample, k=19\n",
            "Retraining PyTorch model for new k sample, k=20\n",
            "Retraining PyTorch model for new k sample, k=21\n",
            "Retraining PyTorch model for new k sample, k=22\n",
            "Retraining PyTorch model for new k sample, k=23\n",
            "Retraining PyTorch model for new k sample, k=24\n",
            "Retraining PyTorch model for new k sample, k=25\n",
            "Retraining PyTorch model for new k sample, k=26\n",
            "Retraining PyTorch model for new k sample, k=27\n",
            "Retraining PyTorch model for new k sample, k=28\n",
            "Retraining PyTorch model for new k sample, k=29\n",
            "Retraining PyTorch model for new k sample, k=30\n",
            "Retraining PyTorch model for new k sample, k=31\n",
            "Retraining PyTorch model for new k sample, k=32\n",
            "Retraining PyTorch model for new k sample, k=33\n",
            "Retraining PyTorch model for new k sample, k=34\n",
            "Retraining PyTorch model for new k sample, k=35\n",
            "Retraining PyTorch model for new k sample, k=36\n",
            "Retraining PyTorch model for new k sample, k=37\n",
            "Retraining PyTorch model for new k sample, k=38\n",
            "Retraining PyTorch model for new k sample, k=39\n",
            "Retraining PyTorch model for new k sample, k=40\n",
            "Retraining PyTorch model for new k sample, k=41\n",
            "Retraining PyTorch model for new k sample, k=42\n",
            "Retraining PyTorch model for new k sample, k=43\n",
            "Retraining PyTorch model for new k sample, k=44\n",
            "Retraining PyTorch model for new k sample, k=45\n",
            "Retraining PyTorch model for new k sample, k=46\n",
            "Retraining PyTorch model for new k sample, k=47\n",
            "Retraining PyTorch model for new k sample, k=48\n",
            "Retraining PyTorch model for new k sample, k=49\n",
            "Retraining PyTorch model for new k sample, k=50\n",
            "Retraining PyTorch model for new k sample, k=51\n",
            "Retraining PyTorch model for new k sample, k=52\n",
            "Retraining PyTorch model for new k sample, k=53\n",
            "Retraining PyTorch model for new k sample, k=54\n",
            "Retraining PyTorch model for new k sample, k=55\n",
            "Retraining PyTorch model for new k sample, k=56\n",
            "Retraining PyTorch model for new k sample, k=57\n",
            "Retraining PyTorch model for new k sample, k=58\n",
            "Retraining PyTorch model for new k sample, k=59\n",
            "Retraining PyTorch model for new k sample, k=60\n",
            "Retraining PyTorch model for new k sample, k=61\n",
            "Retraining PyTorch model for new k sample, k=62\n",
            "Retraining PyTorch model for new k sample, k=63\n",
            "Retraining PyTorch model for new k sample, k=64\n",
            "Retraining PyTorch model for new k sample, k=65\n",
            "Retraining PyTorch model for new k sample, k=66\n",
            "Retraining PyTorch model for new k sample, k=67\n",
            "Retraining PyTorch model for new k sample, k=68\n",
            "Retraining PyTorch model for new k sample, k=69\n",
            "Retraining PyTorch model for new k sample, k=70\n",
            "Retraining PyTorch model for new k sample, k=71\n",
            "Retraining PyTorch model for new k sample, k=72\n",
            "Retraining PyTorch model for new k sample, k=73\n",
            "Retraining PyTorch model for new k sample, k=74\n",
            "Retraining PyTorch model for new k sample, k=75\n",
            "Retraining PyTorch model for new k sample, k=76\n",
            "Retraining PyTorch model for new k sample, k=77\n",
            "Retraining PyTorch model for new k sample, k=78\n",
            "Retraining PyTorch model for new k sample, k=79\n",
            "Retraining PyTorch model for new k sample, k=80\n",
            "Retraining PyTorch model for new k sample, k=81\n",
            "Retraining PyTorch model for new k sample, k=82\n",
            "Retraining PyTorch model for new k sample, k=83\n",
            "Retraining PyTorch model for new k sample, k=84\n",
            "Retraining PyTorch model for new k sample, k=85\n",
            "Retraining PyTorch model for new k sample, k=86\n",
            "Retraining PyTorch model for new k sample, k=87\n",
            "Retraining PyTorch model for new k sample, k=88\n",
            "Retraining PyTorch model for new k sample, k=89\n",
            "Retraining PyTorch model for new k sample, k=90\n",
            "Retraining PyTorch model for new k sample, k=91\n",
            "Retraining PyTorch model for new k sample, k=92\n",
            "Retraining PyTorch model for new k sample, k=93\n",
            "Retraining PyTorch model for new k sample, k=94\n",
            "Retraining PyTorch model for new k sample, k=95\n",
            "Retraining PyTorch model for new k sample, k=96\n",
            "Retraining PyTorch model for new k sample, k=97\n",
            "Retraining PyTorch model for new k sample, k=98\n",
            "Retraining PyTorch model for new k sample, k=99\n",
            "Retraining PyTorch model for new k sample, k=100\n",
            "Retraining PyTorch model for new k sample, k=101\n",
            "Retraining PyTorch model for new k sample, k=102\n",
            "Retraining PyTorch model for new k sample, k=103\n",
            "Retraining PyTorch model for new k sample, k=104\n",
            "Retraining PyTorch model for new k sample, k=105\n",
            "Retraining PyTorch model for new k sample, k=106\n",
            "Retraining PyTorch model for new k sample, k=107\n",
            "Retraining PyTorch model for new k sample, k=108\n",
            "Retraining PyTorch model for new k sample, k=109\n",
            "Retraining PyTorch model for new k sample, k=110\n",
            "Retraining PyTorch model for new k sample, k=111\n",
            "Retraining PyTorch model for new k sample, k=112\n",
            "Retraining PyTorch model for new k sample, k=113\n",
            "Retraining PyTorch model for new k sample, k=114\n",
            "Retraining PyTorch model for new k sample, k=115\n",
            "Retraining PyTorch model for new k sample, k=116\n",
            "Retraining PyTorch model for new k sample, k=117\n",
            "Retraining PyTorch model for new k sample, k=118\n",
            "Retraining PyTorch model for new k sample, k=119\n",
            "Retraining PyTorch model for new k sample, k=120\n",
            "Retraining PyTorch model for new k sample, k=121\n",
            "Retraining PyTorch model for new k sample, k=122\n",
            "Retraining PyTorch model for new k sample, k=123\n",
            "Retraining PyTorch model for new k sample, k=124\n",
            "Retraining PyTorch model for new k sample, k=125\n",
            "Retraining PyTorch model for new k sample, k=126\n",
            "Retraining PyTorch model for new k sample, k=127\n",
            "Retraining PyTorch model for new k sample, k=128\n",
            "Retraining PyTorch model for new k sample, k=129\n",
            "Retraining PyTorch model for new k sample, k=130\n",
            "Retraining PyTorch model for new k sample, k=131\n",
            "Retraining PyTorch model for new k sample, k=132\n",
            "Retraining PyTorch model for new k sample, k=133\n",
            "Retraining PyTorch model for new k sample, k=134\n",
            "Retraining PyTorch model for new k sample, k=135\n",
            "Retraining PyTorch model for new k sample, k=136\n",
            "Retraining PyTorch model for new k sample, k=137\n",
            "Retraining PyTorch model for new k sample, k=138\n",
            "Retraining PyTorch model for new k sample, k=139\n",
            "Retraining PyTorch model for new k sample, k=140\n",
            "Retraining PyTorch model for new k sample, k=141\n",
            "Retraining PyTorch model for new k sample, k=142\n",
            "Retraining PyTorch model for new k sample, k=143\n",
            "Retraining PyTorch model for new k sample, k=144\n",
            "Retraining PyTorch model for new k sample, k=145\n",
            "Retraining PyTorch model for new k sample, k=146\n",
            "Retraining PyTorch model for new k sample, k=147\n",
            "Retraining PyTorch model for new k sample, k=148\n",
            "Retraining PyTorch model for new k sample, k=149\n",
            "Retraining PyTorch model for new k sample, k=150\n",
            "Retraining PyTorch model for new k sample, k=151\n",
            "Retraining PyTorch model for new k sample, k=152\n",
            "Retraining PyTorch model for new k sample, k=153\n",
            "Retraining PyTorch model for new k sample, k=154\n",
            "Retraining PyTorch model for new k sample, k=155\n",
            "Retraining PyTorch model for new k sample, k=156\n",
            "Retraining PyTorch model for new k sample, k=157\n",
            "Retraining PyTorch model for new k sample, k=158\n",
            "Retraining PyTorch model for new k sample, k=159\n",
            "Retraining PyTorch model for new k sample, k=160\n",
            "Retraining PyTorch model for new k sample, k=161\n",
            "Retraining PyTorch model for new k sample, k=162\n",
            "Retraining PyTorch model for new k sample, k=163\n",
            "Retraining PyTorch model for new k sample, k=164\n",
            "Retraining PyTorch model for new k sample, k=165\n",
            "Retraining PyTorch model for new k sample, k=166\n",
            "Retraining PyTorch model for new k sample, k=167\n",
            "Retraining PyTorch model for new k sample, k=168\n",
            "Retraining PyTorch model for new k sample, k=169\n",
            "Retraining PyTorch model for new k sample, k=170\n",
            "Retraining PyTorch model for new k sample, k=171\n",
            "Retraining PyTorch model for new k sample, k=172\n",
            "Retraining PyTorch model for new k sample, k=173\n",
            "Retraining PyTorch model for new k sample, k=174\n",
            "Retraining PyTorch model for new k sample, k=175\n",
            "Retraining PyTorch model for new k sample, k=176\n",
            "Retraining PyTorch model for new k sample, k=177\n",
            "Retraining PyTorch model for new k sample, k=178\n",
            "Retraining PyTorch model for new k sample, k=179\n",
            "Retraining PyTorch model for new k sample, k=180\n",
            "Retraining PyTorch model for new k sample, k=181\n",
            "Retraining PyTorch model for new k sample, k=182\n",
            "Retraining PyTorch model for new k sample, k=183\n",
            "Retraining PyTorch model for new k sample, k=184\n",
            "Retraining PyTorch model for new k sample, k=185\n",
            "Retraining PyTorch model for new k sample, k=186\n",
            "Retraining PyTorch model for new k sample, k=187\n",
            "Retraining PyTorch model for new k sample, k=188\n",
            "Retraining PyTorch model for new k sample, k=189\n",
            "Retraining PyTorch model for new k sample, k=190\n",
            "Retraining PyTorch model for new k sample, k=191\n",
            "Retraining PyTorch model for new k sample, k=192\n",
            "Retraining PyTorch model for new k sample, k=193\n",
            "Retraining PyTorch model for new k sample, k=194\n",
            "Retraining PyTorch model for new k sample, k=195\n",
            "Retraining PyTorch model for new k sample, k=196\n",
            "Retraining PyTorch model for new k sample, k=197\n",
            "Retraining PyTorch model for new k sample, k=198\n",
            "Retraining PyTorch model for new k sample, k=199\n",
            "Retraining PyTorch model for new k sample, k=200\n",
            "Retraining PyTorch model for new k sample, k=201\n",
            "Retraining PyTorch model for new k sample, k=202\n",
            "Retraining PyTorch model for new k sample, k=203\n",
            "Retraining PyTorch model for new k sample, k=204\n",
            "Retraining PyTorch model for new k sample, k=205\n",
            "Retraining PyTorch model for new k sample, k=206\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "dASlzKw81h1g",
        "outputId": "bb188770-2982-4e03-9625-d3fcd656158c"
      },
      "source": [
        "labeled_data"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Cement</th>\n",
              "      <th>Blast Furnace Slag</th>\n",
              "      <th>Fly Ash</th>\n",
              "      <th>Water</th>\n",
              "      <th>Superplasticizer</th>\n",
              "      <th>Coarse Aggregate</th>\n",
              "      <th>Fine Aggregate</th>\n",
              "      <th>Age</th>\n",
              "      <th>Strength</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>985</th>\n",
              "      <td>-0.247524</td>\n",
              "      <td>0.288646</td>\n",
              "      <td>0.356449</td>\n",
              "      <td>0.329336</td>\n",
              "      <td>0.049439</td>\n",
              "      <td>-0.693456</td>\n",
              "      <td>-0.302840</td>\n",
              "      <td>28.0</td>\n",
              "      <td>33.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>1.797327</td>\n",
              "      <td>0.501907</td>\n",
              "      <td>-0.846733</td>\n",
              "      <td>-2.049585</td>\n",
              "      <td>4.351528</td>\n",
              "      <td>-1.553862</td>\n",
              "      <td>0.834658</td>\n",
              "      <td>3.0</td>\n",
              "      <td>40.20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>751</th>\n",
              "      <td>2.476712</td>\n",
              "      <td>-0.856472</td>\n",
              "      <td>-0.846733</td>\n",
              "      <td>-0.401199</td>\n",
              "      <td>-1.038638</td>\n",
              "      <td>1.955927</td>\n",
              "      <td>-2.002850</td>\n",
              "      <td>7.0</td>\n",
              "      <td>52.61</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>827</th>\n",
              "      <td>2.304473</td>\n",
              "      <td>-0.856472</td>\n",
              "      <td>-0.846733</td>\n",
              "      <td>-1.665586</td>\n",
              "      <td>-1.038638</td>\n",
              "      <td>-0.989261</td>\n",
              "      <td>1.526885</td>\n",
              "      <td>7.0</td>\n",
              "      <td>50.51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>224</th>\n",
              "      <td>-1.082880</td>\n",
              "      <td>-0.368522</td>\n",
              "      <td>1.712762</td>\n",
              "      <td>-2.798851</td>\n",
              "      <td>-0.084478</td>\n",
              "      <td>1.103237</td>\n",
              "      <td>0.081315</td>\n",
              "      <td>3.0</td>\n",
              "      <td>7.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>926</th>\n",
              "      <td>-1.140293</td>\n",
              "      <td>1.623844</td>\n",
              "      <td>1.715887</td>\n",
              "      <td>0.956847</td>\n",
              "      <td>0.635327</td>\n",
              "      <td>-1.966703</td>\n",
              "      <td>-1.167189</td>\n",
              "      <td>28.0</td>\n",
              "      <td>30.65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>857</th>\n",
              "      <td>-1.427357</td>\n",
              "      <td>1.542712</td>\n",
              "      <td>1.669010</td>\n",
              "      <td>-0.120224</td>\n",
              "      <td>-0.201656</td>\n",
              "      <td>-1.362232</td>\n",
              "      <td>-0.468725</td>\n",
              "      <td>28.0</td>\n",
              "      <td>33.30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>924</th>\n",
              "      <td>-1.389082</td>\n",
              "      <td>1.415219</td>\n",
              "      <td>0.684589</td>\n",
              "      <td>0.816359</td>\n",
              "      <td>-0.034259</td>\n",
              "      <td>-1.619454</td>\n",
              "      <td>0.117485</td>\n",
              "      <td>28.0</td>\n",
              "      <td>26.97</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>904</th>\n",
              "      <td>-1.207274</td>\n",
              "      <td>1.264546</td>\n",
              "      <td>-0.846733</td>\n",
              "      <td>0.535385</td>\n",
              "      <td>0.467930</td>\n",
              "      <td>-1.233621</td>\n",
              "      <td>1.177653</td>\n",
              "      <td>28.0</td>\n",
              "      <td>23.79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>666</th>\n",
              "      <td>-0.853229</td>\n",
              "      <td>2.481523</td>\n",
              "      <td>-0.846733</td>\n",
              "      <td>0.488555</td>\n",
              "      <td>-1.038638</td>\n",
              "      <td>-0.554556</td>\n",
              "      <td>-0.716929</td>\n",
              "      <td>3.0</td>\n",
              "      <td>12.79</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>206 rows  9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Cement  Blast Furnace Slag   Fly Ash  ...  Fine Aggregate   Age  Strength\n",
              "985 -0.247524            0.288646  0.356449  ...       -0.302840  28.0     33.80\n",
              "76   1.797327            0.501907 -0.846733  ...        0.834658   3.0     40.20\n",
              "751  2.476712           -0.856472 -0.846733  ...       -2.002850   7.0     52.61\n",
              "827  2.304473           -0.856472 -0.846733  ...        1.526885   7.0     50.51\n",
              "224 -1.082880           -0.368522  1.712762  ...        0.081315   3.0      7.75\n",
              "..        ...                 ...       ...  ...             ...   ...       ...\n",
              "926 -1.140293            1.623844  1.715887  ...       -1.167189  28.0     30.65\n",
              "857 -1.427357            1.542712  1.669010  ...       -0.468725  28.0     33.30\n",
              "924 -1.389082            1.415219  0.684589  ...        0.117485  28.0     26.97\n",
              "904 -1.207274            1.264546 -0.846733  ...        1.177653  28.0     23.79\n",
              "666 -0.853229            2.481523 -0.846733  ...       -0.716929   3.0     12.79\n",
              "\n",
              "[206 rows x 9 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30qDOgFv1leU",
        "outputId": "6a22f795-4c26-423b-bba8-87c090260bc2"
      },
      "source": [
        "unlabeled_data_tensor = torch.tensor(unlabeled_data.values, dtype=torch.float)\n",
        "labeled_data_tensor = torch.tensor(labeled_data.values, dtype=torch.float)\n",
        "predictions = model(unlabeled_data_tensor)\n",
        "real_labels = torch.tensor(df.loc[unlabeled_data.index, ['Age', 'Strength']].values, dtype=torch.float)\n",
        "loss = loss_fn(predictions, real_labels)\n",
        "print('Loss:', loss)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: tensor(3501.6204, grad_fn=<MseLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eq5QWD5T526V",
        "outputId": "8f1228c5-407e-4a47-bb8a-0e910e95e158"
      },
      "source": [
        "real_labels[:5]"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 28.0000,  79.9900],\n",
              "        [ 28.0000,  61.8900],\n",
              "        [360.0000,  44.3000],\n",
              "        [ 90.0000,  38.0700],\n",
              "        [ 28.0000,  28.0200]])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRZa9Ckz56j6",
        "outputId": "036320b5-1275-4df5-d4cc-149332f34297"
      },
      "source": [
        "predictions[:5]"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[17.2830, 13.1119],\n",
              "        [15.7924, 13.1427],\n",
              "        [20.2455,  0.8679],\n",
              "        [20.2455,  0.8679],\n",
              "        [20.2455,  0.8679]], grad_fn=<SliceBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALMjNuU_FAq9"
      },
      "source": [
        "## iGS Algorithm\n",
        "\n",
        "This is a combination of both GSx and GSy where it finds the best samples to label based on both feature and output space. It is almost identical to GSy where it uses GSx to get the initial K_0 set, but the minimum distance calculated is not a distance but the product of the two distances of GSx and GSy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8YrnNAPGVNi"
      },
      "source": [
        "# This is the function that computes distance for iGS algorithm (Equation 2 in Section 2.3)\n",
        "def distance_iGS(unlabeled_sample, unlabeled_prediction, K_features, K_labels):\n",
        "    \"\"\"\n",
        "    This function combines both distances of GSx and GSy\n",
        "\n",
        "    params:\n",
        "        unlabeled_sample (np.array) - The unlabelled sample that is used to find the closest sampled K sample\n",
        "        unlabeled_prediction (np.array) - The unlabelled prediction that is used to find the closest sampled K label\n",
        "        K_features (pd.DataFrame) - A DataFrame of all K samples\n",
        "        K_labels (pd.Series or pd.DataFrame) - A Series or DataFrame of all labels collected for the K samples\n",
        "\n",
        "    returns:\n",
        "        The minimum distance of all the distances between the unlabelled sample's prediction and all K samples' predictions\n",
        "    \"\"\"\n",
        "    distances = []\n",
        "\n",
        "    # If there are multiple targets, will use euchlidean distance\n",
        "    for i in range(K_features.shape[0]):\n",
        "\n",
        "        # Getting k labels and samples at ith index\n",
        "        k_label = K_labels.values[i]\n",
        "        k_sample = K_features.values[i]\n",
        "\n",
        "        # GSx distance\n",
        "        GSx_distance = np.linalg.norm(k_sample - unlabeled_sample)\n",
        "\n",
        "        # GSy distance\n",
        "        # If there are multiple targets, will use euchlidean distance\n",
        "        if K_labels.values.ndim > 1:\n",
        "            GSy_distance = np.linalg.norm(unlabeled_prediction - k_label)\n",
        "        else:\n",
        "            GSy_distance = np.abs(unlabeled_prediction - k_label)\n",
        "\n",
        "        # Product of both\n",
        "        distances.append(GSx_distance*GSy_distance)\n",
        "\n",
        "    return min(distances)\n",
        "\n",
        "# This will use iGS method from paper to get a model able to make predictions from \n",
        "def iGS_Algorithm(features, K, model, true_dataset=None, label_cols=None, K_0=None, epochs=None):\n",
        "    \"\"\"\n",
        "    params:\n",
        "        features (pd.DataFrame) - This is the dataset that is unlabelled, which will provide samples to hand label\n",
        "                                before constructing a model to then use predictions to label\n",
        "        K (int) - This is the total amount of samples that would be hand labelled before constructing a model\n",
        "        model (PyTorch Model) - This is the model that will be trained overtime\n",
        "        true_dataset (pd.DataFrame) - This is the real dataset to sample labels from since this is for simulation\n",
        "        label_cols (list) - A list of all label column names in true_dataset to label the K samples\n",
        "        K_0 (int) - The number of initial k samples that are sampled from the GSx algorithm to then train the model.\n",
        "                    According to the authors, it is set to the number of features in the dataset.\n",
        "        epochs (int) - The number of epochs to train the model for each update\n",
        "\n",
        "    returns:\n",
        "        This will return a PyTorch model trained on the K labelled Samples, the leftover unlabelled data to predict,\n",
        "        and the Final K dataset\n",
        "    \"\"\"\n",
        "    \n",
        "    # Initializing K_0 to the number of features\n",
        "    if K_0 == None:\n",
        "        K_0 = features.shape[1]\n",
        "\n",
        "    print('Running iGS Algorithm to get K_0 data')\n",
        "    # Running GSx algorithm to collect K_0 samples\n",
        "    K_0_features, K_0_labels = GSx_Sampling(features, K=K_0, true_dataset=true_dataset, label_cols=label_cols)\n",
        "\n",
        "    # Removing labeled K_0 data from features\n",
        "    features = features.drop(index = K_0_features.index)\n",
        "\n",
        "    # Creating Torch tensors from K_0 data to train\n",
        "    K_torch_features = torch.tensor(K_0_features.values, dtype=torch.float)\n",
        "    K_torch_labels = torch.tensor(K_0_labels.values, dtype=torch.float)\n",
        "\n",
        "    # Training the PyTorch model with K_0 data\n",
        "    print('Training Model on K_0 Data')\n",
        "    train_model(model, epochs, loss_fn, optimizer, K_torch_features, K_torch_labels)\n",
        "\n",
        "    print('\\n\\nNow Finding K-K_0 Samples')\n",
        "\n",
        "    # Renaming K_0 data to K data since we will be adding to it\n",
        "    K_features, K_labels = K_0_features, K_0_labels\n",
        "\n",
        "    # Sampling the K-K_0 samples\n",
        "    \n",
        "    for k in range(K_0+1, K+1):\n",
        "        # This will save all the distances for this iteration\n",
        "        iGS_distances = []\n",
        "        \n",
        "        for sample_idx in features.index:\n",
        "\n",
        "            unlabeled_sample = features.loc[sample_idx].values\n",
        "            unlabeled_prediction = model(torch.tensor(unlabeled_sample, dtype=torch.float)).detach().numpy()\n",
        "\n",
        "            # Computing the minimum product of both GSx and GSy distances\n",
        "            min_distance = distance_iGS(unlabeled_sample, unlabeled_prediction, K_features, K_labels)\n",
        "            \n",
        "            # Appending the minimum distance\n",
        "            iGS_distances.append(min_distance)\n",
        "\n",
        "        # Grabbing the sample idx with the largest minimum iGS distance\n",
        "        new_k_sample_idx = features.index[np.argmax(iGS_distances)]\n",
        "        \n",
        "        # Placing new k sample and label into the K dataframes\n",
        "        new_k_sample = features.loc[new_k_sample_idx]\n",
        "        new_k_label = true_dataset.loc[new_k_sample_idx, label_cols]\n",
        "        K_features = K_features.append(new_k_sample)\n",
        "        K_labels = K_labels.append(new_k_label)\n",
        "\n",
        "        # Delecting the new k sample from features\n",
        "        features = features.drop(new_k_sample_idx, axis=0)\n",
        "        \n",
        "        # Retraining PyTorch model with new value\n",
        "        print('Retraining PyTorch model for new k sample, k={}'.format(k))\n",
        "        K_torch_features = torch.tensor(K_features.values, dtype=torch.float)\n",
        "        K_torch_labels = torch.tensor(K_labels.values, dtype=torch.float)\n",
        "        train_model(model, epochs, loss_fn, optimizer, K_torch_features, K_torch_labels)\n",
        "\n",
        "    return model, features, pd.concat([K_features, K_labels], axis=1)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JbYPJg8K-Cu",
        "outputId": "1fcffb5a-6727-470b-c6be-d3a701675c7a"
      },
      "source": [
        "# K can be a subset of the original data, I am choosing 20% of original data\n",
        "K=int(df.shape[0] * 0.2)\n",
        "output = iGS_Algorithm(features, K, model, true_dataset=df, label_cols=['Age', 'Strength'], epochs=50)\n",
        "model, unlabeled_data, labeled_data = output"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running iGS Algorithm to get K_0 data\n",
            "Training Model on K_0 Data\n",
            "\n",
            "\n",
            "Now Finding K-K_0 Samples\n",
            "Retraining PyTorch model for new k sample, k=8\n",
            "Retraining PyTorch model for new k sample, k=9\n",
            "Retraining PyTorch model for new k sample, k=10\n",
            "Retraining PyTorch model for new k sample, k=11\n",
            "Retraining PyTorch model for new k sample, k=12\n",
            "Retraining PyTorch model for new k sample, k=13\n",
            "Retraining PyTorch model for new k sample, k=14\n",
            "Retraining PyTorch model for new k sample, k=15\n",
            "Retraining PyTorch model for new k sample, k=16\n",
            "Retraining PyTorch model for new k sample, k=17\n",
            "Retraining PyTorch model for new k sample, k=18\n",
            "Retraining PyTorch model for new k sample, k=19\n",
            "Retraining PyTorch model for new k sample, k=20\n",
            "Retraining PyTorch model for new k sample, k=21\n",
            "Retraining PyTorch model for new k sample, k=22\n",
            "Retraining PyTorch model for new k sample, k=23\n",
            "Retraining PyTorch model for new k sample, k=24\n",
            "Retraining PyTorch model for new k sample, k=25\n",
            "Retraining PyTorch model for new k sample, k=26\n",
            "Retraining PyTorch model for new k sample, k=27\n",
            "Retraining PyTorch model for new k sample, k=28\n",
            "Retraining PyTorch model for new k sample, k=29\n",
            "Retraining PyTorch model for new k sample, k=30\n",
            "Retraining PyTorch model for new k sample, k=31\n",
            "Retraining PyTorch model for new k sample, k=32\n",
            "Retraining PyTorch model for new k sample, k=33\n",
            "Retraining PyTorch model for new k sample, k=34\n",
            "Retraining PyTorch model for new k sample, k=35\n",
            "Retraining PyTorch model for new k sample, k=36\n",
            "Retraining PyTorch model for new k sample, k=37\n",
            "Retraining PyTorch model for new k sample, k=38\n",
            "Retraining PyTorch model for new k sample, k=39\n",
            "Retraining PyTorch model for new k sample, k=40\n",
            "Retraining PyTorch model for new k sample, k=41\n",
            "Retraining PyTorch model for new k sample, k=42\n",
            "Retraining PyTorch model for new k sample, k=43\n",
            "Retraining PyTorch model for new k sample, k=44\n",
            "Retraining PyTorch model for new k sample, k=45\n",
            "Retraining PyTorch model for new k sample, k=46\n",
            "Retraining PyTorch model for new k sample, k=47\n",
            "Retraining PyTorch model for new k sample, k=48\n",
            "Retraining PyTorch model for new k sample, k=49\n",
            "Retraining PyTorch model for new k sample, k=50\n",
            "Retraining PyTorch model for new k sample, k=51\n",
            "Retraining PyTorch model for new k sample, k=52\n",
            "Retraining PyTorch model for new k sample, k=53\n",
            "Retraining PyTorch model for new k sample, k=54\n",
            "Retraining PyTorch model for new k sample, k=55\n",
            "Retraining PyTorch model for new k sample, k=56\n",
            "Retraining PyTorch model for new k sample, k=57\n",
            "Retraining PyTorch model for new k sample, k=58\n",
            "Retraining PyTorch model for new k sample, k=59\n",
            "Retraining PyTorch model for new k sample, k=60\n",
            "Retraining PyTorch model for new k sample, k=61\n",
            "Retraining PyTorch model for new k sample, k=62\n",
            "Retraining PyTorch model for new k sample, k=63\n",
            "Retraining PyTorch model for new k sample, k=64\n",
            "Retraining PyTorch model for new k sample, k=65\n",
            "Retraining PyTorch model for new k sample, k=66\n",
            "Retraining PyTorch model for new k sample, k=67\n",
            "Retraining PyTorch model for new k sample, k=68\n",
            "Retraining PyTorch model for new k sample, k=69\n",
            "Retraining PyTorch model for new k sample, k=70\n",
            "Retraining PyTorch model for new k sample, k=71\n",
            "Retraining PyTorch model for new k sample, k=72\n",
            "Retraining PyTorch model for new k sample, k=73\n",
            "Retraining PyTorch model for new k sample, k=74\n",
            "Retraining PyTorch model for new k sample, k=75\n",
            "Retraining PyTorch model for new k sample, k=76\n",
            "Retraining PyTorch model for new k sample, k=77\n",
            "Retraining PyTorch model for new k sample, k=78\n",
            "Retraining PyTorch model for new k sample, k=79\n",
            "Retraining PyTorch model for new k sample, k=80\n",
            "Retraining PyTorch model for new k sample, k=81\n",
            "Retraining PyTorch model for new k sample, k=82\n",
            "Retraining PyTorch model for new k sample, k=83\n",
            "Retraining PyTorch model for new k sample, k=84\n",
            "Retraining PyTorch model for new k sample, k=85\n",
            "Retraining PyTorch model for new k sample, k=86\n",
            "Retraining PyTorch model for new k sample, k=87\n",
            "Retraining PyTorch model for new k sample, k=88\n",
            "Retraining PyTorch model for new k sample, k=89\n",
            "Retraining PyTorch model for new k sample, k=90\n",
            "Retraining PyTorch model for new k sample, k=91\n",
            "Retraining PyTorch model for new k sample, k=92\n",
            "Retraining PyTorch model for new k sample, k=93\n",
            "Retraining PyTorch model for new k sample, k=94\n",
            "Retraining PyTorch model for new k sample, k=95\n",
            "Retraining PyTorch model for new k sample, k=96\n",
            "Retraining PyTorch model for new k sample, k=97\n",
            "Retraining PyTorch model for new k sample, k=98\n",
            "Retraining PyTorch model for new k sample, k=99\n",
            "Retraining PyTorch model for new k sample, k=100\n",
            "Retraining PyTorch model for new k sample, k=101\n",
            "Retraining PyTorch model for new k sample, k=102\n",
            "Retraining PyTorch model for new k sample, k=103\n",
            "Retraining PyTorch model for new k sample, k=104\n",
            "Retraining PyTorch model for new k sample, k=105\n",
            "Retraining PyTorch model for new k sample, k=106\n",
            "Retraining PyTorch model for new k sample, k=107\n",
            "Retraining PyTorch model for new k sample, k=108\n",
            "Retraining PyTorch model for new k sample, k=109\n",
            "Retraining PyTorch model for new k sample, k=110\n",
            "Retraining PyTorch model for new k sample, k=111\n",
            "Retraining PyTorch model for new k sample, k=112\n",
            "Retraining PyTorch model for new k sample, k=113\n",
            "Retraining PyTorch model for new k sample, k=114\n",
            "Retraining PyTorch model for new k sample, k=115\n",
            "Retraining PyTorch model for new k sample, k=116\n",
            "Retraining PyTorch model for new k sample, k=117\n",
            "Retraining PyTorch model for new k sample, k=118\n",
            "Retraining PyTorch model for new k sample, k=119\n",
            "Retraining PyTorch model for new k sample, k=120\n",
            "Retraining PyTorch model for new k sample, k=121\n",
            "Retraining PyTorch model for new k sample, k=122\n",
            "Retraining PyTorch model for new k sample, k=123\n",
            "Retraining PyTorch model for new k sample, k=124\n",
            "Retraining PyTorch model for new k sample, k=125\n",
            "Retraining PyTorch model for new k sample, k=126\n",
            "Retraining PyTorch model for new k sample, k=127\n",
            "Retraining PyTorch model for new k sample, k=128\n",
            "Retraining PyTorch model for new k sample, k=129\n",
            "Retraining PyTorch model for new k sample, k=130\n",
            "Retraining PyTorch model for new k sample, k=131\n",
            "Retraining PyTorch model for new k sample, k=132\n",
            "Retraining PyTorch model for new k sample, k=133\n",
            "Retraining PyTorch model for new k sample, k=134\n",
            "Retraining PyTorch model for new k sample, k=135\n",
            "Retraining PyTorch model for new k sample, k=136\n",
            "Retraining PyTorch model for new k sample, k=137\n",
            "Retraining PyTorch model for new k sample, k=138\n",
            "Retraining PyTorch model for new k sample, k=139\n",
            "Retraining PyTorch model for new k sample, k=140\n",
            "Retraining PyTorch model for new k sample, k=141\n",
            "Retraining PyTorch model for new k sample, k=142\n",
            "Retraining PyTorch model for new k sample, k=143\n",
            "Retraining PyTorch model for new k sample, k=144\n",
            "Retraining PyTorch model for new k sample, k=145\n",
            "Retraining PyTorch model for new k sample, k=146\n",
            "Retraining PyTorch model for new k sample, k=147\n",
            "Retraining PyTorch model for new k sample, k=148\n",
            "Retraining PyTorch model for new k sample, k=149\n",
            "Retraining PyTorch model for new k sample, k=150\n",
            "Retraining PyTorch model for new k sample, k=151\n",
            "Retraining PyTorch model for new k sample, k=152\n",
            "Retraining PyTorch model for new k sample, k=153\n",
            "Retraining PyTorch model for new k sample, k=154\n",
            "Retraining PyTorch model for new k sample, k=155\n",
            "Retraining PyTorch model for new k sample, k=156\n",
            "Retraining PyTorch model for new k sample, k=157\n",
            "Retraining PyTorch model for new k sample, k=158\n",
            "Retraining PyTorch model for new k sample, k=159\n",
            "Retraining PyTorch model for new k sample, k=160\n",
            "Retraining PyTorch model for new k sample, k=161\n",
            "Retraining PyTorch model for new k sample, k=162\n",
            "Retraining PyTorch model for new k sample, k=163\n",
            "Retraining PyTorch model for new k sample, k=164\n",
            "Retraining PyTorch model for new k sample, k=165\n",
            "Retraining PyTorch model for new k sample, k=166\n",
            "Retraining PyTorch model for new k sample, k=167\n",
            "Retraining PyTorch model for new k sample, k=168\n",
            "Retraining PyTorch model for new k sample, k=169\n",
            "Retraining PyTorch model for new k sample, k=170\n",
            "Retraining PyTorch model for new k sample, k=171\n",
            "Retraining PyTorch model for new k sample, k=172\n",
            "Retraining PyTorch model for new k sample, k=173\n",
            "Retraining PyTorch model for new k sample, k=174\n",
            "Retraining PyTorch model for new k sample, k=175\n",
            "Retraining PyTorch model for new k sample, k=176\n",
            "Retraining PyTorch model for new k sample, k=177\n",
            "Retraining PyTorch model for new k sample, k=178\n",
            "Retraining PyTorch model for new k sample, k=179\n",
            "Retraining PyTorch model for new k sample, k=180\n",
            "Retraining PyTorch model for new k sample, k=181\n",
            "Retraining PyTorch model for new k sample, k=182\n",
            "Retraining PyTorch model for new k sample, k=183\n",
            "Retraining PyTorch model for new k sample, k=184\n",
            "Retraining PyTorch model for new k sample, k=185\n",
            "Retraining PyTorch model for new k sample, k=186\n",
            "Retraining PyTorch model for new k sample, k=187\n",
            "Retraining PyTorch model for new k sample, k=188\n",
            "Retraining PyTorch model for new k sample, k=189\n",
            "Retraining PyTorch model for new k sample, k=190\n",
            "Retraining PyTorch model for new k sample, k=191\n",
            "Retraining PyTorch model for new k sample, k=192\n",
            "Retraining PyTorch model for new k sample, k=193\n",
            "Retraining PyTorch model for new k sample, k=194\n",
            "Retraining PyTorch model for new k sample, k=195\n",
            "Retraining PyTorch model for new k sample, k=196\n",
            "Retraining PyTorch model for new k sample, k=197\n",
            "Retraining PyTorch model for new k sample, k=198\n",
            "Retraining PyTorch model for new k sample, k=199\n",
            "Retraining PyTorch model for new k sample, k=200\n",
            "Retraining PyTorch model for new k sample, k=201\n",
            "Retraining PyTorch model for new k sample, k=202\n",
            "Retraining PyTorch model for new k sample, k=203\n",
            "Retraining PyTorch model for new k sample, k=204\n",
            "Retraining PyTorch model for new k sample, k=205\n",
            "Retraining PyTorch model for new k sample, k=206\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Kjmdv4vSA5z"
      },
      "source": [
        "#Analyzing and differentiating from different Techniques\n",
        "\n",
        "Next Steps:\n",
        "\n",
        "1. Create a Baseline model with entire dataset to find best hyperparameters for the neural network.\n",
        "2. GSx Model\n",
        "3. GSy Model\n",
        "4. iGS Model\n",
        "5. Inferencing from variables to see how they correlate to the increase or decrease of both targerts\n",
        "5. Investigating which technique leads to higher performance\n",
        "6. Plotting what algorithm leads to a faster convergence\n",
        "7. Investigate how K effects both convergence and performance of all the models"
      ]
    }
  ]
}